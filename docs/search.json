[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Welcome!\nWelcome to the course materials for EDS 220 - Working with Environmental Datasets! This course is part of the UCSB Masters in Environmental Data Science.\nIn this website you will find the materials covered during lectures and discussion sections for the Fall 2023 term. This site will be under construction throughout the course."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "About",
    "text": "About\nThis hands-on course explores widely used environmental data formats and Python libraries for analyzing diverse environmental data. Students will gain experience working with popular open data repositories and cloud platforms to source and analyze real-world environmental datasets. The course will also serve as an introduction to Python programming and provide opportunities to practice effective communication of the strengths and weaknesses of students’ data products and analyses."
  },
  {
    "objectID": "index.html#instruction-team",
    "href": "index.html#instruction-team",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Instruction Team",
    "text": "Instruction Team\n\nInstructor\nCarmen Galaz García (she/her/hers)\nE-mail: galaz-garcia@nceas.ucsb.edu\nOffice hour: Wednesday 3:30-4:30 pm, NCEAS classroom\nBest way to contact me: e-mail\n\n\nTA\nYutian Fang (she/her/hers)\nE-mail: yutianfang@bren.ucsb.edu\nOffice Hours: Thursday, 3-4 pm, Marine Science Institute office 1208.\nBest way to contact me: email"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Syllabus",
    "text": "Syllabus\nClick here to access the syllabus."
  },
  {
    "objectID": "index.html#calendar",
    "href": "index.html#calendar",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Calendar",
    "text": "Calendar\nThe following is our ideal calendar, the course content and calendar may be subject to change as the course progresses."
  },
  {
    "objectID": "index.html#about-this-website",
    "href": "index.html#about-this-website",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "About this website",
    "text": "About this website\nThis website is created with Quarto and is published through GitHub pages. Click here to go to the website’s GitHub repository."
  },
  {
    "objectID": "index.html#contribute",
    "href": "index.html#contribute",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Contribute",
    "text": "Contribute\nIf you have suggestions on how to correct, improve, or expand this book, please feel free to email galaz-garcia@nceas.ucsb.edu or file a GitHub issue."
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#about",
    "href": "lectures/lesson-1-python-review.html#about",
    "title": "1  Python Review",
    "section": "1.1 About",
    "text": "1.1 About\nThis is a short reveiw about some core concepts in Python exemplified by objects in the numpy library."
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#numpy",
    "href": "lectures/lesson-1-python-review.html#numpy",
    "title": "1  Python Review",
    "section": "1.2 numpy",
    "text": "1.2 numpy\nNumPy is one of the core packages for numerical computing in Python. Many of the packages we will use in this course use NumPy’s arrays as their building blocks. Additionally, numpy objects have been optimized for processing, so computations on them are really fast and use less memory than doing the equivalent using base Python.\nIn this lesson we will use numpy to review some core concepts in Python you’re already familiar with.\nFirst, let’s start by importing the library:\n\nimport numpy as np"
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#variables",
    "href": "lectures/lesson-1-python-review.html#variables",
    "title": "1  Python Review",
    "section": "1.3 Variables",
    "text": "1.3 Variables\nWe can think of a variable as a name we assign to a particular object in Python. For example:\n\n# assign a small array to variable a\na = np.array([[1,1,2],[3,5,8]])\n\nWhen we run the cell, we store the variables and their value. We can view a variable’s value in two ways:\n\nrunning a cell with the variable name\nusing the print function to print the value\n\n\n# show the value\na\n\narray([[1, 1, 2],\n       [3, 5, 8]])\n\n\n\n# print the value \nprint(a)\n\n[[1 1 2]\n [3 5 8]]\n\n\n\n\n\n\n\n\nR and Python\n\n\n\nIn Python we use the equal sign = to assign values to variables in the same way the left-arrow &lt;- is used in R.\n\n\n\n\n\n\n\n\nNaming Variables\n\n\n\nThere are many ways of constructing multi-word variable names. In this course we will name variables using snake_case, where words are all in small caps and separated by underscores (ex: my_variable). This is the naming convention suggested by the Style Guide for Python Code."
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#variables-and-objects",
    "href": "lectures/lesson-1-python-review.html#variables-and-objects",
    "title": "1  Python Review",
    "section": "1.4 Variables and Objects",
    "text": "1.4 Variables and Objects\nYou will often encounter the word object in Python documentation and tutorials. Informally speaking, an object is a bundle of properties and actions about something specific. For example, an object could represent a data frame with properties such as number of rows, names of columns, and date created, and actions suchs as selecting a specific row or adding a new column.\nA variable is the name we give a specific object, and the same object can be referenced by different variables. An analogy for this is the following: the Sun (object) is called “sol” in Spanish and “soleil” in French, so two different names (variables) represent the same object. You can read more technical details about the difference between objects and variables in Python here.\nIn practice, we can often use the word variable and object interchangeably. I want to bring up what objects are so you’re not caught off-guard with vocabulary you’ll often encounter in the documentation, StackExchange, etc. We’ll often use the word object too (for example, in the next subsection!)."
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#types",
    "href": "lectures/lesson-1-python-review.html#types",
    "title": "1  Python Review",
    "section": "1.5 Types",
    "text": "1.5 Types\nEvery object in Python has a type, the type tells us what kind of object it is. We can also call the type of an object, the class of an object (so class and type both mean what kind of object we have).\nWe can see the type/class of a variable/object by using the type function:\n\nprint(a)\ntype(a)\n\n[[1 1 2]\n [3 5 8]]\n\n\nnumpy.ndarray\n\n\nThe numpy.ndarray is the core object/data type in the NumPy pakcage. We can check the type of an entry in the array by indexing:\n\nprint(a[0,0])\ntype(a[0,0])\n\n1\n\n\nnumpy.int64\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nHow would you access the value 5 in the array a? Remember indexing in Python starts from 0!\n\n\nNotice the type of the value 1 in the array is numpy.int64 and not just the core Python integer type int. The NumPy type numpy.int64 is telling us 1 is an integer stored as a 64-bit number. NumPy has its own data types to deal with numbers depending on memory storage and floating point precision, click here to know see all the types.\nSince “everything in Python is an object” and every object has a class, we will interact with SO MANY classes in this course. Often, knowing the type of an object is the first step to finding information to code what you want!"
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#functions",
    "href": "lectures/lesson-1-python-review.html#functions",
    "title": "1  Python Review",
    "section": "1.6 Functions",
    "text": "1.6 Functions\nprint was our first example of a Python function. Functions take in a set of arguments, separated by commas, and use those arguments to create some output. There are several built-in funcions in Python, most of them are for interacting with the basic Python data types. You can see a list of them here.\n\n\n\n\n\n\nArgument or Parameter?\n\n\n\nWe can interchangeably say arguments or parameters. You will see argument more often in the documentation.\n\n\nWe can ask for information about a function by executing ? followed by the function name:\n\n?print\n\n\nThe first line is always the function showing all of its arguments in parenthesis. Then there is a short description of what the function does. And finally a list of the arguments and a brief explanation about each of them.\nYou can see there are different types of arguments inside the parenthesis. Roughly speaking, a function has two types of arguments:\n\nnon-optional arguments: arguments you need to specify for the function to do something, and\noptional arguments: arguments that are pre-filled with a default value by the function, but you can override them. Optional arguments appear inside the parenthesis () in the form optional_argument = default_value.\n\nExample:\nend is an argument in print with the default value a new line. We can change this argument so that finishes the line with ^_^ instead:\n\n# notice we had always used print withough specifying any value for the `end` argument\nprint('I am changing the default end argument of the print function', end=' ^_^')\n\nI am changing the default end argument of the print function ^_^"
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#attributes-methods",
    "href": "lectures/lesson-1-python-review.html#attributes-methods",
    "title": "1  Python Review",
    "section": "1.7 Attributes & Methods",
    "text": "1.7 Attributes & Methods\nAn object in Python has attributes and methods. An attribute is a property of the object, some piece of information about it. A method is a procedure associated with an object, so it is an action where the main ingredient is the object.\nFor example, these could be some attributes and methods a class cat:\n\n\n\n.\n\n\nMore formally, a method is a function that acts on the object it is part of.\nWe can access a variable’s attributes and methods by adding a period . at the end of the variable’s name. So we would write variable.variable_method() or variable.variable_attribute.\n\n\n\n\n\n\nCheck-in\n\n\n\nSuppose we have a class fish, make a diagram similar to the cat class diagram showing 3 attributes for the class and 3 methods.\n\n\nExample\nNumPy arrays have many methods and attributes. Let’s see some concrete examples.\n\n# define a 3x3 array\nvar = np.array([[1,2,3],[4,5,6],[7,8,9]])\nvar\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\n# T is an example of attribute, it returns the transpose of var\nprint(var.T)\nprint(type(var.T))\n\n[[1 4 7]\n [2 5 8]\n [3 6 9]]\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n# shape, another attribute, tells us the shape of the array (3x3)\nprint(var.shape)\nprint(type(var.shape))\n\n(3, 3)\n&lt;class 'tuple'&gt;\n\n\n\n# ndim is an attribute holding the number of array dimensions\nprint(var.ndim)\nprint(type(var.ndim))\n\n2\n&lt;class 'int'&gt;\n\n\nNotice these attributes can have many different data types. Here we saw tuples and int, two of the core Python classes, and also a numpy array as attributes of var.\nNow some examples of methods:\n\n# the tolist method returns the array as a nested list of scalars\nvar.tolist()\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n\n# the min method returns the minimum value in the array along an axis\nvar.min(axis=0)\n\narray([1, 2, 3])\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWe can also call the min method without any parameters:\n\nvar.min()\n\n1\n\n\nWhat kind of parameter is axis in our previous call of the var method?\n\n\nRemember, methods are functions associated to an object. We can check this!\n\ntype(var.tolist)\n\nbuiltin_function_or_method\n\n\n\ntype(var.min)\n\nbuiltin_function_or_method\n\n\nYou can see a complete list of NumPy array’s methods and attributes in the documentation.\n\n\n\n\n\n\nR and Python\n\n\n\nIn R we don’t use methods within an object. Rather, functions are extrinsic to (outside) the objects they are acting on. In R, for example, there would be two separate items: the variable var and a separate function min that gets var as a parameter:\n# this is R code\nvar &lt;- array(c(1,4,7,2,5,8,3,6,9), dim =c(3,3))\nmin(var)\nUsing the pipe operator %&gt;% in R’s tidyverse is closer to the dot . in Python:\n# this is R code\nvar &lt;- array(c(1,4,7,2,5,8,3,6,9), dim =c(3,3))\nvar %&gt;% min()\nWhat happens here is that the pipe %&gt;% is passing var to the min() function as its first argument. This is essentially what happens in Python when a function is a method of a class:\n# this is Python code\nvar = np.array([[1,2,3],[4,5,6],[7,8,9]])\nvar.min()\nWhen working in Python, remember that methods are functions that are part of an object and a method uses the object it is part of to produce some information."
  },
  {
    "objectID": "lectures/lesson-2-pandas-basics.html#about",
    "href": "lectures/lesson-2-pandas-basics.html#about",
    "title": "2  Pandas Fundamentals",
    "section": "2.1 About",
    "text": "2.1 About\nIn this lesson we cover the two core objects in the pandas library, the pandas.Series and the pandas.DataFrame. We will also learn methods to select data from our datasets."
  },
  {
    "objectID": "lectures/lesson-2-pandas-basics.html#pandas",
    "href": "lectures/lesson-2-pandas-basics.html#pandas",
    "title": "2  Pandas Fundamentals",
    "section": "2.2 pandas",
    "text": "2.2 pandas\npandas is a Python package to wrangle and analyze tabular data. It is built on top of NumPy and has become the core tool for doing data analysis in Python.\nThe convention to import it is:\n\nimport pandas as pd\n\n# we will also import numpy \nimport numpy as np\n\nThere is so much to learn about pandas. While we won’t be able to cover every single functionality of this package in the next three lecutres, the goal is to get you started with the basic tools for data wrangling and give you a solid basis on which you can explore further."
  },
  {
    "objectID": "lectures/lesson-2-pandas-basics.html#series",
    "href": "lectures/lesson-2-pandas-basics.html#series",
    "title": "2  Pandas Fundamentals",
    "section": "2.3 Series",
    "text": "2.3 Series\nThe first core data structure of pandas is the series. A series is a one-dimensional array of indexed data. A pandas.Series having an index is the main difference between a pandas.Series and a numpy array. See the difference:\n\n# a numpy array\n# np.random.randn returns values from the std normal distribution\narr = np.random.randn(4) \nprint(type(arr))\nprint(arr, \"\\n\")\n\n# a pandas series made from the previous array\ns = pd.Series(arr)\nprint(type(s))\nprint(s)\n\n&lt;class 'numpy.ndarray'&gt;\n[ 0.55130267 -0.38950712 -1.50835146  1.01872105] \n\n&lt;class 'pandas.core.series.Series'&gt;\n0    0.551303\n1   -0.389507\n2   -1.508351\n3    1.018721\ndtype: float64\n\n\n\n2.3.1 Creating a pandas.Series\nThe basic method to create a pandas.Series is to call\ns = pd.Series(data, index=index)\nThe data parameter can be:\n\na numpy array or a list\na Python dictionary\na number\n\nThe index parameter is a list of index labels.\nFor now, we will create a pandas.Series from a numpy array or list. To use this method we need to pass a numpy array (or a list of objects that can be converted to NumPy types) as data and a list of indices of the same length as data.\n\n# a Series from a numpy array \npd.Series(np.arange(3), index=['a','b','c'])\n\na    0\nb    1\nc    2\ndtype: int64\n\n\nThe index parameter is optional. If we don’t include it, the default is to make the index equal to [0,...,len(data)-1]. For example:\n\n# a Series from a list of strings with default index\npd.Series(['EDS 220', 'EDS 222', 'EDS 223', 'EDS 242'])\n\n0    EDS 220\n1    EDS 222\n2    EDS 223\n3    EDS 242\ndtype: object\n\n\n\n\n\n2.3.2 Simple operations\nArithmetic operations work on series and also most NumPy functions. For example:\n\n# define a series\ns = pd.Series([98,73,65],index=['Andrea', 'Beth', 'Carolina'])\n\n# divide each element in series by 10\nprint(s /10, '\\n')\n\n# take the exponential of each element in series\nprint(np.exp(s), '\\n')\n\n# notice this doesn't change the values of our series\nprint(s)\n\nAndrea      9.8\nBeth        7.3\nCarolina    6.5\ndtype: float64 \n\nAndrea      3.637971e+42\nBeth        5.052394e+31\nCarolina    1.694889e+28\ndtype: float64 \n\nAndrea      98\nBeth        73\nCarolina    65\ndtype: int64\n\n\nWe can also produce new pandas.Series with True/False values indicating whether the elements in a series satisfy a condition or not:\n\ns &gt; 10\n\nAndrea      True\nBeth        True\nCarolina    True\ndtype: bool\n\n\nThis kind of simple conditions on pandas.Series will be key when we are selecting data from data frames.\n\n\n\n2.3.3 Attributes & Methods\npandas.Series have many attributes and methods, you can see a full list in the pandas documentation. For now we will cover two examples that have to do with identifying missing values.\npandas represents a missing or NA value with NaN, which stands for not a number. Let’s construct a small series with some NA values:\n\n# series with NAs in it\ns = pd.Series([1, 2, np.NaN, 4, np.NaN])\n\nA pandas.Series has an attribute called hasnans that returns True if there are any NaNs:\n\n# check if series has NAs\ns.hasnans\n\nTrue\n\n\nThen we might be intersted in knowing which elements in the series are NAs. We can do this using the isna method:\n\ns.isna()\n\n0    False\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n\n\nWe can see the ouput is a pd.Series of boolean values indicating if an element in the row at the given index is NA (True = is NA) or not (False = not NA).\n\n\n\n\n\n\nmoving on\n\n\n\nThere’s much more to say about pandas.Series, but this is enought to get us going. At this point, we mainly want to know about pandas.Series because pandas.Series are the columns of pandas.DataFrames."
  },
  {
    "objectID": "lectures/lesson-2-pandas-basics.html#data-frames",
    "href": "lectures/lesson-2-pandas-basics.html#data-frames",
    "title": "2  Pandas Fundamentals",
    "section": "2.4 Data Frames",
    "text": "2.4 Data Frames\nThe Data Frame is the most used pandas object. It represents tabular data and we can think of it as a spreadhseet. Each column of a pandas.DataFrame is a pandas.Series.\n\n2.4.1 Creating a pandas.DataFrame\nThere are many ways of creating a pandas.DataFrame.\n\nWe already mentioned each column of a pandas.DataFrame is a pandas.Series. In fact, the pandas.DataFrame is a dictionary of pandas.Series, with each column name being the key and the column values being the key’s value. Thus, we can create a pandas.DataFrame in this way:\n\n# initialize dictionary with columns' data \nd = {'col_name_1' : pd.Series(np.arange(3)),\n     'col_name_2' : pd.Series([3.1, 3.2, 3.3]),\n     }\n\n# create data frame\ndf = pd.DataFrame(d)\ndf\n\n\n\n\n\n\n\n\ncol_name_1\ncol_name_2\n\n\n\n\n0\n0\n3.1\n\n\n1\n1\n3.2\n\n\n2\n2\n3.3\n\n\n\n\n\n\n\nWe can change the index and column names by changing the index and columns attributes in the data frame.\n\n# print original index\nprint(df.index)\n\n# change the index\ndf.index = ['a','b','c']\ndf\n\nRangeIndex(start=0, stop=3, step=1)\n\n\n\n\n\n\n\n\n\ncol_name_1\ncol_name_2\n\n\n\n\na\n0\n3.1\n\n\nb\n1\n3.2\n\n\nc\n2\n3.3\n\n\n\n\n\n\n\n\n# print original column names\nprint(df.columns)\n\n# change column names \ndf.columns = ['C1','C2']\ndf\n\nIndex(['col_name_1', 'col_name_2'], dtype='object')\n\n\n\n\n\n\n\n\n\nC1\nC2\n\n\n\n\na\n0\n3.1\n\n\nb\n1\n3.2\n\n\nc\n2\n3.3"
  },
  {
    "objectID": "lectures/lesson-2-pandas-basics.html#subsetting-a-pandas.dataframe",
    "href": "lectures/lesson-2-pandas-basics.html#subsetting-a-pandas.dataframe",
    "title": "2  Pandas Fundamentals",
    "section": "2.5 Subsetting a pandas.DataFrame",
    "text": "2.5 Subsetting a pandas.DataFrame\nLike it’s often the case when working with pandas, there are many ways in which we can subset a data frame. We will review the core methods to do this. \nFor all examples we will use simplified data (glacial_loss.csv) from the National Snow and Ice Data Center (Original dataset). The column descriptions are:\n\nyear: ​calendar year\neurope - antarctica: ​change in glacial volume (km3​ ​) in each region that year\nglobal_glacial_volume_change: ​cumulative​ global glacial volume change (km3), starting in 1961\nannual_sea_level_rise: ​annual rise in sea level (mm)\ncumulative_sea_level_rise:​ cumulative rise in sea level (mm) since 1961\n\nFirst, we read-in the file and get some baisc information about this data frame:\n\n# read in file\ndf = pd.read_csv('data/lesson-1/glacial_loss.csv')\n\n# see the first five rows\ndf.head()\n\n\n\n\n\n\n\n\nyear\neurope\narctic\nalaska\nasia\nnorth_america\nsouth_america\nantarctica\nglobal_glacial_volume_change\nannual_sea_level_rise\ncumulative_sea_level_rise\n\n\n\n\n0\n1961\n-5.128903\n-108.382987\n-18.721190\n-32.350759\n-14.359007\n-4.739367\n-35.116389\n-220.823515\n0.610010\n0.610010\n\n\n1\n1962\n5.576282\n-173.252450\n-24.324790\n-4.675440\n-2.161842\n-13.694367\n-78.222887\n-514.269862\n0.810625\n1.420635\n\n\n2\n1963\n-10.123105\n-0.423751\n-2.047567\n-3.027298\n-27.535881\n3.419633\n3.765109\n-550.575640\n0.100292\n1.520927\n\n\n3\n1964\n-4.508358\n20.070148\n0.477800\n-18.675385\n-2.248286\n20.732633\n14.853096\n-519.589859\n-0.085596\n1.435331\n\n\n4\n1965\n10.629385\n43.695389\n-0.115332\n-18.414602\n-19.398765\n6.862102\n22.793484\n-473.112003\n-0.128392\n1.306939\n\n\n\n\n\n\n\n\n# get column names\ndf.columns\n\nIndex(['year', 'europe', 'arctic', 'alaska', 'asia', 'north_america',\n       'south_america', 'antarctica', 'global_glacial_volume_change',\n       'annual_sea_level_rise', 'cumulative_sea_level_rise'],\n      dtype='object')\n\n\n\n# check the data types of each column\ndf.dtypes\n\nyear                              int64\neurope                          float64\narctic                          float64\nalaska                          float64\nasia                            float64\nnorth_america                   float64\nsouth_america                   float64\nantarctica                      float64\nglobal_glacial_volume_change    float64\nannual_sea_level_rise           float64\ncumulative_sea_level_rise       float64\ndtype: object\n\n\n\n# data frame's shape: output is a tuple (# rows, # columns)\ndf.shape\n\n(43, 11)\n\n\n\n2.5.1 Selecting a single column…\n\n2.5.1.1 …by column name\nThis is the simplest case for selecting data. Suppose we are interested in the annual sea level rise. Then we can access that single column in this way:\n\n# seelect a single column by using square brackets []\nannual_rise = df['annual_sea_level_rise']\n\n# check the type of the ouput\nprint(type(annual_rise))\n\nannual_rise.head()\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n0    0.610010\n1    0.810625\n2    0.100292\n3   -0.085596\n4   -0.128392\nName: annual_sea_level_rise, dtype: float64\n\n\nSince we only selected a single column the output is a pandas.Series.\n\n\n\n\n\n\npd.DataFrame = dictionary of columns\n\n\n\nRemember we can think of a pandas.DataFrame as a dictionary of its columns? Then we can access a single column using the column name as the key, just like we would do in a dictionary. That is the we just used: df['column_name'].\n\n\nThis is an example of selecting by label, which means we want to select data from our data frame using the names of the columns, not their position.\n\n\n2.5.1.2 … with attribute syntax\nWe can also access a single column by using attribute syntax:\n\nannual_rise_2 = df.annual_sea_level_rise\nannual_rise_2.head()\n\n0    0.610010\n1    0.810625\n2    0.100292\n3   -0.085596\n4   -0.128392\nName: annual_sea_level_rise, dtype: float64\n\n\n\n\n\n2.5.2 Selecting multiple columns…\n\n2.5.2.1 … using a list of column names\nThis is another example of selecting by labels. We just need to pass a list with the column names to the square brackets []. For example, say we want to look at the change in glacial volume in Europe and Asia, then we can select those columns like this:\n\n# select columns with names \"europe\" and \"asia\"\neurope_asia = df[['europe','asia']]\n\nNotice there are double square brackets. This is because we are passing the list of names ['europe','asia'] to the selection brakcets [].\n\n# check the type of the resulting selection\nprint(type(europe_asia))\n\n# check the shape of the selection\nprint((europe_asia.shape))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n(43, 2)\n\n\n\n\n2.5.2.2 … using a slice\nYet another example of selecting by label! In this case we will use the loc function. This is a powerful function! The general syntax is\ndf.loc[ row-selection , column-selection]\nwhere row-selection and column-selection are the rows and columns we want to subset from the data frame.\nLet’s start by a simple example, where we want to select a slice of columns, say the change in glacial volume per year in all regions. This corresponds to all columns between arctic and antarctica.\n\n# select all columns between 'arctic' and 'antarctica'\nall_regions = df.loc[:,'arctic':'antarctica']\nall_regions.head()\n\n\n\n\n\n\n\n\narctic\nalaska\nasia\nnorth_america\nsouth_america\nantarctica\n\n\n\n\n0\n-108.382987\n-18.721190\n-32.350759\n-14.359007\n-4.739367\n-35.116389\n\n\n1\n-173.252450\n-24.324790\n-4.675440\n-2.161842\n-13.694367\n-78.222887\n\n\n2\n-0.423751\n-2.047567\n-3.027298\n-27.535881\n3.419633\n3.765109\n\n\n3\n20.070148\n0.477800\n-18.675385\n-2.248286\n20.732633\n14.853096\n\n\n4\n43.695389\n-0.115332\n-18.414602\n-19.398765\n6.862102\n22.793484\n\n\n\n\n\n\n\nNotice two things:\n\nwe used the colon : as the row-selection parameter, which means “select all the rows”\nthe slice of the data frame we got includes both endpoints of the slice 'arctic':'antarctica'. In other words we get the arctic column and the antarctica column. This is different from how slicing works in base Python and NumPy, where the end point is not included.\n\n\n\n\n2.5.3 Selecting rows…\nNow that we are familiar with some methods for selecting columns, let’s move on to selecting rows.\n\n2.5.3.1 … using a condition\nSelecting which rows satisfy a particular condition is, in my experience, the most usual kind of row subsetting. The general syntax for this type of selection is df[condition_on_rows]. For example, suppose we are intersted in all data after 1996. We can select those rows in this way:\n\n# select all rows with year &gt; 1996\nafter_96 = df[df['year']&gt;1996]\nafter_96\n\n\n\n\n\n\n\n\nyear\neurope\narctic\nalaska\nasia\nnorth_america\nsouth_america\nantarctica\nglobal_glacial_volume_change\nannual_sea_level_rise\ncumulative_sea_level_rise\n\n\n\n\n36\n1997\n-13.724106\n-24.832246\n-167.229145\n-34.406403\n-27.680661\n-38.213286\n-20.179090\n-4600.686013\n0.909625\n12.709077\n\n\n37\n1998\n-13.083338\n-110.429302\n-107.879027\n-58.115702\n30.169987\n-3.797978\n-48.129928\n-4914.831966\n0.867807\n13.576884\n\n\n38\n1999\n-8.039555\n-64.644068\n-87.714653\n-26.211723\n5.888512\n-8.038630\n-40.653001\n-5146.368231\n0.639603\n14.216487\n\n\n39\n2000\n-17.008590\n-96.494055\n-44.445000\n-37.518173\n-29.191986\n-2.767698\n-58.873830\n-5435.317175\n0.798202\n15.014688\n\n\n40\n2001\n-8.419109\n-145.415483\n-55.749505\n-35.977022\n-0.926134\n7.553503\n-86.774675\n-5764.039931\n0.908074\n15.922762\n\n\n41\n2002\n-3.392361\n-48.718943\n-87.120000\n-36.127226\n-27.853498\n-13.484593\n-30.203960\n-6013.225500\n0.688358\n16.611120\n\n\n42\n2003\n-3.392361\n-48.718943\n-67.253634\n-36.021991\n-75.066475\n-13.223430\n-30.203960\n-6289.640976\n0.763579\n17.374699\n\n\n\n\n\n\n\nLet’s break down what is happening here. In this case the condition for our rows is df['year']&gt;1996, this checks which rows have a value greater than 1996 in the year column. Let’s see this explicitely:\n\n# check the type of df['year']&gt;1996\nprint(type(df['year']&gt;1996))\n\ndf['year']&gt;1996\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14    False\n15    False\n16    False\n17    False\n18    False\n19    False\n20    False\n21    False\n22    False\n23    False\n24    False\n25    False\n26    False\n27    False\n28    False\n29    False\n30    False\n31    False\n32    False\n33    False\n34    False\n35    False\n36     True\n37     True\n38     True\n39     True\n40     True\n41     True\n42     True\nName: year, dtype: bool\n\n\nThe output is a pandas.Series with boolean values (True or False) indicating which rows satisfy the condition year&gt;1996. When we pass such a series of boolean values to the selection brackets [] we keep only those rows with a True value.\nHere’s another example of using a condition. Suppose we want to look at data from years 1970 to 1979. One way of doing this is to use the in operator in our condition:\n\nseventies = df[df['year'].isin(range(1970,1980))]\nseventies\n\n\n\n\n\n\n\n\nyear\neurope\narctic\nalaska\nasia\nnorth_america\nsouth_america\nantarctica\nglobal_glacial_volume_change\nannual_sea_level_rise\ncumulative_sea_level_rise\n\n\n\n\n9\n1970\n-6.452316\n-24.494667\n-0.125296\n-36.120199\n11.619790\n11.636911\n4.400377\n-999.018177\n0.110225\n2.759719\n\n\n10\n1971\n0.414711\n-42.904189\n28.103328\n-8.702938\n-9.964542\n1.061299\n-6.735536\n-1038.104459\n0.107973\n2.867692\n\n\n11\n1972\n-5.144729\n-27.004031\n-22.143350\n-40.883357\n32.363730\n-14.968034\n-6.223849\n-1122.885506\n0.234202\n3.101894\n\n\n12\n1973\n4.081090\n9.839444\n22.985188\n-31.432594\n-20.883232\n2.103649\n10.539823\n-1125.677743\n0.007713\n3.109607\n\n\n13\n1974\n1.545615\n-40.126998\n-29.517874\n-43.861622\n-23.991402\n-21.338825\n4.419343\n-1279.964287\n0.426206\n3.535813\n\n\n14\n1975\n7.431192\n-32.410467\n-44.094084\n-43.357442\n-30.858810\n-2.368842\n-7.775315\n-1434.818037\n0.427773\n3.963586\n\n\n15\n1976\n3.986753\n21.686639\n-28.234725\n-67.292125\n-12.534421\n-19.465358\n19.250607\n-1518.185129\n0.230296\n4.193882\n\n\n16\n1977\n4.891410\n-33.123010\n-5.662139\n-62.165684\n-15.905332\n2.654950\n-23.727249\n-1652.453400\n0.370907\n4.564788\n\n\n17\n1978\n8.404591\n-77.561015\n-12.503384\n-22.858040\n-31.097609\n7.127708\n-9.140167\n-1791.355022\n0.383706\n4.948495\n\n\n18\n1979\n3.916703\n-88.351684\n-63.938851\n-49.242043\n-12.076624\n-17.718503\n-9.578557\n-2030.537848\n0.660726\n5.609221\n\n\n\n\n\n\n\nLet’s break it down:\n\ndf['year'] is the column with the year values, a pandas.Series,\nin df['year'].isin(), we have that isin is a method for the pandas.Series and we are calling it using the dot ..\nrange(1970,1980) constructs consecutive integers from 1970 to 1979 - remember the right endopoint (1980) is not included!\ndf['year'].isin(range(1970,1980)) is then a pandas.Series of boolean values indicating which rows have year equal to 1970, …, 1979.\nwhen we put df['year'].isin(range(1970,1980)) inside the selection brackets [] we obtain the rows of the data frame with year equal to 1970, …, 1979.\n\n\n\n\n\n\n\nloc for row selection\n\n\n\nIt is equivalent to write\n# select rows with year&lt;1965\ndf[df['year'] &lt; 1965]\nand\n# select rows with year&lt;1965 using love\ndf.loc[ df['year'] &lt;1965 , :]\nIn the second one:\n\nwe are using the df.loc[ row-selection , column-selection] syntax\nthe row-selection parameter is the condition df['year']&lt;1965\nthe column-selection parameter is a colon :, which indicates we want all columns for the rows we are selecting.\n\nWe prefer the first syntax when we are selecting rows and not columns since it is simpler.\n\n\n\n\n2.5.3.2 … using multiple conditions\nWe can combine multipe conditions by surrounding each one in parenthesis () and using the or operator | and the and operator and.\nor example:\n\n# select rows with \n# annual_sea_level_rise&lt;0.5 mm OR annual_sea_level_rise&gt;0.8 mm\n\ndf[ (df['annual_sea_level_rise']&lt;0.5) | (df['annual_sea_level_rise']&gt;0.8)]\ndf.head()\n\n\n\n\n\n\n\n\nyear\neurope\narctic\nalaska\nasia\nnorth_america\nsouth_america\nantarctica\nglobal_glacial_volume_change\nannual_sea_level_rise\ncumulative_sea_level_rise\n\n\n\n\n0\n1961\n-5.128903\n-108.382987\n-18.721190\n-32.350759\n-14.359007\n-4.739367\n-35.116389\n-220.823515\n0.610010\n0.610010\n\n\n1\n1962\n5.576282\n-173.252450\n-24.324790\n-4.675440\n-2.161842\n-13.694367\n-78.222887\n-514.269862\n0.810625\n1.420635\n\n\n2\n1963\n-10.123105\n-0.423751\n-2.047567\n-3.027298\n-27.535881\n3.419633\n3.765109\n-550.575640\n0.100292\n1.520927\n\n\n3\n1964\n-4.508358\n20.070148\n0.477800\n-18.675385\n-2.248286\n20.732633\n14.853096\n-519.589859\n-0.085596\n1.435331\n\n\n4\n1965\n10.629385\n43.695389\n-0.115332\n-18.414602\n-19.398765\n6.862102\n22.793484\n-473.112003\n-0.128392\n1.306939\n\n\n\n\n\n\n\nand example\n\n# select rows with cumulative_sea_level_rise&gt;10 AND  global_glacial_volume_change&lt;-300\ndf[ (df['cumulative_sea_level_rise']&gt;10) & (df['global_glacial_volume_change']&lt;-300)]\n\n\n\n\n\n\n\n\nyear\neurope\narctic\nalaska\nasia\nnorth_america\nsouth_america\nantarctica\nglobal_glacial_volume_change\nannual_sea_level_rise\ncumulative_sea_level_rise\n\n\n\n\n32\n1993\n16.685013\n-73.666274\n-43.702040\n-65.995130\n-33.151246\n-20.578403\n-20.311577\n-3672.582082\n0.671126\n10.145254\n\n\n33\n1994\n0.741751\n-3.069084\n-59.962273\n-59.004710\n-89.506142\n-15.258449\n-8.168498\n-3908.977191\n0.653025\n10.798280\n\n\n34\n1995\n-2.139665\n-58.167778\n-74.141762\n3.500155\n-0.699374\n-19.863392\n-25.951496\n-4088.082873\n0.494767\n11.293047\n\n\n35\n1996\n-6.809834\n-4.550205\n-74.847017\n-67.436591\n4.867530\n-21.080115\n-11.781489\n-4271.401594\n0.506405\n11.799452\n\n\n36\n1997\n-13.724106\n-24.832246\n-167.229145\n-34.406403\n-27.680661\n-38.213286\n-20.179090\n-4600.686013\n0.909625\n12.709077\n\n\n37\n1998\n-13.083338\n-110.429302\n-107.879027\n-58.115702\n30.169987\n-3.797978\n-48.129928\n-4914.831966\n0.867807\n13.576884\n\n\n38\n1999\n-8.039555\n-64.644068\n-87.714653\n-26.211723\n5.888512\n-8.038630\n-40.653001\n-5146.368231\n0.639603\n14.216487\n\n\n39\n2000\n-17.008590\n-96.494055\n-44.445000\n-37.518173\n-29.191986\n-2.767698\n-58.873830\n-5435.317175\n0.798202\n15.014688\n\n\n40\n2001\n-8.419109\n-145.415483\n-55.749505\n-35.977022\n-0.926134\n7.553503\n-86.774675\n-5764.039931\n0.908074\n15.922762\n\n\n41\n2002\n-3.392361\n-48.718943\n-87.120000\n-36.127226\n-27.853498\n-13.484593\n-30.203960\n-6013.225500\n0.688358\n16.611120\n\n\n42\n2003\n-3.392361\n-48.718943\n-67.253634\n-36.021991\n-75.066475\n-13.223430\n-30.203960\n-6289.640976\n0.763579\n17.374699\n\n\n\n\n\n\n\n\n\n2.5.3.3 … by position\nAll the selections we have done so far have been using labels or using a condition. Sometimes we might want to select certain rows depending on their actual position in the data frame. In this case we use the iloc method with the syntax df.iloc[row-indices]. iloc stands for integer-location based indexing. Let’s see some examples:\n\n# select the fifht row = index 4\ndf.iloc[4]\n\nyear                            1965.000000\neurope                            10.629385\narctic                            43.695389\nalaska                            -0.115332\nasia                             -18.414602\nnorth_america                    -19.398765\nsouth_america                      6.862102\nantarctica                        22.793484\nglobal_glacial_volume_change    -473.112003\nannual_sea_level_rise             -0.128392\ncumulative_sea_level_rise          1.306939\nName: 4, dtype: float64\n\n\n\n# select rows 23 through 30, inclduing 30\ndf.iloc[23:31]\n\n\n\n\n\n\n\n\nyear\neurope\narctic\nalaska\nasia\nnorth_america\nsouth_america\nantarctica\nglobal_glacial_volume_change\nannual_sea_level_rise\ncumulative_sea_level_rise\n\n\n\n\n23\n1984\n8.581427\n-5.755672\n-33.466092\n-20.528535\n-20.734676\n-8.267686\n-3.261011\n-2569.339802\n0.232609\n7.097624\n\n\n24\n1985\n-5.970980\n-49.651089\n12.065473\n-31.571622\n-33.833985\n10.072906\n-13.587886\n-2682.857926\n0.313586\n7.411210\n\n\n25\n1986\n-5.680642\n22.900847\n7.557447\n-18.920773\n-33.014743\n-4.652030\n30.482473\n-2684.197632\n0.003701\n7.414911\n\n\n26\n1987\n8.191477\n12.387780\n-24.007862\n-41.121970\n-48.560996\n1.670733\n3.130190\n-2773.325568\n0.246210\n7.661120\n\n\n27\n1988\n-11.117228\n-31.066489\n49.897712\n-21.300712\n-46.545435\n13.460422\n-37.986834\n-2858.767621\n0.236028\n7.897148\n\n\n28\n1989\n14.863220\n-23.462392\n-36.112726\n-46.528372\n-57.756422\n-21.687470\n-10.044757\n-3041.169131\n0.503872\n8.401020\n\n\n29\n1990\n-1.226009\n-27.484542\n-92.713339\n-35.553433\n-56.563056\n-31.077022\n-29.893352\n-3318.220397\n0.765335\n9.166355\n\n\n30\n1991\n-14.391425\n-34.898689\n-8.822063\n-15.338299\n-31.458010\n-7.162909\n-35.968429\n-3467.630284\n0.412734\n9.579089\n\n\n\n\n\n\n\nNotice since we are back to indexing by position the right endpoint of the slice (6) is not included in the ouput.\n\n\n\n2.5.4 Selecting rows and columns simultaneously…\nSelecting rows and columns simultaneously can be done using loc (labels or conditions) or iloc (integer position).\n\n2.5.4.1 …by labels or conditions\nWhen we want to select rows and columns simultaneously by labels or conditions we can use the method loc with the syntax\ndf.loc[ row-selection , column-selection]\nspecifying both paratmers: row-selection and column-selection. These parameters can be a condition (which generates a boolean array) or a subset of labels from the index or the column names. Let’s see an examples:\n\n# select change in glacial volume in Europe per year after 2000\ndf.loc[df['year']&gt;2000,['year','europe']]\n\n\n\n\n\n\n\n\nyear\neurope\n\n\n\n\n40\n2001\n-8.419109\n\n\n41\n2002\n-3.392361\n\n\n42\n2003\n-3.392361\n\n\n\n\n\n\n\nLet’s break it down:\n\nwe are using the df.loc[ row-selection , column-selection] syntax\nthe row-selection parameter is the condition df['year']&gt;1990, which is a boolean array saying which years are greater than 1990\nthe column-selection parameter is ['year','europe'] which is a list with the names of the two columns we are intersted in.\n\n\n\n2.5.4.2 … by position\nWhen we want to select rows and columns simultaneously by position we use the iloc method with the syntax:\ndf.iloc[ row-indices , column-indices]\nFor example,\n\n# select rows 3-7 (including 7) and columns 3 and 4\ndf.iloc[ 3:8, [3,4] ]\n\n\n\n\n\n\n\n\nalaska\nasia\n\n\n\n\n3\n0.477800\n-18.675385\n\n\n4\n-0.115332\n-18.414602\n\n\n5\n0.224762\n-14.630284\n\n\n6\n-7.174030\n-39.013695\n\n\n7\n-0.660556\n7.879589\n\n\n\n\n\n\n\nLet’s break it down:\n\nwe are using the df.iloc[ row-indices , column-indices] syntax\nthe row-indices parameter is the slice of integer indices 3:8. Remember the right endpoint (8) won’t be included.\nthe column-indices parameter is the list of integer indices 3 and 4. This means we are selecting the fourth and fifth column.\n\n\n\n\n2.5.5 Notes about loc and iloc\n\n\n\n\n\n\niloc vs. loc\n\n\n\nAt the beginning, the difference between iloc and loc can be confusing. Remember the i in iloc stands for integer-location, so this function only uses integer indexing to retrieve information from the data frames in the same way as indexing for Python lists.\nIf you want to dive deeper, this is a great discussion about the difference between iloc and loc: Stackoverflow - How are iloc and loc different?\nAnd, as always, the documentation will provide you with more information: pandas.DataFrame.loc and pandas.DataFrame.iloc.\n\n\n\n\n\n\n\n\niloc for column selection? Avoid it!\n\n\n\nWe can also access columns by position using iloc - but it is best not to if possible.\nSuppose we want to access the 10th column in the data frame - then we want to select a column by position. In this case the 10th column is the annual sea level rise data and the 10th position corresponds to the index 9. We can select this column by position using the iloc method*:\n\n# select column by position using iloc\n# the syntax is iloc[rows,columns]\n# [:,9] means \"select all rows from the 10th column\"\nannual_rise_3 = df.iloc[:,9]\nannual_rise_3.head()\n\n0    0.610010\n1    0.810625\n2    0.100292\n3   -0.085596\n4   -0.128392\nName: annual_sea_level_rise, dtype: float64\n\n\nUnless you are really looking for information about the 10th column, do not access a column by position. This is bound to break in many ways:\n\nit relies on a person correctly counting the position of a column. Even with a small dataset this can be prone to error.\nit is not explicit: if we want information about sea level rise df.annual_sea_level_rise or df['annual_sea_level_rise'] are explicitely telling us we are accessing that information. df.iloc[:,9] is obscure and uninformative.\ndatastets can get updated. Maybe a new column was added before annual_sea_level_rise, this would change the position of the column, which would make any code depending on df.iloc[:,9] invalid. Accessing by label helps reproducibility!"
  },
  {
    "objectID": "lectures/lesson-2-pandas-basics.html#resources",
    "href": "lectures/lesson-2-pandas-basics.html#resources",
    "title": "2  Pandas Fundamentals",
    "section": "2.6 Resources",
    "text": "2.6 Resources\nWhat is presented in this section is a comprehensive, but not an exhaustive list of methods to select data in pandas.DataFrames. There are so many ways to subset data to get the same result. Some of the content from this lesson is adapted from the following resources and I encourage you to read them to learn more!\n📖 Pandas getting started tutorials - How to I select a subset of a DataFrame\n📖 Pandas documentation - User Guide - Indexing and Selecting Data\n📖 Python for Data Analysis, 3E - Getting started with pandas"
  },
  {
    "objectID": "lectures/lesson-2-pandas-basics.html#acknowledgements",
    "href": "lectures/lesson-2-pandas-basics.html#acknowledgements",
    "title": "2  Pandas Fundamentals",
    "section": "2.7 Acknowledgements",
    "text": "2.7 Acknowledgements\nThe simplified glacial_loss.csv dataset was created by Dr. Allison Horst as part of her course materials on environmental data science."
  },
  {
    "objectID": "discussion-sections/discussion-sect-week-1-methods.html",
    "href": "discussion-sections/discussion-sect-week-1-methods.html",
    "title": "3  Notebook Setup",
    "section": "",
    "text": "4 References\nRachel King, Jenna Braun, Michael Westphal, & CJ Lortie. (2023). Compiled occurrence records for prey items of listed species found in California drylands with associated environmental data. Knowledge Network for Biocomplexity. doi:10.5063/F1VM49RH.\nLortie, C. J., Braun, J., King, R., & Westphal, M. (2023). The importance of open data describing prey item species lists for endangered species. Ecological Solutions and Evidence, 4(2), e12251. https://doi.org/10.1002/2688-8319.12251"
  },
  {
    "objectID": "discussion-sections/discussion-sect-week-1-methods.html#archive-exploration",
    "href": "discussion-sections/discussion-sect-week-1-methods.html#archive-exploration",
    "title": "3  Notebook Setup",
    "section": "3.1 1. Archive exploration",
    "text": "3.1 1. Archive exploration\nFor many datasets, data exploration begins at the data repository. Take some time to look through the dataset’s description in KNB. Discuss the following questions with your team:\n\nDoes this dataset come with an associated metadata file?\nIs this data collected in-situ by the authors or is it a synthesis of multiple datasets?\nDoes the dataset contain sensitive data?\nDuring what time frame were the observations in the dataset collected?"
  },
  {
    "objectID": "discussion-sections/discussion-sect-week-1-methods.html#xml-metadata-exploration",
    "href": "discussion-sections/discussion-sect-week-1-methods.html#xml-metadata-exploration",
    "title": "3  Notebook Setup",
    "section": "3.2 2. .xml metadata exploration",
    "text": "3.2 2. .xml metadata exploration\nYou may have noticed two metadata files: Compiled_occurrence_records_for_prey_items_of.xml and metadata_arth_occurrences.csv. Download both of them.\n\nNotice the .xml document file type is EML which stands for EML: Ecological Metadata Language.\nOpen the .xml file: there’s a lot going on. This is a machine-readable file that has metadata about the whole dataset. You can proably identify some items like title and creators."
  },
  {
    "objectID": "discussion-sections/discussion-sect-week-1-methods.html#csv-metadata-exploration",
    "href": "discussion-sections/discussion-sect-week-1-methods.html#csv-metadata-exploration",
    "title": "3  Notebook Setup",
    "section": "3.3 3. .csv metadata exploration",
    "text": "3.3 3. .csv metadata exploration\nBack in your notebook, import the pandas package using standard abbreviation in a code cell. Then follow these steps to read in the metadata csv using the pandas.read_csv() function:\n\nNavigate to the data package site and copy the URL to access the arth_occurrences_with_env csv file. To copy the URL:\n\n\nhover over the Download button –&gt; right click –&gt; “Copy Link”.\n\n\nRead in the data from the URL using the pd.read_csv() function like this:\n# read in data\npd.read_csv('the URL goes here')\nTake a minute to look at the descriptions for the columns.\n\nNote: Not all datasets have column descriptions in a csv file. Often they come with a doc or txt file with information."
  },
  {
    "objectID": "discussion-sections/discussion-sect-week-1-methods.html#data-loading",
    "href": "discussion-sections/discussion-sect-week-1-methods.html#data-loading",
    "title": "3  Notebook Setup",
    "section": "3.4 4. Data loading",
    "text": "3.4 4. Data loading\n\nFollow steps (a) and (b) from the previous exercise to read in the drylands prey data file arth_occurrences_with_env.csv using pd.read_csv(). Store the dataframe to a variable called prey like this:\n\n# read in data\nprey = pd.read_csv('the URL goes here')\n\nUse a Python function to see what is the type of the prey variable.\n\n\nCHECK IN WITH YOUR TEAM\n\n\nMAKE SURE YOU’VE ALL SUCCESSFULLY ACCESSED THE DATA BEFORE CONTINUING"
  },
  {
    "objectID": "discussion-sections/discussion-sect-week-1-methods.html#look-at-your-data",
    "href": "discussion-sections/discussion-sect-week-1-methods.html#look-at-your-data",
    "title": "3  Notebook Setup",
    "section": "3.5 5. Look at your data",
    "text": "3.5 5. Look at your data\n\nRun prey in a cell. What do you notice in the columns section?\nTo see all the column names in the same display we need to set a pandas option. Run the following command and then look at the prey data again:\n\npd.set_option(\"display.max.columns\",None)"
  },
  {
    "objectID": "discussion-sections/discussion-sect-week-1-methods.html#pd.dataframe-preliminary-exploration",
    "href": "discussion-sections/discussion-sect-week-1-methods.html#pd.dataframe-preliminary-exploration",
    "title": "3  Notebook Setup",
    "section": "3.6 6. pd.DataFrame preliminary exploration",
    "text": "3.6 6. pd.DataFrame preliminary exploration\nRun each of the following methods for prey in a different cell and write a brief description of what they do as a comment:\n\nhead()\ntail()\ninfo()\nnunique()\n\nFor example:\n# head()\n# returns the first five rows of the data frame\nprey.head()\nIf you’re not sure about what the method does, try looking it up in the pandas.DataFrame documentation.\n\nCheck the documentation for head(). If this function has any default parameters, change them to get a different output.\n\nPrint each of the following attributes of prey in a different cell and write a brief explanation of what they do as a comment:\n\nshape\ncolumns\ndtypes\n\nIf you’re not sure about what info is the attribute showing, try looking it up in the pandas.DataFrame documentation."
  },
  {
    "objectID": "discussion-sections/discussion-sect-week-1-methods.html#for-fun",
    "href": "discussion-sections/discussion-sect-week-1-methods.html#for-fun",
    "title": "3  Notebook Setup",
    "section": "3.7 7. For fun",
    "text": "3.7 7. For fun\nChange the column names of institutionCode and datasetKey to institution_code and dataset_key, respectively. HINT: yesterday’s class."
  }
]