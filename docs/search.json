[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Welcome!\nWelcome to the course materials for EDS 220 - Working with Environmental Datasets! This course is part of the UCSB Masters in Environmental Data Science.\nIn this website you will find the materials covered during lectures and discussion sections for the Fall 2023 term. This site will be under construction throughout the course."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "About",
    "text": "About\nThis hands-on course explores widely used environmental data formats and Python libraries for analyzing diverse environmental data. Students will gain experience working with popular open data repositories and cloud platforms to source and analyze real-world environmental datasets. The course will also serve as an introduction to Python programming and provide opportunities to practice effective communication of the strengths and weaknesses of studentsâ€™ data products and analyses."
  },
  {
    "objectID": "index.html#instruction-team",
    "href": "index.html#instruction-team",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Instruction Team",
    "text": "Instruction Team\n\nInstructor\nCarmen Galaz GarcÃ­a (she/her/hers)\nE-mail: galaz-garcia@nceas.ucsb.edu\nOffice hour: Wednesday 3:30-4:30 pm, NCEAS classroom\nBest way to contact me: e-mail\n\n\nTA\nYutian Fang (she/her/hers)\nE-mail: yutianfang@bren.ucsb.edu\nOffice Hours: Tuesday, 4-5 pm, Deckerâ€™s deck (Bren school).\nBest way to contact me: email"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Syllabus",
    "text": "Syllabus\nClick here to access the syllabus."
  },
  {
    "objectID": "index.html#calendar",
    "href": "index.html#calendar",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Calendar",
    "text": "Calendar\nThe following is our ideal calendar, the course content and calendar may be subject to change as the course progresses."
  },
  {
    "objectID": "index.html#about-this-website",
    "href": "index.html#about-this-website",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "About this website",
    "text": "About this website\nThis website is created with Quarto and is published through GitHub pages. Click here to go to the websiteâ€™s GitHub repository."
  },
  {
    "objectID": "index.html#contribute",
    "href": "index.html#contribute",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Contribute",
    "text": "Contribute\nIf you have suggestions on how to correct, improve, or expand these notes, please feel free to email galaz-garcia@nceas.ucsb.edu or file a GitHub issue."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "License",
    "text": "License\nAll content in this book is licensed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license."
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#numpy",
    "href": "lectures/lesson-1-python-review.html#numpy",
    "title": "1Â  Python Review",
    "section": "1.1 numpy",
    "text": "1.1 numpy\nNumPy is one of the core packages for numerical computing in Python. Many of the packages we will use in this course use NumPyâ€™s arrays as their building blocks. Additionally, numpy objects have been optimized for processing, so computations on them are really fast and use less memory than doing the equivalent using base Python.\nIn this lesson we will use numpy to review some core concepts in Python youâ€™re already familiar with.\nFirst, letâ€™s start by importing the library:\n\nimport numpy as np"
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#variables",
    "href": "lectures/lesson-1-python-review.html#variables",
    "title": "1Â  Python Review",
    "section": "1.2 Variables",
    "text": "1.2 Variables\nWe can think of a variable as a name we assign to a particular object in Python. For example:\n\n# assign a small array to variable a\na = np.array([[1,1,2],[3,5,8]])\n\nWhen we run the cell, we store the variables and their value. We can view a variableâ€™s value in two ways:\n\nrunning a cell with the variable name\nusing the print function to print the value\n\n\n# show the value\na\n\narray([[1, 1, 2],\n       [3, 5, 8]])\n\n\n\n# print the value \nprint(a)\n\n[[1 1 2]\n [3 5 8]]\n\n\n\n\n\n\n\n\nR and Python\n\n\n\nIn Python we use the equal sign = to assign values to variables in the same way the left-arrow <- is used in R.\n\n\n\n\n\n\n\n\nNaming Variables\n\n\n\nThere are many ways of constructing multi-word variable names. In this course we will name variables using snake_case, where words are all in small caps and separated by underscores (ex: my_variable). This is the naming convention suggested by the Style Guide for Python Code."
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#variables-and-objects",
    "href": "lectures/lesson-1-python-review.html#variables-and-objects",
    "title": "1Â  Python Review",
    "section": "1.3 Variables and Objects",
    "text": "1.3 Variables and Objects\nYou will often encounter the word object in Python documentation and tutorials. Informally speaking, an object is a bundle of properties and actions about something specific. For example, an object could represent a data frame with properties such as number of rows, names of columns, and date created, and actions suchs as selecting a specific row or adding a new column.\nA variable is the name we give a specific object, and the same object can be referenced by different variables. An analogy for this is the following: the Sun (object) is called â€œsolâ€ in Spanish and â€œsoleilâ€ in French, so two different names (variables) represent the same object. You can read more technical details about the difference between objects and variables in Python here.\nIn practice, we can often use the word variable and object interchangeably. I want to bring up what objects are so youâ€™re not caught off-guard with vocabulary youâ€™ll often encounter in the documentation, StackExchange, etc. Weâ€™ll often use the word object too (for example, in the next subsection!)."
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#types",
    "href": "lectures/lesson-1-python-review.html#types",
    "title": "1Â  Python Review",
    "section": "1.4 Types",
    "text": "1.4 Types\nEvery object in Python has a type, the type tells us what kind of object it is. We can also call the type of an object, the class of an object (so class and type both mean what kind of object we have).\nWe can see the type/class of a variable/object by using the type function:\n\nprint(a)\ntype(a)\n\n[[1 1 2]\n [3 5 8]]\n\n\nnumpy.ndarray\n\n\nThe numpy.ndarray is the core object/data type in the NumPy pakcage. We can check the type of an entry in the array by indexing:\n\nprint(a[0,0])\ntype(a[0,0])\n\n1\n\n\nnumpy.int64\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nHow would you access the value 5 in the array a? Remember indexing in Python starts from 0!\n\n\nNotice the type of the value 1 in the array is numpy.int64 and not just the core Python integer type int. The NumPy type numpy.int64 is telling us 1 is an integer stored as a 64-bit number. NumPy has its own data types to deal with numbers depending on memory storage and floating point precision, click here to know see all the types.\nSince â€œeverything in Python is an objectâ€ and every object has a class, we will interact with SO MANY classes in this course. Often, knowing the type of an object is the first step to finding information to code what you want!"
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#functions",
    "href": "lectures/lesson-1-python-review.html#functions",
    "title": "1Â  Python Review",
    "section": "1.5 Functions",
    "text": "1.5 Functions\nprint was our first example of a Python function. Functions take in a set of arguments, separated by commas, and use those arguments to create some output. There are several built-in funcions in Python, most of them are for interacting with the basic Python data types. You can see a list of them here.\n\n\n\n\n\n\nArgument or Parameter?\n\n\n\nWe can interchangeably say arguments or parameters. You will see argument more often in the documentation.\n\n\nWe can ask for information about a function by executing ? followed by the function name:\n\n?print\n\n\nThe first line is always the function showing all of its arguments in parenthesis. Then there is a short description of what the function does. And finally a list of the arguments and a brief explanation about each of them.\nYou can see there are different types of arguments inside the parenthesis. Roughly speaking, a function has two types of arguments:\n\nnon-optional arguments: arguments you need to specify for the function to do something, and\noptional arguments: arguments that are pre-filled with a default value by the function, but you can override them. Optional arguments appear inside the parenthesis () in the form optional_argument = default_value.\n\nExample:\nend is an argument in print with the default value a new line. We can change this argument so that finishes the line with ^_^ instead:\n\n# notice we had always used print withough specifying any value for the `end` argument\nprint('I am changing the default end argument of the print function', end=' ^_^')\n\nI am changing the default end argument of the print function ^_^"
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#attributes-methods",
    "href": "lectures/lesson-1-python-review.html#attributes-methods",
    "title": "1Â  Python Review",
    "section": "1.6 Attributes & Methods",
    "text": "1.6 Attributes & Methods\nAn object in Python has attributes and methods. An attribute is a property of the object, some piece of information about it. A method is a procedure associated with an object, so it is an action where the main ingredient is the object.\nFor example, these could be some attributes and methods a class cat:\n\n\n\n.\n\n\nMore formally, a method is a function that acts on the object it is part of.\nWe can access a variableâ€™s attributes and methods by adding a period . at the end of the variableâ€™s name. So we would write variable.variable_method() or variable.variable_attribute.\n\n\n\n\n\n\nCheck-in\n\n\n\nSuppose we have a class fish, make a diagram similar to the cat class diagram showing 3 attributes for the class and 3 methods.\n\n\nExample\nNumPy arrays have many methods and attributes. Letâ€™s see some concrete examples.\n\n# define a 3x3 array\nvar = np.array([[1,2,3],[4,5,6],[7,8,9]])\nvar\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\n# T is an example of attribute, it returns the transpose of var\nprint(var.T)\nprint(type(var.T))\n\n[[1 4 7]\n [2 5 8]\n [3 6 9]]\n<class 'numpy.ndarray'>\n\n\n\n# shape, another attribute, tells us the shape of the array (3x3)\nprint(var.shape)\nprint(type(var.shape))\n\n(3, 3)\n<class 'tuple'>\n\n\n\n# ndim is an attribute holding the number of array dimensions\nprint(var.ndim)\nprint(type(var.ndim))\n\n2\n<class 'int'>\n\n\nNotice these attributes can have many different data types. Here we saw tuples and int, two of the core Python classes, and also a numpy array as attributes of var.\nNow some examples of methods:\n\n# the tolist method returns the array as a nested list of scalars\nvar.tolist()\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n\n# the min method returns the minimum value in the array along an axis\nvar.min(axis=0)\n\narray([1, 2, 3])\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWe can also call the min method without any parameters:\n\nvar.min()\n\n1\n\n\nWhat kind of parameter is axis in our previous call of the var method?\n\n\nRemember, methods are functions associated to an object. We can check this!\n\ntype(var.tolist)\n\nbuiltin_function_or_method\n\n\n\ntype(var.min)\n\nbuiltin_function_or_method\n\n\nYou can see a complete list of NumPy arrayâ€™s methods and attributes in the documentation.\n\n\n\n\n\n\nR and Python\n\n\n\nIn R we donâ€™t use methods within an object. Rather, functions are extrinsic to (outside) the objects they are acting on. In R, for example, there would be two separate items: the variable var and a separate function min that gets var as a parameter:\n# this is R code\nvar <- array(c(1,4,7,2,5,8,3,6,9), dim =c(3,3))\nmin(var)\nUsing the pipe operator %>% in Râ€™s tidyverse is closer to the dot . in Python:\n# this is R code\nvar <- array(c(1,4,7,2,5,8,3,6,9), dim =c(3,3))\nvar %>% min()\nWhat happens here is that the pipe %>% is passing var to the min() function as its first argument. This is essentially what happens in Python when a function is a method of a class:\n# this is Python code\nvar = np.array([[1,2,3],[4,5,6],[7,8,9]])\nvar.min()\nWhen working in Python, remember that methods are functions that are part of an object and a method uses the object it is part of to produce some information."
  },
  {
    "objectID": "lectures/lesson-2-series-dataframes.html#pandas",
    "href": "lectures/lesson-2-series-dataframes.html#pandas",
    "title": "2Â  pd.Series and pd.DataFrames",
    "section": "2.1 pandas",
    "text": "2.1 pandas\npandas is a Python package to wrangle and analyze tabular data. It is built on top of NumPy and has become the core tool for doing data analysis in Python.\nThe convention to import it is:\n\nimport pandas as pd\n\n# we will also import numpy \nimport numpy as np\n\nThere is so much to learn about pandas. While we wonâ€™t be able to cover every single functionality of this package in the next three lecutres, the goal is to get you started with the basic tools for data wrangling and give you a solid basis on which you can explore further."
  },
  {
    "objectID": "lectures/lesson-2-series-dataframes.html#series",
    "href": "lectures/lesson-2-series-dataframes.html#series",
    "title": "2Â  pd.Series and pd.DataFrames",
    "section": "2.2 Series",
    "text": "2.2 Series\nThe first core data structure of pandas is the series. A series is a one-dimensional array of indexed data. A pandas.Series having an index is the main difference between a pandas.Series and a numpy array. See the difference:\n\n# a numpy array\n# np.random.randn returns values from the std normal distribution\narr = np.random.randn(4) \nprint(type(arr))\nprint(arr, \"\\n\")\n\n# a pandas series made from the previous array\ns = pd.Series(arr)\nprint(type(s))\nprint(s)\n\n<class 'numpy.ndarray'>\n[-1.17272646 -1.24909307 -0.71114546  0.56909155] \n\n<class 'pandas.core.series.Series'>\n0   -1.172726\n1   -1.249093\n2   -0.711145\n3    0.569092\ndtype: float64\n\n\n\n2.2.1 Creating a pandas.Series\nThe basic method to create a pandas.Series is to call\ns = pd.Series(data, index=index)\nThe data parameter can be:\n\na numpy array or a list\na Python dictionary\na number\n\nThe index parameter is a list of index labels.\nFor now, we will create a pandas.Series from a numpy array or list. To use this method we need to pass a numpy array (or a list of objects that can be converted to NumPy types) as data and a list of indices of the same length as data.\n\n# a Series from a numpy array \npd.Series(np.arange(3), index=['a','b','c'])\n\na    0\nb    1\nc    2\ndtype: int64\n\n\nThe index parameter is optional. If we donâ€™t include it, the default is to make the index equal to [0,...,len(data)-1]. For example:\n\n# a Series from a list of strings with default index\npd.Series(['EDS 220', 'EDS 222', 'EDS 223', 'EDS 242'])\n\n0    EDS 220\n1    EDS 222\n2    EDS 223\n3    EDS 242\ndtype: object\n\n\n\n2.2.1.1 From a dictionary\nRemember a dictionary is a set of key-value pairs. If we create a pandas.Series via a dictionary the keys will become the index and the values the corresponding data.\n\n# construct dictionary\nd = {'a':0, 'b':1, 'c':2}\n\n# initialize a sries using a dictionary\npd.Series(d)\n\na    0\nb    1\nc    2\ndtype: int64\n\n\n\n\n2.2.1.2 From a number\nIf we only provide a number as the data for the series, we need to provide an index. The number will be repeated to match the length of the index.\n\npd.Series(3.0, index = ['A', 'B', 'C'])\n\nA    3.0\nB    3.0\nC    3.0\ndtype: float64\n\n\n\n\n\n2.2.2 Simple operations\nArithmetic operations work on series and also most NumPy functions. For example:\n\n# define a series\ns = pd.Series([98,73,65],index=['Andrea', 'Beth', 'Carolina'])\n\n# divide each element in series by 10\nprint(s /10, '\\n')\n\n# take the exponential of each element in series\nprint(np.exp(s), '\\n')\n\n# notice this doesn't change the values of our series\nprint(s)\n\nAndrea      9.8\nBeth        7.3\nCarolina    6.5\ndtype: float64 \n\nAndrea      3.637971e+42\nBeth        5.052394e+31\nCarolina    1.694889e+28\ndtype: float64 \n\nAndrea      98\nBeth        73\nCarolina    65\ndtype: int64\n\n\nWe can also produce new pandas.Series with True/False values indicating whether the elements in a series satisfy a condition or not:\n\ns > 10\n\nAndrea      True\nBeth        True\nCarolina    True\ndtype: bool\n\n\nThis kind of simple conditions on pandas.Series will be key when we are selecting data from data frames.\n\n\n\n2.2.3 Attributes & Methods\npandas.Series have many attributes and methods, you can see a full list in the pandas documentation. For now we will cover two examples that have to do with identifying missing values.\npandas represents a missing or NA value with NaN, which stands for not a number. Letâ€™s construct a small series with some NA values:\n\n# series with NAs in it\ns = pd.Series([1, 2, np.NaN, 4, np.NaN])\n\nA pandas.Series has an attribute called hasnans that returns True if there are any NaNs:\n\n# check if series has NAs\ns.hasnans\n\nTrue\n\n\nThen we might be intersted in knowing which elements in the series are NAs. We can do this using the isna method:\n\ns.isna()\n\n0    False\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n\n\nWe can see the ouput is a pd.Series of boolean values indicating if an element in the row at the given index is NA (True = is NA) or not (False = not NA).\n\n\n\n\n\n\nmoving on\n\n\n\nThereâ€™s much more to say about pandas.Series, but this is enought to get us going. At this point, we mainly want to know about pandas.Series because pandas.Series are the columns of pandas.DataFrames.\n\n\n\n\n\n\n\n\nslicing with loc\n\n\n\nNotice that when use slicing with loc we get both the start and the end of the indices we indicated. This is different to slicing in numpy arrays or lists where we do not get the element at the end of the slice. Compare the following:\n\nx = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(x)\n\n# slicing will return elements at indices 2 trhough 4 (inclusive)\nx[2:5]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n[2, 3, 4]\n\n\n\n# define a np array with integers from 0 to 9\ny = np.arange(10)\nprint(y)\n\n# slicing will return elements at indices 2 trhough 4 (inclusive)\ny[2:5]\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\narray([2, 3, 4])\n\n\n\n z = pd.Series(y)\n print(z)\n\n# slicing will return elements with index labels 2 through 5 (inclusive)\n z.loc[2:5]\n\n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\ndtype: int64\n\n\n2    2\n3    3\n4    4\n5    5\ndtype: int64"
  },
  {
    "objectID": "lectures/lesson-2-series-dataframes.html#data-frames",
    "href": "lectures/lesson-2-series-dataframes.html#data-frames",
    "title": "2Â  pd.Series and pd.DataFrames",
    "section": "2.3 Data Frames",
    "text": "2.3 Data Frames\nThe Data Frame is the most used pandas object. It represents tabular data and we can think of it as a spreadhseet. Each column of a pandas.DataFrame is a pandas.Series.\n\n2.3.1 Creating a pandas.DataFrame\nThere are many ways of creating a pandas.DataFrame.\nWe already mentioned each column of a pandas.DataFrame is a pandas.Series. In fact, the pandas.DataFrame is a dictionary of pandas.Series, with each column name being the key and the column values being the keyâ€™s value. Thus, we can create a pandas.DataFrame in this way:\n\n# initialize dictionary with columns' data \nd = {'col_name_1' : pd.Series(np.arange(3)),\n     'col_name_2' : pd.Series([3.1, 3.2, 3.3]),\n     }\n\n# create data frame\ndf = pd.DataFrame(d)\ndf\n\n\n\n\n\n  \n    \n      \n      col_name_1\n      col_name_2\n    \n  \n  \n    \n      0\n      0\n      3.1\n    \n    \n      1\n      1\n      3.2\n    \n    \n      2\n      2\n      3.3\n    \n  \n\n\n\n\nWe can change the index and column names by changing the index and columns attributes in the data frame.\n\n# print original index\nprint(df.index)\n\n# change the index\ndf.index = ['a','b','c']\ndf\n\nRangeIndex(start=0, stop=3, step=1)\n\n\n\n\n\n\n  \n    \n      \n      col_name_1\n      col_name_2\n    \n  \n  \n    \n      a\n      0\n      3.1\n    \n    \n      b\n      1\n      3.2\n    \n    \n      c\n      2\n      3.3\n    \n  \n\n\n\n\n\n# print original column names\nprint(df.columns)\n\n# change column names \ndf.columns = ['C1','C2']\ndf\n\nIndex(['col_name_1', 'col_name_2'], dtype='object')\n\n\n\n\n\n\n  \n    \n      \n      C1\n      C2\n    \n  \n  \n    \n      a\n      0\n      3.1\n    \n    \n      b\n      1\n      3.2\n    \n    \n      c\n      2\n      3.3"
  },
  {
    "objectID": "lectures/lesson-3-pandas-subsetting.html#subsetting-a-pandas.dataframe",
    "href": "lectures/lesson-3-pandas-subsetting.html#subsetting-a-pandas.dataframe",
    "title": "3Â  Subsetting",
    "section": "3.1 Subsetting a pandas.DataFrame",
    "text": "3.1 Subsetting a pandas.DataFrame\nLike itâ€™s often the case when working with pandas, there are many ways in which we can subset a data frame. We will review the core methods to do this. \nFor all examples we will use simplified data (glacial_loss.csv) from the National Snow and Ice Data Center (Original dataset). The column descriptions are:\n\nyear: â€‹calendar year\neurope - antarctica: â€‹change in glacial volume (km3â€‹ â€‹) in each region that year\nglobal_glacial_volume_change: â€‹cumulativeâ€‹ global glacial volume change (km3), starting in 1961\nannual_sea_level_rise: â€‹annual rise in sea level (mm)\ncumulative_sea_level_rise:â€‹ cumulative rise in sea level (mm) since 1961\n\nFirst, we read-in the file and get some baisc information about this data frame:\n\n# import pandas\nimport pandas as pd\n\n# read in file\ndf = pd.read_csv('data/lesson-1/glacial_loss.csv')\n\n# see the first five rows\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      0\n      1961\n      -5.128903\n      -108.382987\n      -18.721190\n      -32.350759\n      -14.359007\n      -4.739367\n      -35.116389\n      -220.823515\n      0.610010\n      0.610010\n    \n    \n      1\n      1962\n      5.576282\n      -173.252450\n      -24.324790\n      -4.675440\n      -2.161842\n      -13.694367\n      -78.222887\n      -514.269862\n      0.810625\n      1.420635\n    \n    \n      2\n      1963\n      -10.123105\n      -0.423751\n      -2.047567\n      -3.027298\n      -27.535881\n      3.419633\n      3.765109\n      -550.575640\n      0.100292\n      1.520927\n    \n    \n      3\n      1964\n      -4.508358\n      20.070148\n      0.477800\n      -18.675385\n      -2.248286\n      20.732633\n      14.853096\n      -519.589859\n      -0.085596\n      1.435331\n    \n    \n      4\n      1965\n      10.629385\n      43.695389\n      -0.115332\n      -18.414602\n      -19.398765\n      6.862102\n      22.793484\n      -473.112003\n      -0.128392\n      1.306939\n    \n  \n\n\n\n\n\n# get column names\ndf.columns\n\nIndex(['year', 'europe', 'arctic', 'alaska', 'asia', 'north_america',\n       'south_america', 'antarctica', 'global_glacial_volume_change',\n       'annual_sea_level_rise', 'cumulative_sea_level_rise'],\n      dtype='object')\n\n\n\n# check the data types of each column\ndf.dtypes\n\nyear                              int64\neurope                          float64\narctic                          float64\nalaska                          float64\nasia                            float64\nnorth_america                   float64\nsouth_america                   float64\nantarctica                      float64\nglobal_glacial_volume_change    float64\nannual_sea_level_rise           float64\ncumulative_sea_level_rise       float64\ndtype: object\n\n\n\n# data frame's shape: output is a tuple (# rows, # columns)\ndf.shape\n\n(43, 11)\n\n\n\n3.1.1 Selecting a single columnâ€¦\n\n3.1.1.1 â€¦by column name\nThis is the simplest case for selecting data. Suppose we are interested in the annual sea level rise. Then we can access that single column in this way:\n\n# seelect a single column by using square brackets []\nannual_rise = df['annual_sea_level_rise']\n\n# check the type of the ouput\nprint(type(annual_rise))\n\nannual_rise.head()\n\n<class 'pandas.core.series.Series'>\n\n\n0    0.610010\n1    0.810625\n2    0.100292\n3   -0.085596\n4   -0.128392\nName: annual_sea_level_rise, dtype: float64\n\n\nSince we only selected a single column the output is a pandas.Series.\n\n\n\n\n\n\npd.DataFrame = dictionary of columns\n\n\n\nRemember we can think of a pandas.DataFrame as a dictionary of its columns? Then we can access a single column using the column name as the key, just like we would do in a dictionary. That is the we just used: df['column_name'].\n\n\nThis is an example of selecting by label, which means we want to select data from our data frame using the names of the columns, not their position.\n\n\n3.1.1.2 â€¦ with attribute syntax\nWe can also access a single column by using attribute syntax:\n\nannual_rise_2 = df.annual_sea_level_rise\nannual_rise_2.head()\n\n0    0.610010\n1    0.810625\n2    0.100292\n3   -0.085596\n4   -0.128392\nName: annual_sea_level_rise, dtype: float64\n\n\n\n\n\n3.1.2 Selecting multiple columnsâ€¦\n\n3.1.2.1 â€¦ using a list of column names\nThis is another example of selecting by labels. We just need to pass a list with the column names to the square brackets []. For example, say we want to look at the change in glacial volume in Europe and Asia, then we can select those columns like this:\n\n# select columns with names \"europe\" and \"asia\"\neurope_asia = df[['europe','asia']]\n\nNotice there are double square brackets. This is because we are passing the list of names ['europe','asia'] to the selection brakcets [].\n\n# check the type of the resulting selection\nprint(type(europe_asia))\n\n# check the shape of the selection\nprint((europe_asia.shape))\n\n<class 'pandas.core.frame.DataFrame'>\n(43, 2)\n\n\n\n\n3.1.2.2 â€¦ using a slice\nYet another example of selecting by label! In this case we will use the loc selection. The general syntax is\ndf.loc[ row-selection , column-selection]\nwhere row-selection and column-selection are the rows and columns we want to subset from the data frame.\nLetâ€™s start by a simple example, where we want to select a slice of columns, say the change in glacial volume per year in all regions. This corresponds to all columns between europe and antarctica.\n\n# select all columns between 'arctic' and 'antarctica'\nall_regions = df.loc[:,'europe':'antarctica']\nall_regions.head()\n\n\n\n\n\n  \n    \n      \n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n    \n  \n  \n    \n      0\n      -5.128903\n      -108.382987\n      -18.721190\n      -32.350759\n      -14.359007\n      -4.739367\n      -35.116389\n    \n    \n      1\n      5.576282\n      -173.252450\n      -24.324790\n      -4.675440\n      -2.161842\n      -13.694367\n      -78.222887\n    \n    \n      2\n      -10.123105\n      -0.423751\n      -2.047567\n      -3.027298\n      -27.535881\n      3.419633\n      3.765109\n    \n    \n      3\n      -4.508358\n      20.070148\n      0.477800\n      -18.675385\n      -2.248286\n      20.732633\n      14.853096\n    \n    \n      4\n      10.629385\n      43.695389\n      -0.115332\n      -18.414602\n      -19.398765\n      6.862102\n      22.793484\n    \n  \n\n\n\n\nNotice two things:\n\nwe used the colon : as the row-selection parameter, which means â€œselect all the rowsâ€\nthe slice of the data frame we got includes both endpoints of the slice 'arctic':'antarctica'. In other words we get the europe column and the antarctica column. This is different from how slicing works in base Python and NumPy, where the endpoint is not included.\n\n\n\n\n3.1.3 Selecting rowsâ€¦\nNow that we are familiar with some methods for selecting columns, letâ€™s move on to selecting rows.\n\n3.1.3.1 â€¦ using a condition\nSelecting which rows satisfy a particular condition is, in my experience, the most usual kind of row subsetting. The general syntax for this type of selection is df[condition_on_rows]. For example, suppose we are intersted in all data after 1996. We can select those rows in this way:\n\n# select all rows with year > 1996\nafter_96 = df[df['year']>1996]\nafter_96\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      36\n      1997\n      -13.724106\n      -24.832246\n      -167.229145\n      -34.406403\n      -27.680661\n      -38.213286\n      -20.179090\n      -4600.686013\n      0.909625\n      12.709077\n    \n    \n      37\n      1998\n      -13.083338\n      -110.429302\n      -107.879027\n      -58.115702\n      30.169987\n      -3.797978\n      -48.129928\n      -4914.831966\n      0.867807\n      13.576884\n    \n    \n      38\n      1999\n      -8.039555\n      -64.644068\n      -87.714653\n      -26.211723\n      5.888512\n      -8.038630\n      -40.653001\n      -5146.368231\n      0.639603\n      14.216487\n    \n    \n      39\n      2000\n      -17.008590\n      -96.494055\n      -44.445000\n      -37.518173\n      -29.191986\n      -2.767698\n      -58.873830\n      -5435.317175\n      0.798202\n      15.014688\n    \n    \n      40\n      2001\n      -8.419109\n      -145.415483\n      -55.749505\n      -35.977022\n      -0.926134\n      7.553503\n      -86.774675\n      -5764.039931\n      0.908074\n      15.922762\n    \n    \n      41\n      2002\n      -3.392361\n      -48.718943\n      -87.120000\n      -36.127226\n      -27.853498\n      -13.484593\n      -30.203960\n      -6013.225500\n      0.688358\n      16.611120\n    \n    \n      42\n      2003\n      -3.392361\n      -48.718943\n      -67.253634\n      -36.021991\n      -75.066475\n      -13.223430\n      -30.203960\n      -6289.640976\n      0.763579\n      17.374699\n    \n  \n\n\n\n\nLetâ€™s break down what is happening here. In this case the condition for our rows is df['year']>1996, this checks which rows have a value greater than 1996 in the year column. Letâ€™s see this explicitely:\n\n# check the type of df['year']>1996\nprint(type(df['year']>1996))\n\ndf['year']>1996\n\n<class 'pandas.core.series.Series'>\n\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14    False\n15    False\n16    False\n17    False\n18    False\n19    False\n20    False\n21    False\n22    False\n23    False\n24    False\n25    False\n26    False\n27    False\n28    False\n29    False\n30    False\n31    False\n32    False\n33    False\n34    False\n35    False\n36     True\n37     True\n38     True\n39     True\n40     True\n41     True\n42     True\nName: year, dtype: bool\n\n\nThe output is a pandas.Series with boolean values (True or False) indicating which rows satisfy the condition year>1996. When we pass such a series of boolean values to the selection brackets [] we keep only those rows with a True value.\nHereâ€™s another example of using a condition. Suppose we want to look at data from years 1970 to 1979. One way of doing this is to use the in operator in our condition:\n\nseventies = df[df['year'].isin(range(1970,1980))]\nseventies\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      9\n      1970\n      -6.452316\n      -24.494667\n      -0.125296\n      -36.120199\n      11.619790\n      11.636911\n      4.400377\n      -999.018177\n      0.110225\n      2.759719\n    \n    \n      10\n      1971\n      0.414711\n      -42.904189\n      28.103328\n      -8.702938\n      -9.964542\n      1.061299\n      -6.735536\n      -1038.104459\n      0.107973\n      2.867692\n    \n    \n      11\n      1972\n      -5.144729\n      -27.004031\n      -22.143350\n      -40.883357\n      32.363730\n      -14.968034\n      -6.223849\n      -1122.885506\n      0.234202\n      3.101894\n    \n    \n      12\n      1973\n      4.081090\n      9.839444\n      22.985188\n      -31.432594\n      -20.883232\n      2.103649\n      10.539823\n      -1125.677743\n      0.007713\n      3.109607\n    \n    \n      13\n      1974\n      1.545615\n      -40.126998\n      -29.517874\n      -43.861622\n      -23.991402\n      -21.338825\n      4.419343\n      -1279.964287\n      0.426206\n      3.535813\n    \n    \n      14\n      1975\n      7.431192\n      -32.410467\n      -44.094084\n      -43.357442\n      -30.858810\n      -2.368842\n      -7.775315\n      -1434.818037\n      0.427773\n      3.963586\n    \n    \n      15\n      1976\n      3.986753\n      21.686639\n      -28.234725\n      -67.292125\n      -12.534421\n      -19.465358\n      19.250607\n      -1518.185129\n      0.230296\n      4.193882\n    \n    \n      16\n      1977\n      4.891410\n      -33.123010\n      -5.662139\n      -62.165684\n      -15.905332\n      2.654950\n      -23.727249\n      -1652.453400\n      0.370907\n      4.564788\n    \n    \n      17\n      1978\n      8.404591\n      -77.561015\n      -12.503384\n      -22.858040\n      -31.097609\n      7.127708\n      -9.140167\n      -1791.355022\n      0.383706\n      4.948495\n    \n    \n      18\n      1979\n      3.916703\n      -88.351684\n      -63.938851\n      -49.242043\n      -12.076624\n      -17.718503\n      -9.578557\n      -2030.537848\n      0.660726\n      5.609221\n    \n  \n\n\n\n\nLetâ€™s break it down:\n\ndf['year'] is the column with the year values, a pandas.Series,\nin df['year'].isin(), we have that isin is a method for the pandas.Series and we are calling it using the dot ..\nrange(1970,1980) constructs consecutive integers from 1970 to 1979 - remember the right endopoint (1980) is not included!\ndf['year'].isin(range(1970,1980)) is then a pandas.Series of boolean values indicating which rows have year equal to 1970, â€¦, 1979.\nwhen we put df['year'].isin(range(1970,1980)) inside the selection brackets [] we obtain the rows of the data frame with year equal to 1970, â€¦, 1979.\n\n\n\n\n\n\n\nloc for row selection\n\n\n\nIt is equivalent to write\n# select rows with year<1965\ndf[df['year'] < 1965]\nand\n# select rows with year<1965 using love\ndf.loc[ df['year'] <1965 , :]\nIn the second one:\n\nwe are using the df.loc[ row-selection , column-selection] syntax\nthe row-selection parameter is the condition df['year']<1965\nthe column-selection parameter is a colon :, which indicates we want all columns for the rows we are selecting.\n\nWe prefer the first syntax when we are selecting rows and not columns since it is simpler.\n\n\n\n\n3.1.3.2 â€¦ using multiple conditions\nWe can combine multipe conditions by surrounding each one in parenthesis () and using the or operator | and the and operator &.\nor example:\n\n# select rows with \n# annual_sea_level_rise<0.5 mm OR annual_sea_level_rise>0.8 mm\n\ndf[ (df['annual_sea_level_rise']<0.5) | (df['annual_sea_level_rise']>0.8)]\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      0\n      1961\n      -5.128903\n      -108.382987\n      -18.721190\n      -32.350759\n      -14.359007\n      -4.739367\n      -35.116389\n      -220.823515\n      0.610010\n      0.610010\n    \n    \n      1\n      1962\n      5.576282\n      -173.252450\n      -24.324790\n      -4.675440\n      -2.161842\n      -13.694367\n      -78.222887\n      -514.269862\n      0.810625\n      1.420635\n    \n    \n      2\n      1963\n      -10.123105\n      -0.423751\n      -2.047567\n      -3.027298\n      -27.535881\n      3.419633\n      3.765109\n      -550.575640\n      0.100292\n      1.520927\n    \n    \n      3\n      1964\n      -4.508358\n      20.070148\n      0.477800\n      -18.675385\n      -2.248286\n      20.732633\n      14.853096\n      -519.589859\n      -0.085596\n      1.435331\n    \n    \n      4\n      1965\n      10.629385\n      43.695389\n      -0.115332\n      -18.414602\n      -19.398765\n      6.862102\n      22.793484\n      -473.112003\n      -0.128392\n      1.306939\n    \n  \n\n\n\n\nand example\n\n# select rows with cumulative_sea_level_rise>10 AND  global_glacial_volume_change<-300\ndf[ (df['cumulative_sea_level_rise']>10) & (df['global_glacial_volume_change']<-300)]\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      32\n      1993\n      16.685013\n      -73.666274\n      -43.702040\n      -65.995130\n      -33.151246\n      -20.578403\n      -20.311577\n      -3672.582082\n      0.671126\n      10.145254\n    \n    \n      33\n      1994\n      0.741751\n      -3.069084\n      -59.962273\n      -59.004710\n      -89.506142\n      -15.258449\n      -8.168498\n      -3908.977191\n      0.653025\n      10.798280\n    \n    \n      34\n      1995\n      -2.139665\n      -58.167778\n      -74.141762\n      3.500155\n      -0.699374\n      -19.863392\n      -25.951496\n      -4088.082873\n      0.494767\n      11.293047\n    \n    \n      35\n      1996\n      -6.809834\n      -4.550205\n      -74.847017\n      -67.436591\n      4.867530\n      -21.080115\n      -11.781489\n      -4271.401594\n      0.506405\n      11.799452\n    \n    \n      36\n      1997\n      -13.724106\n      -24.832246\n      -167.229145\n      -34.406403\n      -27.680661\n      -38.213286\n      -20.179090\n      -4600.686013\n      0.909625\n      12.709077\n    \n    \n      37\n      1998\n      -13.083338\n      -110.429302\n      -107.879027\n      -58.115702\n      30.169987\n      -3.797978\n      -48.129928\n      -4914.831966\n      0.867807\n      13.576884\n    \n    \n      38\n      1999\n      -8.039555\n      -64.644068\n      -87.714653\n      -26.211723\n      5.888512\n      -8.038630\n      -40.653001\n      -5146.368231\n      0.639603\n      14.216487\n    \n    \n      39\n      2000\n      -17.008590\n      -96.494055\n      -44.445000\n      -37.518173\n      -29.191986\n      -2.767698\n      -58.873830\n      -5435.317175\n      0.798202\n      15.014688\n    \n    \n      40\n      2001\n      -8.419109\n      -145.415483\n      -55.749505\n      -35.977022\n      -0.926134\n      7.553503\n      -86.774675\n      -5764.039931\n      0.908074\n      15.922762\n    \n    \n      41\n      2002\n      -3.392361\n      -48.718943\n      -87.120000\n      -36.127226\n      -27.853498\n      -13.484593\n      -30.203960\n      -6013.225500\n      0.688358\n      16.611120\n    \n    \n      42\n      2003\n      -3.392361\n      -48.718943\n      -67.253634\n      -36.021991\n      -75.066475\n      -13.223430\n      -30.203960\n      -6289.640976\n      0.763579\n      17.374699\n    \n  \n\n\n\n\n\n\n3.1.3.3 â€¦ by position\nAll the selections we have done so far have been using labels or using a condition. Sometimes we might want to select certain rows depending on their actual position in the data frame. In this case we use iloc selection with the syntax df.iloc[row-indices]. iloc stands for integer-location based indexing. Letâ€™s see some examples:\n\n# select the fifht row = index 4\ndf.iloc[4]\n\nyear                            1965.000000\neurope                            10.629385\narctic                            43.695389\nalaska                            -0.115332\nasia                             -18.414602\nnorth_america                    -19.398765\nsouth_america                      6.862102\nantarctica                        22.793484\nglobal_glacial_volume_change    -473.112003\nannual_sea_level_rise             -0.128392\ncumulative_sea_level_rise          1.306939\nName: 4, dtype: float64\n\n\n\n# select rows 23 through 30, inclduing 30\ndf.iloc[23:31]\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      23\n      1984\n      8.581427\n      -5.755672\n      -33.466092\n      -20.528535\n      -20.734676\n      -8.267686\n      -3.261011\n      -2569.339802\n      0.232609\n      7.097624\n    \n    \n      24\n      1985\n      -5.970980\n      -49.651089\n      12.065473\n      -31.571622\n      -33.833985\n      10.072906\n      -13.587886\n      -2682.857926\n      0.313586\n      7.411210\n    \n    \n      25\n      1986\n      -5.680642\n      22.900847\n      7.557447\n      -18.920773\n      -33.014743\n      -4.652030\n      30.482473\n      -2684.197632\n      0.003701\n      7.414911\n    \n    \n      26\n      1987\n      8.191477\n      12.387780\n      -24.007862\n      -41.121970\n      -48.560996\n      1.670733\n      3.130190\n      -2773.325568\n      0.246210\n      7.661120\n    \n    \n      27\n      1988\n      -11.117228\n      -31.066489\n      49.897712\n      -21.300712\n      -46.545435\n      13.460422\n      -37.986834\n      -2858.767621\n      0.236028\n      7.897148\n    \n    \n      28\n      1989\n      14.863220\n      -23.462392\n      -36.112726\n      -46.528372\n      -57.756422\n      -21.687470\n      -10.044757\n      -3041.169131\n      0.503872\n      8.401020\n    \n    \n      29\n      1990\n      -1.226009\n      -27.484542\n      -92.713339\n      -35.553433\n      -56.563056\n      -31.077022\n      -29.893352\n      -3318.220397\n      0.765335\n      9.166355\n    \n    \n      30\n      1991\n      -14.391425\n      -34.898689\n      -8.822063\n      -15.338299\n      -31.458010\n      -7.162909\n      -35.968429\n      -3467.630284\n      0.412734\n      9.579089\n    \n  \n\n\n\n\nNotice since we are back to indexing by position the right endpoint of the slice (6) is not included in the ouput.\n\n\n\n3.1.4 Selecting rows and columns simultaneouslyâ€¦\nSelecting rows and columns simultaneously can be done using loc (labels or conditions) or iloc (integer position).\n\n3.1.4.1 â€¦by labels or conditions\nWhen we want to select rows and columns simultaneously by labels or conditions we can use loc selection with the syntax\ndf.loc[ row-selection , column-selection]\nspecifying both paratmers: row-selection and column-selection. These parameters can be a condition (which generates a boolean array) or a subset of labels from the index or the column names. Letâ€™s see an examples:\n\n# select change in glacial volume in Europe per year after 2000\ndf.loc[df['year']>2000,['year','europe']]\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n    \n  \n  \n    \n      40\n      2001\n      -8.419109\n    \n    \n      41\n      2002\n      -3.392361\n    \n    \n      42\n      2003\n      -3.392361\n    \n  \n\n\n\n\nLetâ€™s break it down:\n\nwe are using the df.loc[ row-selection , column-selection] syntax\nthe row-selection parameter is the condition df['year']>1990, which is a boolean array saying which years are greater than 1990\nthe column-selection parameter is ['year','europe'] which is a list with the names of the two columns we are intersted in.\n\n\n\n3.1.4.2 â€¦ by position\nWhen we want to select rows and columns simultaneously by position we use iloc selection with the syntax:\ndf.iloc[ row-indices , column-indices]\nFor example,\n\n# select rows 3-7 (including 7) and columns 3 and 4\ndf.iloc[ 3:8, [3,4] ]\n\n\n\n\n\n  \n    \n      \n      alaska\n      asia\n    \n  \n  \n    \n      3\n      0.477800\n      -18.675385\n    \n    \n      4\n      -0.115332\n      -18.414602\n    \n    \n      5\n      0.224762\n      -14.630284\n    \n    \n      6\n      -7.174030\n      -39.013695\n    \n    \n      7\n      -0.660556\n      7.879589\n    \n  \n\n\n\n\nLetâ€™s break it down:\n\nwe are using the df.iloc[ row-indices , column-indices] syntax\nthe row-indices parameter is the slice of integer indices 3:8. Remember the right endpoint (8) wonâ€™t be included.\nthe column-indices parameter is the list of integer indices 3 and 4. This means we are selecting the fourth and fifth column.\n\n\n\n\n3.1.5 Notes about loc and iloc\n\n\n\n\n\n\niloc vs.Â loc\n\n\n\nAt the beginning, the difference between iloc and loc can be confusing. Remember the i in iloc stands for integer-location, this reminds us iloc only uses integer indexing to retrieve information from the data frames in the same way as indexing for Python lists.\nIf you want to dive deeper, this is a great discussion about the difference between iloc and loc: Stackoverflow - How are iloc and loc different?\nAnd, as always, the documentation will provide you with more information: pandas.DataFrame.loc and pandas.DataFrame.iloc.\n\n\n\n\n\n\n\n\niloc for column selection? Avoid it!\n\n\n\nWe can also access columns by position using iloc - but it is best not to if possible.\nSuppose we want to access the 10th column in the data frame - then we want to select a column by position. In this case the 10th column is the annual sea level rise data and the 10th position corresponds to the index 9. We can select this column by position using the iloc selection:\n\n# select column by position using iloc\n# the syntax is iloc[row-indices, column-indices]\n# [:,9] means \"select all rows from the 10th column\"\nannual_rise_3 = df.iloc[:,9]\nannual_rise_3.head()\n\n0    0.610010\n1    0.810625\n2    0.100292\n3   -0.085596\n4   -0.128392\nName: annual_sea_level_rise, dtype: float64\n\n\nUnless you are really looking for information about the 10th column, do not access a column by position. This is bound to break in many ways:\n\nit relies on a person correctly counting the position of a column. Even with a small dataset this can be prone to error.\nit is not explicit: if we want information about sea level rise df.annual_sea_level_rise or df['annual_sea_level_rise'] are explicitely telling us we are accessing that information. df.iloc[:,9] is obscure and uninformative.\ndatastets can get updated. Maybe a new column was added before annual_sea_level_rise, this would change the position of the column, which would make any code depending on df.iloc[:,9] invalid. Accessing by label helps reproducibility!"
  },
  {
    "objectID": "lectures/lesson-3-pandas-subsetting.html#summary",
    "href": "lectures/lesson-3-pandas-subsetting.html#summary",
    "title": "3Â  Subsetting",
    "section": "3.2 Summary",
    "text": "3.2 Summary\n\n\n\npandas.DataFrame selection flow chart"
  },
  {
    "objectID": "lectures/lesson-3-pandas-subsetting.html#resources",
    "href": "lectures/lesson-3-pandas-subsetting.html#resources",
    "title": "3Â  Subsetting",
    "section": "3.3 Resources",
    "text": "3.3 Resources\nWhat is presented in this section is a comprehensive, but not an exhaustive list of methods to select data in pandas.DataFrames. There are so many ways to subset data to get the same result. Some of the content from this lesson is adapted from the following resources and I encourage you to read them to learn more!\nðŸ“– Pandas getting started tutorials - How to I select a subset of a DataFrame\nðŸ“– Pandas documentation - User Guide - Indexing and Selecting Data\nðŸ“– Python for Data Analysis, 3E - Getting started with pandas"
  },
  {
    "objectID": "lectures/lesson-3-pandas-subsetting.html#acknowledgements",
    "href": "lectures/lesson-3-pandas-subsetting.html#acknowledgements",
    "title": "3Â  Subsetting",
    "section": "3.4 Acknowledgements",
    "text": "3.4 Acknowledgements\nThe simplified glacial_loss.csv dataset was created by Dr.Â Allison Horst as part of her course materials on environmental data science."
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#plot-method",
    "href": "lectures/lesson-4-basic-plotting.html#plot-method",
    "title": "4Â  Plotting",
    "section": "4.1 plot() method",
    "text": "4.1 plot() method\nA pandas.DataFrame has a built-in method plot() for plotting. When we call it without specifying any other parameters plot() creates one line plot for each of the columns with numeric data.\n\n# one line plot per column with numeric data - a mess\ndf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nAs we can see, this doesnâ€™t make any sense! In particular, look at the x-axis. The default for plot is to use the values of the index as the x-axis values. Letâ€™s see some examples about how to improve this situation."
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#line-plots",
    "href": "lectures/lesson-4-basic-plotting.html#line-plots",
    "title": "4Â  Plotting",
    "section": "4.2 Line plots",
    "text": "4.2 Line plots\nWe can make a line plot of one column against another by using the following syntax:\ndf.plot(x='x_values_column', y='y_values_column')\nFor example,\n\n# change in glacial volume per year in Europe\ndf.plot(x='year', y='europe')\n\n<AxesSubplot:xlabel='year'>\n\n\n\n\n\nWe can do some basic customization specifying other arguments of the plot function. Some basic ones are:\n\ntitle: Title to use for the plot.\nxlabel: Name to use for the xlabel on x-axis\nylabel: Name to use for the ylabel on y-axis\ncolor: change the color of our plot\n\nIn action:\n\ndf.plot(x='year', \n        y='europe',\n        title='Change in glacial volume per year in Europe',\n        xlabel='Year',\n        ylabel='â€‹Change in glacial volume (km3â€‹)',\n        color='green'\n        )\n\n<AxesSubplot:title={'center':'Change in glacial volume per year in Europe'}, xlabel='Year', ylabel='\\u200bChange in glacial volume (km3\\u200b)'>\n\n\n\n\n\nYou can see all the optional arguments for the plot() function in the documentation.\n\n4.2.1 Multiple line plots\nLetâ€™s say we want to graph the change in glacial volume in the Arctic and Alaska. We can do it by updating these arguments:\n\ny : a list of column names that will be plotted against x\ncolor: specify the color of each columnâ€™s line with a dictionary {'col_1' : 'color_1', 'col_2':'color_2}\n\n\ndf.plot(x='year', \n        y=['arctic', 'alaska'],\n        title = 'Change in glacial volume per year in Alaska and the Arctic',\n        xlabel='Year',\n        ylabel='â€‹Change in glacial volume (km3â€‹)',        \n        color = {'arctic':'#F48FB1',\n                 'alaska': '#AB47BC'\n                 }\n        )\n\n<AxesSubplot:title={'center':'Change in glacial volume per year in Alaska and the Arctic'}, xlabel='Year', ylabel='\\u200bChange in glacial volume (km3\\u200b)'>\n\n\n\n\n\nNotice that for specifying the colors we used a HEX code, this gives us more control over how our graph looks.\nWe can also create separate plots for each column by setting the subset to True.\n\ndf.plot(x='year', \n        y=['arctic', 'alaska'],\n        title = 'Change in glacial volume per year in Alaska and the Arctic',\n        xlabel='Year',\n        ylabel='â€‹Change in glacial volume (km3â€‹)',        \n        color = {'arctic':'#F48FB1',\n                 'alaska': '#AB47BC'\n                 },\n        subplots=True\n        )\n\narray([<AxesSubplot:xlabel='Year', ylabel='\\u200bChange in glacial volume (km3\\u200b)'>,\n       <AxesSubplot:xlabel='Year', ylabel='\\u200bChange in glacial volume (km3\\u200b)'>],\n      dtype=object)\n\n\n\n\n\n\n\n4.2.2 Check-in\n\nPlot a graph of the annual sea level rise with respect to the years.\nWhat information is the columns variable retrieving from the data frame? Describe in a sentence what is being plotted.\n\ncolumns = df.loc[:,'europe':'antarctica'].columns\ndf.plot(x='year', \n        y=columns, \n        subplots=True)\nWe will move on to another dataset for the rest of the lecture. The greatâ€¦"
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#palmer-penguins-dataset",
    "href": "lectures/lesson-4-basic-plotting.html#palmer-penguins-dataset",
    "title": "4Â  Plotting",
    "section": "4.3 Palmer penguins dataset",
    "text": "4.3 Palmer penguins dataset\nFor the next plots we will use the Palmer Penguins dataset (Horst et al., 2020). This contains size measurements for three penguin species in the Palmer Archipelago, Antarctica.\n\n\n\nThe Palmer Archipelago penguins. Artwork by @allison_horst.\n\n\nThe data is usually accessed through the palmerpenguins R data package. Today we will access the csv directly into Python using the URL: https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv\nThe Palmer penguins dataset has the following columns:\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_lenght_mm\nbody_mass_g\nsex\nyear\n\nLetâ€™s start by reading in the data.\n\n# read in data\npenguins = pd.read_csv('https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv')\n\n# look at dataframe's head\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      2007\n    \n  \n\n\n\n\n\n# check column data types and NA values\npenguins.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB\n\n\n\n# simple statistics about numeric columns\npenguins.describe()\n\n\n\n\n\n  \n    \n      \n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      year\n    \n  \n  \n    \n      count\n      342.000000\n      342.000000\n      342.000000\n      342.000000\n      344.000000\n    \n    \n      mean\n      43.921930\n      17.151170\n      200.915205\n      4201.754386\n      2008.029070\n    \n    \n      std\n      5.459584\n      1.974793\n      14.061714\n      801.954536\n      0.818356\n    \n    \n      min\n      32.100000\n      13.100000\n      172.000000\n      2700.000000\n      2007.000000\n    \n    \n      25%\n      39.225000\n      15.600000\n      190.000000\n      3550.000000\n      2007.000000\n    \n    \n      50%\n      44.450000\n      17.300000\n      197.000000\n      4050.000000\n      2008.000000\n    \n    \n      75%\n      48.500000\n      18.700000\n      213.000000\n      4750.000000\n      2009.000000\n    \n    \n      max\n      59.600000\n      21.500000\n      231.000000\n      6300.000000\n      2009.000000\n    \n  \n\n\n\n\nWe can also subset the dataframe to get information about a particular column or groups of columns.\n\n# get count unique values in categorical columns and year\npenguins[['species', 'island', 'sex', 'year']].nunique()\n\nspecies    3\nisland     3\nsex        2\nyear       3\ndtype: int64\n\n\n\n# get unique values in species column\nprint(penguins.species.unique())\n\n['Adelie' 'Gentoo' 'Chinstrap']\n\n\n\n# species unique value counts \nprint(penguins.species.value_counts())\n\nAdelie       152\nGentoo       124\nChinstrap     68\nName: species, dtype: int64"
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#kind-argument-in-plot",
    "href": "lectures/lesson-4-basic-plotting.html#kind-argument-in-plot",
    "title": "4Â  Plotting",
    "section": "4.4 kind argument in plot()",
    "text": "4.4 kind argument in plot()\nWe talked about how the plot() function creates by default a line plot. The parameter that controls this behaviour is plot()â€™s kind parameter. By changing the value of kind we can create different kinds of plots. Letâ€™s look at the documentation to see what these values are:\n\n\n\npandas.DataFrame.plot documentation extract - accessed Oct 10,2023\n\n\nNotice the default value of kind is 'line'.\nLetâ€™s change the kind parameter to create some different plots."
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#scatter-plots",
    "href": "lectures/lesson-4-basic-plotting.html#scatter-plots",
    "title": "4Â  Plotting",
    "section": "4.5 Scatter plots",
    "text": "4.5 Scatter plots\nSuppose we want to visualy compare the flipper length against the body mass, we can do this with a scatterplot.\nExample:\n\npenguins.plot(kind='scatter',\n        x='flipper_length_mm', \n        y='body_mass_g')\n\n<AxesSubplot:xlabel='flipper_length_mm', ylabel='body_mass_g'>\n\n\n\n\n\nWe can update some other arguments to customize the graph:\n\npenguins.plot(kind='scatter',\n        x='flipper_length_mm', \n        y='body_mass_g',\n        title='Flipper length and body mass for Palmer penguins',\n        xlabel='Flipper length (mm)',\n        ylabel='Body mass (g)',\n        color='#ff3b01',\n        alpha=0.4  # controls transparency\n        )\n\n<AxesSubplot:title={'center':'Flipper length and body mass for Palmer penguins'}, xlabel='Flipper length (mm)', ylabel='Body mass (g)'>"
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#bar-plots",
    "href": "lectures/lesson-4-basic-plotting.html#bar-plots",
    "title": "4Â  Plotting",
    "section": "4.6 Bar plots",
    "text": "4.6 Bar plots\nWe can create bar plots of our data setting kind='bar' in the plot() method.\nFor example, letâ€™s say we want to get data about the 10 penguins with lowest body mass. We can first select this data using the nsmallest() method for series:\n\nsmallest = penguins.body_mass_g.nsmallest(10).sort_values()\nsmallest\n\n314    2700.0\n58     2850.0\n64     2850.0\n54     2900.0\n98     2900.0\n116    2900.0\n298    2900.0\n104    2925.0\n47     2975.0\n44     3000.0\nName: body_mass_g, dtype: float64\n\n\nWe can then plot this data as a bar plot\n\nsmallest.plot(kind='bar')\n\n<AxesSubplot:>\n\n\n\n\n\nIf we wanted to look at other data for these smallest penguins we can use the index of the smallest pandas.Series to select those rows in the original penguins data frame using loc:\n\npenguins.loc[smallest.index]\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      314\n      Chinstrap\n      Dream\n      46.9\n      16.6\n      192.0\n      2700.0\n      female\n      2008\n    \n    \n      58\n      Adelie\n      Biscoe\n      36.5\n      16.6\n      181.0\n      2850.0\n      female\n      2008\n    \n    \n      64\n      Adelie\n      Biscoe\n      36.4\n      17.1\n      184.0\n      2850.0\n      female\n      2008\n    \n    \n      54\n      Adelie\n      Biscoe\n      34.5\n      18.1\n      187.0\n      2900.0\n      female\n      2008\n    \n    \n      98\n      Adelie\n      Dream\n      33.1\n      16.1\n      178.0\n      2900.0\n      female\n      2008\n    \n    \n      116\n      Adelie\n      Torgersen\n      38.6\n      17.0\n      188.0\n      2900.0\n      female\n      2009\n    \n    \n      298\n      Chinstrap\n      Dream\n      43.2\n      16.6\n      187.0\n      2900.0\n      female\n      2007\n    \n    \n      104\n      Adelie\n      Biscoe\n      37.9\n      18.6\n      193.0\n      2925.0\n      female\n      2009\n    \n    \n      47\n      Adelie\n      Dream\n      37.5\n      18.9\n      179.0\n      2975.0\n      NaN\n      2007\n    \n    \n      44\n      Adelie\n      Dream\n      37.0\n      16.9\n      185.0\n      3000.0\n      female\n      2007"
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#histograms",
    "href": "lectures/lesson-4-basic-plotting.html#histograms",
    "title": "4Â  Plotting",
    "section": "4.7 Histograms",
    "text": "4.7 Histograms\nWe can create a histogram of our data setting kind='hist' in plot().\n\n# using plot without subsetting data - a mess again\npenguins.plot(kind='hist')\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\nTo gain actual information, letâ€™s subset the data before plotting it. For example, suppose we want to look at the distribution of flipper length. We could do it in this way:\n\n# distribution of flipper length measurements\n# first select data, then plot\npenguins.flipper_length_mm.plot(kind='hist',\n                                title='Penguin flipper lengths',\n                                xlabel='Flipper length (mm)',\n                                grid=True)\n\n<AxesSubplot:title={'center':'Penguin flipper lengths'}, ylabel='Frequency'>\n\n\n\n\n\n\n4.7.1 Check-in\n\nSelect the bill_length_mm and bill_depth_mm columns in the penguins dataframe and then update the kind parameter to box to make boxplots of the bill length and bill depth. \nSelect both rows and columns to create a histogram of the flipper length of gentoo penguins."
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#references",
    "href": "lectures/lesson-4-basic-plotting.html#references",
    "title": "4Â  Plotting",
    "section": "4.8 References",
    "text": "4.8 References\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi:10.5281/zenodo.3960218."
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#data-palmer-penguins",
    "href": "lectures/lesson-5-updating-dataframes.html#data-palmer-penguins",
    "title": "5Â  Updating data frames",
    "section": "5.1 Data: Palmer penguins",
    "text": "5.1 Data: Palmer penguins\nWe will use the Palmer penguins dataset (Horst et al., 2020). This time we will import it via the seaborn package since it is included as one of seabornâ€™s example datasets.\n\n\n\n\n\n\nseaborn\n\n\n\nseaborn is a library to make nice looking graphs in Python. Weâ€™ve been using plot() to make basic exploratory graphs. seaborn is the next step if youâ€™re interested in making publication-level statistical graphics.\n\n\n\n# standard libraries\nimport pandas as pd\nimport numpy as np\n\n# importing seaborn with its standard abbreviation\nimport seaborn as sns\n\n# will use the random library to create some random numbers\nimport random\n\npenguins = sns.load_dataset(\"penguins\")\n\n# look at dataframe's head\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#adding-a-single-column",
    "href": "lectures/lesson-5-updating-dataframes.html#adding-a-single-column",
    "title": "5Â  Updating data frames",
    "section": "5.2 Adding a single column",
    "text": "5.2 Adding a single column\nThe general syntax to adding a new column to a pd.DataFrame is\ndf['new_col_name'] = new_column_values\nwhere new_column values could be:\n\na pd.Series or numpy.array of the same length as the data frame\na single scalar\n\nExample\nSuppose we want to create a new column where the body mass is in kilograms instead of grams, so we need to divide the body_mass_g by 1000.\n\n# add a new column body_mass_kg \n# sane syntax as adding a new key to a dictionary\npenguins['body_mass_kg'] = penguins.body_mass_g/1000\n\n# confirm the new column is in the data frame\nprint('body_mass_kg' in penguins.columns)\n\n# take a look at the new column\npenguins.head()\n\nTrue\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      body_mass_kg\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n      3.75\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n      3.80\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n      3.25\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female\n      3.45\n    \n  \n\n\n\n\n\n\n\n\n\n\npd.DataFrames and dictionaries\n\n\n\nRemember we talked about how a pandas.DataFrame could be seen as a dictionary of its columns. The most basic way of adding a new column to a data frame is the same way of adding a new key-value pair to a dictionary:\n# adding a new key-value pair to a dictionary\n# same as adding a new column in pandas\ndict[new_key] = new_value\n\n\nNotice the new column got added at the end. If we want to create a new column and insert it at a particular position we use the data frame method insert():\ndf.insert(loc = integer_index,  # location of new column\n          column = 'new_col_name', \n          value = new_col_values)\nExample\nSuppose each penguin observation gets a unique identifier as a three digit number. We want to add this column, at the beginning of the data frame. We can use insert to do this:\n\n# create random 3-digit codes\n# random.sample used for random sampling wo replacement\ncodes = random.sample(range(100,1000), len(penguins))\n\n# insert codes at the front of data frame = index 0\npenguins.insert(loc=0, \n                column = 'code',\n                value = codes)\n        \npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      body_mass_kg\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n      3.75\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n      3.80\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n      3.25\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female\n      3.45"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#adding-multiple-columns",
    "href": "lectures/lesson-5-updating-dataframes.html#adding-multiple-columns",
    "title": "5Â  Updating data frames",
    "section": "5.3 Adding multiple columns",
    "text": "5.3 Adding multiple columns\nWe can assign multiple columns in the same call by using the data frameâ€™s assign() method. The general syntax is:\ndf = df.assign( new_col1_name = new_col1_values, \n           new_col2_name = new_col2_values)\nNotice the new column names are not strings, we declare them as if we were creating variables.\nExample\nSuppose we want to add these new columns:\n\nflipper length converted from mm to cm, and\na code representing the observer.\n\nWe can add these columns to penguins using assign():\n\n# create new columns in the data frame\n# random.choices used for random sampling with replacement\n# need to reassign output of assign() to update the data frame\npenguins = penguins.assign( flipper_length_cm = penguins.flipper_length_mm /10, \n                            observer =   random.choices(['A','B','C'], k=len(penguins)))\n# look at result\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n      3.75\n      18.1\n      C\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n      3.80\n      18.6\n      C\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n      3.25\n      19.5\n      B\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female\n      3.45\n      19.3\n      A"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#removing-columns",
    "href": "lectures/lesson-5-updating-dataframes.html#removing-columns",
    "title": "5Â  Updating data frames",
    "section": "5.4 Removing columns",
    "text": "5.4 Removing columns\nWe can remove columns using the drop() method for data frames, the syntax is:\ndf = df.drop(columns = col_names)\nwhere col_names can be a single column name (str) or a list of column names. The drop() method does not modify the data frame in place, so you need to reassign the output.\nExample\nNow that we updated the units for flipper length and body mass, it makes sense to remove the previous columns to avoid duplicate information. We can do this using drop():\n\n# use a list of column names\n# reassign output of drop() to dataframe to update it\npenguins = penguins.drop(columns=['flipper_length_mm','body_mass_g'])\n\n# check columns\nprint(penguins.columns)\n\nIndex(['code', 'species', 'island', 'bill_length_mm', 'bill_depth_mm', 'sex',\n       'body_mass_kg', 'flipper_length_cm', 'observer'],\n      dtype='object')"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#updating-values",
    "href": "lectures/lesson-5-updating-dataframes.html#updating-values",
    "title": "5Â  Updating data frames",
    "section": "5.5 Updating values",
    "text": "5.5 Updating values\nSometimes we want to update certain value within our data frame. Weâ€™ll review some methods and best practices to do that in this section.\n\n5.5.1 A single value\nWe can access a single value in a pd.DataFrame using the locators\n\nat[] to select by labels, or\niat[] to select by position.\n\nThe syntax for at[] is:\ndf.at[single_index_value, 'column_name']\nThink of at[] as the equivalent to loc[] when trying to access a single value.\nExample\nLetâ€™s say we want to know what was the bill length of the penguin in the fourth row. We can access that using at[]:\n\n# access value at row with index=3 and column='bill_length_mm'\npenguins.at[3,'bill_length_mm']\n\nnan\n\n\nWe get this is an NA. Maybe we want to update it to 38.3 mm. We can do this with at[] too:\n\n# update NA to 38.3\npenguins.at[3,'bill_length_mm'] = 38.3\n\n# check it was updated\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      Male\n      3.75\n      18.1\n      C\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      Female\n      3.80\n      18.6\n      C\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      Female\n      3.25\n      19.5\n      B\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      38.3\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      Female\n      3.45\n      19.3\n      A\n    \n  \n\n\n\n\nIf we want to access or update a single value by position we use the iat[] locator.\n\n\n5.5.2 Multiple values in a column\nWhat if we want to update multiple values in a column? Weâ€™ll cover two cases: with a condition on the column values and by selecting a few values to update.\n\n5.5.2.1 By condition\nOften we want to create a new column where the new values depend on conditions on another columnâ€™s values.\nExample\nSuppose we want to classify all penguins with body mass less than 3kg as small, penguins with body mass greater or equal than 3kg but less than 5kg as medium, and those with body mass greater or equal than 5kg as large. One way to add this information in a new column using numpy.select():\n\n# create a list with the conditions\nconditions = [penguins.body_mass_kg < 3, \n              (3 <= penguins.body_mass_kg) & (penguins.body_mass_kg < 5),\n              5 <= penguins.body_mass_kg]\n\n# create a list with the choices\nchoices = [\"small\",\n           \"medium\",\n           \"large\"]\n\n# add the selections using np.select\n# default = value for anything that falls outside conditions\npenguins['size'] = np.select(conditions, choices, default=np.nan)\n\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n      size\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      Male\n      3.75\n      18.1\n      C\n      medium\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      Female\n      3.80\n      18.6\n      C\n      medium\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      Female\n      3.25\n      19.5\n      B\n      medium\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      38.3\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n      nan\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      Female\n      3.45\n      19.3\n      A\n      medium\n    \n  \n\n\n\n\n\n\n\n\n5.5.2.2 By selecting values\nWhen we only want to update some values in a column we can do this by selecting this data using loc (if selecting by labels) or iloc (if selecting by position). The general sytanx for updating data with loc is:\n# modifies data in place\ndf.loc[row_selection, col_name] = new_values\nwhere\n\nrow_selection is the rows we want to update,\ncol_name is a single column name, and\nnew_values is the new value or values we want. If using multiple values, then new_values must be of the same length as the number of rows selected,\n\nExample\nSuppose we want to update the â€œMaleâ€ value in the sex column to â€œMâ€.\n\n# select rows with sex=male and update the values in the sex column\npenguins.loc[penguins.sex=='Male', 'sex'] = 'M'\n\n# check changes\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n      size\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      M\n      3.75\n      18.1\n      C\n      medium\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      Female\n      3.80\n      18.6\n      C\n      medium\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      Female\n      3.25\n      19.5\n      B\n      medium\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      38.3\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n      nan\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      Female\n      3.45\n      19.3\n      A\n      medium"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#settingwithcopywarning",
    "href": "lectures/lesson-5-updating-dataframes.html#settingwithcopywarning",
    "title": "5Â  Updating data frames",
    "section": "5.6 SettingWithCopyWarning",
    "text": "5.6 SettingWithCopyWarning\nSuppose we want to similarly update the â€œFemaleâ€ values in the sex column to â€œFâ€. This is an example of another way we might try to do it, but it wonâ€™t update our data frame:\n\n# select rows and columns with two selection brackets [][] (chained indexing), instead of loc[]\n# then trying to update the values\npenguins[penguins.sex=='Female']['sex'] = 'F'\n\n/opt/python/3.7.13/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nWhen we select the data we want to update using chained indexing ([][]) instead of loc[] we get a SettingWithCopyWarning. Wit this warning, pandas is trying to alert us to a potential bug. In this case that we did not update our data frame:\n\n# no values were updated\npenguins.sex.unique()\n\narray(['M', 'Female', nan], dtype=object)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe SettingWithCopyWarning is letting us know our code is ambiguous and we need to double check the code is doing what we intended. In the words of the pandas documentation:\n\nassigning to the product of chained indexing has inherently unpredictable results.\n\n\n\n\nTo see what is going on we need to understand that some pandas operations return a view to your data, while others return a copy of your data.\n\nViews are actual subsets of the original data, when we update them, we are modifying the original data frame.\nCopies are unique objects, independent of our original data frames. When we update a copy we are not modifying the original data frame.\n\nDepending on what we are trying to do we might want to modify the original data frame or we might want to modify a copy.\nCheck-in\nUpdate the â€œFemaleâ€ values in the penguins data frame to â€œFâ€. Donâ€™t use chained indexing\n\n# no chained indexing in assignment = no warning\npenguins.loc[penguins.sex=='Female','sex'] = 'F'\n\n# notice the values were updated now\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n      size\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      M\n      3.75\n      18.1\n      C\n      medium\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      F\n      3.80\n      18.6\n      C\n      medium\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      F\n      3.25\n      19.5\n      B\n      medium\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      38.3\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n      nan\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      F\n      3.45\n      19.3\n      A\n      medium\n    \n  \n\n\n\n\n\n5.6.0.1 Another SettingWithCopyWarning\nAnother common situation where SettingWithCopyWarning comes up is when we try to update a subset of a data frame we have already stored in a new variable.\nExample\nSuppose we only want to use data from Biscoe island and, after doing some analyses, we want to add a new column to it:\n\n# select penguins from Biscoe island\nbiscoe = penguins[penguins.island=='Biscoe']\n\n# 50 lines of code here\n\n# add a column, we get a warning\nbiscoe['sample_col'] = 100\n\n/opt/python/3.7.13/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nIt might not seem like it, but we have a chained assignment here too. We essentialy wrote:\npenguins[penguins.island=='Biscoe']['sample_col'] = 100\nTo fix this we can take control of the copy-view situation and explicitely ask for a copy of the dataset when subsetting the data. Use the copy() method to do this:\n\n# make sure you get a new data frame with penguins from Biscoe island\nbiscoe = penguins[penguins.island=='Biscoe'].copy()\n\n# add a column, no warning\nbiscoe['sample_col'] = 100\n\nNow we are sure we did not modify our initial data, but rather the biscoe data frame:\n\n# new column is there\nbiscoe.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n      size\n      sample_col\n    \n  \n  \n    \n      20\n      666\n      Adelie\n      Biscoe\n      37.8\n      18.3\n      F\n      3.40\n      17.4\n      A\n      medium\n      100\n    \n    \n      21\n      348\n      Adelie\n      Biscoe\n      37.7\n      18.7\n      M\n      3.60\n      18.0\n      C\n      medium\n      100\n    \n    \n      22\n      753\n      Adelie\n      Biscoe\n      35.9\n      19.2\n      F\n      3.80\n      18.9\n      C\n      medium\n      100\n    \n    \n      23\n      513\n      Adelie\n      Biscoe\n      38.2\n      18.1\n      M\n      3.95\n      18.5\n      C\n      medium\n      100\n    \n    \n      24\n      893\n      Adelie\n      Biscoe\n      38.8\n      17.2\n      M\n      3.80\n      18.0\n      A\n      medium\n      100\n    \n  \n\n\n\n\n\n# and we are sure our original df wasn't modified\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n      size\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      M\n      3.75\n      18.1\n      C\n      medium\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      F\n      3.80\n      18.6\n      C\n      medium\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      F\n      3.25\n      19.5\n      B\n      medium\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      38.3\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n      nan\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      F\n      3.45\n      19.3\n      A\n      medium\n    \n  \n\n\n\n\nThe SettingWithCopyWarning can often be tricky, there are also false positives and false negatives. Avoiding chained indexing and making a copy of your data frame subset whenever possible will save you from the usual pitfalls.\nTo learn more about the SettingWithCopyWarning, these are some articles that go into more depth:\nðŸ“– pandas Documentation - Returning a view versus a copy\nðŸ“– Real Python- SettingWithCopyWarning in pandas: Views vs Copies\nðŸ“– Dataquest - SettingwithCopyWarning: How to Fix This Warning in Pandas"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#references",
    "href": "lectures/lesson-5-updating-dataframes.html#references",
    "title": "5Â  Updating data frames",
    "section": "5.7 References",
    "text": "5.7 References\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi:10.5281/zenodo.3960218."
  },
  {
    "objectID": "lectures/lesson-6-groupby.html#summary-statistics",
    "href": "lectures/lesson-6-groupby.html#summary-statistics",
    "title": "6Â  groupby()",
    "section": "6.1 Summary statistics",
    "text": "6.1 Summary statistics\nIn pandas it is easy to get summary statistics for each column in a dataframe by using methods such as\n\nsum(): sum values in each column,\ncount(): count non-NA values in each column,\nmin() and max(): get the minimum and maximum value in each column,\n\nmean() and median(): get the mean and median value in each column,\nstd() and var(): get the standard deviation and variance in each column.\n\nExample\n\n# get the number of non-NA values in each column \npenguins.count()\n\nspecies              344\nisland               344\nbill_length_mm       342\nbill_depth_mm        342\nflipper_length_mm    342\nbody_mass_g          342\nsex                  333\nyear                 344\ndtype: int64\n\n\n\n# get the minimum value in each column\npenguins.select_dtypes('number').min()\n\nbill_length_mm         32.1\nbill_depth_mm          13.1\nflipper_length_mm     172.0\nbody_mass_g          2700.0\nyear                 2007.0\ndtype: float64"
  },
  {
    "objectID": "lectures/lesson-6-groupby.html#grouping",
    "href": "lectures/lesson-6-groupby.html#grouping",
    "title": "6Â  groupby()",
    "section": "6.2 Grouping",
    "text": "6.2 Grouping\nOur penguins data is naturally split into different groups: there are three different species, two sexes and three islands. Often, we want to calculate a certain statistic for each group. For example, suppose we want to calculate the average flipper length per species. How would we do this â€œby handâ€?\n\nWe start with our data and notice there are multiple species in the species column.\nWe split our original table to group all observations from the same species together.\nWe calculate the average flipper length for each of the groups we formed.\nThen we combine the values for average flipper length per species into a single table.\n\nThis is known as the Split-Apply-Combine strategy. This strategy follows the three steps we explained above:\n\nSplit: Split the data into logical groups (e.g.Â species, sex, island, etc.)\nApply: Calculate some summary statistic on each group (e.g.Â average flipper length by species, number of individuals per island, body mass by sex, etc.)\nCombine: Combine the statistic calculated on each group back together.\n\n\n\n\nSplit-apply-combine to calculate mean flipper length\n\n\nIn Python we can use the groupby() method to split (i.e.Â group) the data into different categories. The general syntax for groupby() is\ndf.groupby(columns_to_group_by)\nwhere most often we will have columns_to_group_by = a single column name (string) or a list of column names, the unique values of the column (or columns) will be used as the groups of the data frame\nExample\nFirst, if we donâ€™t use the groupby() method, we obtain the average for the whole flipper length column:\n\npenguins.flipper_length_mm.mean()\n\n200.91520467836258\n\n\nTo get the mean flipper length by species we first group our dataset by the species columnâ€™s data:\n\n# average flipper length per species\npenguins.groupby(\"species\").flipper_length_mm.mean()\n\nspecies\nAdelie       189.953642\nChinstrap    195.823529\nGentoo       217.186992\nName: flipper_length_mm, dtype: float64\n\n\nThereâ€™s a lot going on there, letâ€™s break it down (remember the . can be read as â€œand thenâ€¦â€)\n\nstart with the penguins data frame, and thenâ€¦\nuse groupby() to group the data frame by species values, and thenâ€¦\nselect the 'flipper_length_mm' column, and thenâ€¦\ncalculate the mean()\n\nWe can store our new data frame as avg_flipper and then graph it as a bar plot:\n\n# average flipper length per species\navg_flipper = penguins.groupby(\"species\").flipper_length_mm.mean()\n\n# sort in descending order\n# remember to reassign to actually update data\navg_flipper = avg_flipper.sort_values(ascending=False)\n\n# plot as a bar plot\navg_flipper.plot(kind='bar',\n                 title='Average flipper length by species',\n                 ylabel='Flipper length (mm)',\n                 rot=360  # rotation for ticks \n                 ) \n\n<AxesSubplot:title={'center':'Average flipper length by species'}, xlabel='species', ylabel='Flipper length (mm)'>\n\n\n\n\n\nWe can also group by combinations of columns.\nExample\nSuppose we want to know what was the number of penguins in every island on different years. We can use the count() method to count the number of non-NA values in each column like this:\n\npenguins.count()\n\nspecies              344\nisland               344\nbill_length_mm       342\nbill_depth_mm        342\nflipper_length_mm    342\nbody_mass_g          342\nsex                  333\nyear                 344\ndtype: int64\n\n\nWhen we group by island and year we get the count of non-NA values for each column divided for each combination of island and year:\n\npenguins.groupby(['island','year']).count()\n\n\n\n\n\n  \n    \n      \n      \n      species\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n    \n      island\n      year\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      2007\n      44\n      44\n      44\n      44\n      44\n      43\n    \n    \n      2008\n      64\n      64\n      64\n      64\n      64\n      63\n    \n    \n      2009\n      60\n      59\n      59\n      59\n      59\n      57\n    \n    \n      Dream\n      2007\n      46\n      46\n      46\n      46\n      46\n      45\n    \n    \n      2008\n      34\n      34\n      34\n      34\n      34\n      34\n    \n    \n      2009\n      44\n      44\n      44\n      44\n      44\n      44\n    \n    \n      Torgersen\n      2007\n      20\n      19\n      19\n      19\n      19\n      15\n    \n    \n      2008\n      16\n      16\n      16\n      16\n      16\n      16\n    \n    \n      2009\n      16\n      16\n      16\n      16\n      16\n      16\n    \n  \n\n\n\n\nLetâ€™s say we want to plot the surveyed population per year and island. We could then use a â€œone linerâ€ to do it in this way:\n\npenguins.groupby(['island','year']).count().species.sort_values().plot(kind='barh')\n\n<AxesSubplot:ylabel='island,year'>\n\n\n\n\n\n\n\n6.2.1 Check-in\n\nUse groupby and the max() method for data frames to calculate the maximum value of penguinâ€™s body mass by year and species.\n\n\n\nUse (1) to display the highest body masses per year and species as a bar plot in descending order."
  },
  {
    "objectID": "lectures/lesson-6-groupby.html#acknowledgmenets",
    "href": "lectures/lesson-6-groupby.html#acknowledgmenets",
    "title": "6Â  groupby()",
    "section": "6.3 Acknowledgmenets",
    "text": "6.3 Acknowledgmenets\nThis section is based on this NCEAS Learning Hub R lesson.\nHalina Do-Linh, Carmen Galaz GarcÃ­a, Matthew B. Jones, Camila Vargas Poulsen. 2023. Open Science Synthesis training Week 1. NCEAS Learning Hub & Delta Stewardship Council."
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#timestamps",
    "href": "lectures/lesson-7-time-series.html#timestamps",
    "title": "7Â  Basic time series",
    "section": "7.1 Timestamps",
    "text": "7.1 Timestamps\npandas represents an instant in time using the pandas.Timestamp class. For example:\n\nimport pandas as pd\n\n# create a timestamp\npd.Timestamp(year=2020, month=10, day=18, hour=12, minute=30, second=15)\n\nTimestamp('2020-10-18 12:30:15')\n\n\nWhen we store multiple Timestamps in a pd.Series (for example when we have a column of dates) the data type of the column is set to datetime64[ns]:\n\n# Notice the dtype of the column is datetime64\npd.Series([pd.Timestamp(2020,10,18), \n           pd.Timestamp(2020,10,17),\n           pd.Timestamp(2020,10,16)])\n\n0   2020-10-18\n1   2020-10-17\n2   2020-10-16\ndtype: datetime64[ns]\n\n\nThis is enough to get us started!"
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#data-precipitation-in-boulder-co",
    "href": "lectures/lesson-7-time-series.html#data-precipitation-in-boulder-co",
    "title": "7Â  Basic time series",
    "section": "7.2 Data: Precipitation in Boulder, CO",
    "text": "7.2 Data: Precipitation in Boulder, CO\nTo exemplify some of the basic time series functionalities weâ€™ll be using data about hourly precipitation in the county of Boulder, Colorado from 2000 to 2014. In September 2013, an unusual weather pattern led to some of the most intense precipitation ever recorded in this region, causing devastating floods throughout the Colorado Front Range. Our goal is to visualize precipitation data in 2013 and identify this unusual weather event.\nThis data was obtained via the National Oceanic and Atmosperic Administration (NOAA) Climate Data Online service. This dataset is a csv and can be acceses at this link. You can view the full documentation here. The following is a summary of the column descriptions:\n\nSTATION: identification number indentifying the station.\nSTATION_NAME: optional field, name identifying the station location.\nDATE: this is the year of the record (4 digits), followed by month (2 digits), followed by day of the month (2 digits), followed by a space and ending with a time of observation that is a two digit indication of the local time hour, followed by a colon (:) followed by a two digit indication of the minute which for this dataset will always be 00. Note: The subsequent data value will be for the hour ending at the time specified here. Hour 00:00 will be listed as the first hour of each date, however since this data is by definition an accumulation of the previous 60 minutes, it actually occurred on the previous day.\nHPCP: The amount of precipitation recorded at the station for the hour ending at the time specified for DATE above given in inches. The values 999.99 means the data value is missing. Hours with no precipitation are not shown."
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#data-preparation",
    "href": "lectures/lesson-7-time-series.html#data-preparation",
    "title": "7Â  Basic time series",
    "section": "7.3 Data preparation",
    "text": "7.3 Data preparation\nLetâ€™s start by reading in the data and taking a look at it.\n\n# read in data \nurl = 'https://raw.githubusercontent.com/carmengg/eds-220-book/main/data/boulder_colorado_2013_hourly_precipitation.csv'\nprecip = pd.read_csv(url)\n\n# check df's head\nprecip.head()\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      DATE\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n  \n  \n    \n      0\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000101 00:00\n      999.99\n      ]\n      \n    \n    \n      1\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000101 01:00\n      0.00\n      g\n      \n    \n    \n      2\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000102 20:00\n      0.00\n      \n      q\n    \n    \n      3\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000103 01:00\n      0.00\n      \n      q\n    \n    \n      4\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000103 05:00\n      0.00\n      \n      q\n    \n  \n\n\n\n\nLetâ€™s make a first attempt at plotting the precipitation:\n\nprecip.HPCP.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nThereâ€™s a few things going on with this graph:\n\nThere are many jumps close to 1000. This is clearly not right and these are outliers. Looking at the column description we can see 999.99 indicates the HPCP data is missing.\nThe x-axis values are given by the index of the dataframe and not relative to time.\nWe are only intersted in the precipitation data from 2013, this graph is trying to plot all our data.\n\nLetâ€™s fix each one of these issues separately."
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#indicate-missing-data",
    "href": "lectures/lesson-7-time-series.html#indicate-missing-data",
    "title": "7Â  Basic time series",
    "section": "7.4 Indicate missing data",
    "text": "7.4 Indicate missing data\nThe metadata states the missing values are indicated by the number 999.99. We can use this information to reload the dataframe indicating 999.99 is the missing value. To do this, we add the na_values parameter to the pd.read_csv() function to indicitate additional values that should be recognized as NA:\n\n# read in csv indicating NA values\nprecip = pd.read_csv(url, na_values=[999.99])\n\n# check updated df\nprecip.head()\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      DATE\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n  \n  \n    \n      0\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000101 00:00\n      NaN\n      ]\n      \n    \n    \n      1\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000101 01:00\n      0.0\n      g\n      \n    \n    \n      2\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000102 20:00\n      0.0\n      \n      q\n    \n    \n      3\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000103 01:00\n      0.0\n      \n      q\n    \n    \n      4\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000103 05:00\n      0.0\n      \n      q\n    \n  \n\n\n\n\n\n\n\n\n\n\nDefault NA values\n\n\n\nCheck the na_values parameter in the pd.read_csv() documentation to learn more about which values are identified as NA by default.\n\n\nWe can try making our plot again:\n\nprecip.HPCP.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nThis looks much better and we can already see there is something going on close to the end of the time series. We still need to set the index as the time."
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#strings-into-dates",
    "href": "lectures/lesson-7-time-series.html#strings-into-dates",
    "title": "7Â  Basic time series",
    "section": "7.5 Strings into dates",
    "text": "7.5 Strings into dates\nNotice that the DATE column in our dataframe is not of type datetime. We can check this using the dtypes attribute for dataframes:\n\n# check the dtypes of all columns\nprecip.dtypes\n\nSTATION              object\nSTATION_NAME         object\nDATE                 object\nHPCP                float64\nMeasurement Flag     object\nQuality Flag         object\ndtype: object\n\n\n\nThe object dtype means that (most likely) all values in that column are strings. We can convert strings to datetime objects using the to_datetime() pandas function:\n\npd.to_datetime() input: a pd.Series with strings that can be converted to dates\npd.to_datetime() output: a pd.Series with the strings converted to datetime objects\n\nExample\n\n# covert precip.DATE column to timedate objects\npd.to_datetime(precip.DATE)\n\n0      2000-01-01 00:00:00\n1      2000-01-01 01:00:00\n2      2000-01-02 20:00:00\n3      2000-01-03 01:00:00\n4      2000-01-03 05:00:00\n               ...        \n9001   2013-12-22 01:00:00\n9002   2013-12-23 00:00:00\n9003   2013-12-23 02:00:00\n9004   2013-12-29 01:00:00\n9005   2013-12-31 00:00:00\nName: DATE, Length: 9006, dtype: datetime64[ns]\n\n\nWe can overwrite the DATE column with this output:\n\n# convert DATE column from string to timestamps\nprecip.DATE = pd.to_datetime(precip.DATE)\n\n# check column data type is updated\nprint(precip.dtypes)\n\n# check new values\nprecip.head()\n\nSTATION                     object\nSTATION_NAME                object\nDATE                datetime64[ns]\nHPCP                       float64\nMeasurement Flag            object\nQuality Flag                object\ndtype: object\n\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      DATE\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n  \n  \n    \n      0\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      2000-01-01 00:00:00\n      NaN\n      ]\n      \n    \n    \n      1\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      2000-01-01 01:00:00\n      0.0\n      g\n      \n    \n    \n      2\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      2000-01-02 20:00:00\n      0.0\n      \n      q\n    \n    \n      3\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      2000-01-03 01:00:00\n      0.0\n      \n      q\n    \n    \n      4\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      2000-01-03 05:00:00\n      0.0\n      \n      q\n    \n  \n\n\n\n\nAnd we can make another try at plotting our precipitation data:\n\nprecip.plot(x='DATE', y='HPCP')\n\n<AxesSubplot:xlabel='DATE'>\n\n\n\n\n\nNotice the x-axis is now neatly organized into years."
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#timestamp-index",
    "href": "lectures/lesson-7-time-series.html#timestamp-index",
    "title": "7Â  Basic time series",
    "section": "7.6 Timestamp index",
    "text": "7.6 Timestamp index\nAt its core, we have time series data if our data frame is indexed by moments in time. Using our DATE column as the index will allows us to do many operations with respect to time, including subsetting and resampling.\nUpdating the index of a dataframe is simple, we use the set_index() method. Itâ€™s general syntax is\ndf = df.set_index(new_index)\nwhere new_index is:\n\nthe name of the column (string) we want to set as index, or\nif our new index is not a column in the dataframe, an array or pd.Series of the same length as our data frame (we need one index per row!).\n\nNotice that set_index does not modify the data frame in place.\nExample\nIn our precipitation data this is:\n\n# set DATE coumn as index\nprecip = precip.set_index('DATE')\n\n# check new index\nprecip.head()\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n    \n      DATE\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      NaN\n      ]\n      \n    \n    \n      2000-01-01 01:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.0\n      g\n      \n    \n    \n      2000-01-02 20:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.0\n      \n      q\n    \n    \n      2000-01-03 01:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.0\n      \n      q\n    \n    \n      2000-01-03 05:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.0\n      \n      q\n    \n  \n\n\n\n\nSince we know the default behaviour of plot() is to use the index as the x-axis and make a line plot for each numeric column, we can simplify our plot making like this:\n\nprecip.plot()\n\n<AxesSubplot:xlabel='DATE'>"
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#subsetting-by-date",
    "href": "lectures/lesson-7-time-series.html#subsetting-by-date",
    "title": "7Â  Basic time series",
    "section": "7.7 Subsetting by date",
    "text": "7.7 Subsetting by date\npandas has great functionality to subset a dataframe when using a time index. These are some examples:\n\n# select data from a given month and year\n# use loc['year-month']\nprecip.loc['2013-09']\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n    \n      DATE\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-09-01 00:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      NaN\n      ]\n      \n    \n    \n      2013-09-01 01:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      NaN\n      [\n      \n    \n    \n      2013-09-01 00:00:00\n      COOP:050183\n      ALLENSPARK 2 SE CO US\n      NaN\n      ]\n      \n    \n    \n      2013-09-01 01:00:00\n      COOP:050183\n      ALLENSPARK 2 SE CO US\n      NaN\n      [\n      \n    \n    \n      2013-09-01 00:00:00\n      COOP:055121\n      LONGMONT 6 NW CO US\n      NaN\n      }\n      \n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2013-09-23 02:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.2\n      \n      \n    \n    \n      2013-09-27 10:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.1\n      \n      \n    \n    \n      2013-09-27 15:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.1\n      \n      \n    \n    \n      2013-09-27 17:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.1\n      \n      \n    \n    \n      2013-09-27 18:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.1\n      \n      \n    \n  \n\n128 rows Ã— 5 columns\n\n\n\n\n# select data from a given year\n# use loc['year']\nprecip.loc['2013']\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n    \n      DATE\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-01-01 01:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.0\n      g\n      \n    \n    \n      2013-01-10 02:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      NaN\n      [\n      \n    \n    \n      2013-01-13 00:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      NaN\n      ]\n      \n    \n    \n      2013-01-26 20:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.1\n      \n      \n    \n    \n      2013-01-28 23:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.1\n      \n      \n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2013-12-22 01:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      NaN\n      [\n      \n    \n    \n      2013-12-23 00:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      NaN\n      ]\n      \n    \n    \n      2013-12-23 02:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.1\n      \n      \n    \n    \n      2013-12-29 01:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      NaN\n      [\n      \n    \n    \n      2013-12-31 00:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      NaN\n      ]\n      \n    \n  \n\n662 rows Ã— 5 columns\n\n\n\nWe can use this selection to plot data as usual. Notice we have a lot of gaps due to missing data.\n\nprecip.loc['2013'].plot()\n\n<AxesSubplot:xlabel='DATE'>"
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#resample",
    "href": "lectures/lesson-7-time-series.html#resample",
    "title": "7Â  Basic time series",
    "section": "7.8 Resample",
    "text": "7.8 Resample\nResampling a time series means converting a time series from one frequency to another. For example, monthly to yearly (downsampling) or weekly to daily (upsampling). We can resample with the resample() method. The simplest use is to call\ndf.resample(new_frequency).aggregator_function()\nwhere: - new_frequency is a string representing the new frequence to resample the data, for example 'D' for day, w for week, M for month, Y for year, and - aggregator_function() is the function we will use to aggregate the data into the new frequence. For example, max(), min(), sum() or average().\nresample() works similarly to groupby() in the sense that you need to specify a way to aggregate the data to get any output.\nExample\nOur 2013 precipitation data has hourly frequency and we want to resample it to daily frequency.\n\n# resample 2013 hourly data to daily frequency\n# nothing happens\nprecip.loc['2013'].resample('D')\n\n<pandas.core.resample.DatetimeIndexResampler object at 0x7f2b11ceb510>\n\n\nTo get an output we need to add an aggregator function that indicates how we want to summarize the data that falls on each day. In this case we want the total precipitation on a day, so we will aggreagte using sum():\n\n# total daily precipitation in 2013\ndaily_precip_2013 = precip.loc['2013'].resample('D').sum()\n\ndaily_precip_2013\n\n\n\n\n\n  \n    \n      \n      HPCP\n    \n    \n      DATE\n      \n    \n  \n  \n    \n      2013-01-01\n      0.0\n    \n    \n      2013-01-02\n      0.0\n    \n    \n      2013-01-03\n      0.0\n    \n    \n      2013-01-04\n      0.0\n    \n    \n      2013-01-05\n      0.0\n    \n    \n      ...\n      ...\n    \n    \n      2013-12-27\n      0.0\n    \n    \n      2013-12-28\n      0.1\n    \n    \n      2013-12-29\n      0.0\n    \n    \n      2013-12-30\n      0.1\n    \n    \n      2013-12-31\n      0.0\n    \n  \n\n365 rows Ã— 1 columns\n\n\n\nNotice the index now is every day in 2013. We should rename that column since it is not longer hourly precipitation:\n\n# rename column\ndaily_precip_2013 = daily_precip_2013.rename(columns={'HPCP':'daily_precipitation'})\n\nFinally, we can plot our data:\n\ndaily_precip_2013.plot(ylabel='daily precipitation (in)', \n                       xlabel='date',\n                       title='Precipitation in Boulder, CO during 2013',\n                       legend=False)\n\n<AxesSubplot:title={'center':'Precipitation in Boulder, CO during 2013'}, xlabel='date', ylabel='daily precipitation (in)'>"
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#more-info",
    "href": "lectures/lesson-7-time-series.html#more-info",
    "title": "7Â  Basic time series",
    "section": "7.9 More info",
    "text": "7.9 More info\nAs usual, thereâ€™s so much more to learn about time series data. These documents will allow you to dive deeper:\nðŸ“– pandas getting started tutorials - How to handle time series data with ease\nðŸ“– Time Series Chapter, Python for Data Analysis, Wes McKinney\nðŸ“– pandas User Guide - Time series/date functionality"
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#acknowledgements",
    "href": "lectures/lesson-7-time-series.html#acknowledgements",
    "title": "7Â  Basic time series",
    "section": "7.10 Acknowledgements",
    "text": "7.10 Acknowledgements\nThis lesson was adapted from Dr.Â Sam Stevensonâ€™s lecture on Data quality control and outliers: 1D time series and Earth Labâ€™s Lesson 1. Work With Datetime Format in Python - Time Series Data."
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html",
    "title": "8Â  Preliminary Data Exploration",
    "section": "",
    "text": "9 References\nRachel King, Jenna Braun, Michael Westphal, & CJ Lortie. (2023). Compiled occurrence records for prey items of listed species found in California drylands with associated environmental data. Knowledge Network for Biocomplexity. doi:10.5063/F1VM49RH.\nLortie, C. J., Braun, J., King, R., & Westphal, M. (2023). The importance of open data describing prey item species lists for endangered species. Ecological Solutions and Evidence, 4(2), e12251. https://doi.org/10.1002/2688-8319.12251"
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#archive-exploration",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#archive-exploration",
    "title": "8Â  Preliminary Data Exploration",
    "section": "8.1 Archive exploration",
    "text": "8.1 Archive exploration\nFor many datasets, data exploration begins at the data repository. Take some time to look through the datasetâ€™s description in KNB. Discuss the following questions with your team:\n\nWhat is this data about?\nIs this data collected in-situ by the authors or is it a synthesis of multiple datasets?\nDuring what time frame were the observations in the dataset collected?\nDoes this dataset come with an associated metadata file?\nDoes the dataset contain sensitive data?\n\nIn your notebook: use a markdown cell to add a brief description of the dataset, including a citation, date of access, and a link to the archive.\n\ncheck git status -> stage changes -> check git status -> commit with message -> push changes"
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#xml-metadata-exploration",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#xml-metadata-exploration",
    "title": "8Â  Preliminary Data Exploration",
    "section": "8.2 .xml metadata exploration",
    "text": "8.2 .xml metadata exploration\nYou may have noticed there are two metadata files: Compiled_occurrence_records_for_prey_items_of.xml and metadata_arth_occurrences.csv.\n\nIn the archiveâ€™s dataset description, notice the .xml document file type is EML which stands for EML: Ecological Metadata Language.\nOpen the .xml file: thereâ€™s a lot going on. This is a machine-readable file that has metadata about the whole dataset. You can proably identify some items like title and creators.\nClose the file and delete it - we wonâ€™t use it today.\nYou donâ€™t need to write anything in your notebook about this section."
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#csv-metadata-exploration",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#csv-metadata-exploration",
    "title": "8Â  Preliminary Data Exploration",
    "section": "8.3 .csv metadata exploration",
    "text": "8.3 .csv metadata exploration\nBack in your notebook, import the pandas package using standard abbreviation in a code cell. Then follow these steps to read in the metadata csv using the pandas.read_csv() function:\n\nNavigate to the data package site and copy the URL to access the metadata_arth_occurrences csv file. To copy the URL:\n\n\nhover over the Download button â€“> right click â€“> â€œCopy Linkâ€.\n\n\nRead in the data from the URL using the pd.read_csv() function like this:\n# look at metadata\npd.read_csv('the URL goes here')\nTake a minute to look at the descriptions for the columns.\n\nNote: Not all datasets have column descriptions in a csv file. Often they come with a doc or txt file with information."
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#data-loading",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#data-loading",
    "title": "8Â  Preliminary Data Exploration",
    "section": "8.4 Data loading",
    "text": "8.4 Data loading\n\nFollow steps (a) and (b) from the previous exercise to read in the drylands prey data file arth_occurrences_with_env.csv using pd.read_csv(). Store the dataframe to a variable called prey like this:\n\n# read in data\nprey = pd.read_csv('the URL goes here')\n\nUse a Python function to see what is the type of the prey variable.\n\n\ncheck git status -> stage changes -> check git status -> commit with message -> push changes\n\n\nCHECK IN WITH YOUR TEAM\n\n\nMAKE SURE YOUâ€™VE ALL SUCCESSFULLY ACCESSED THE DATA BEFORE CONTINUING"
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#look-at-your-data",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#look-at-your-data",
    "title": "8Â  Preliminary Data Exploration",
    "section": "8.5 Look at your data",
    "text": "8.5 Look at your data\n\nRun prey in a cell. What do you notice in the columns section?\nTo see all the column names in the same display we need to set a pandas option. Run the following command and then look at the prey data again:\n\npd.set_option(\"display.max.columns\", None)\n\nAdd a comment explaining what pd.set_option(\"display.max.columns\", None) does.\n\n\ncheck git status -> stage changes -> check git status -> commit with message -> push changes"
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#pd.dataframe-preliminary-exploration",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#pd.dataframe-preliminary-exploration",
    "title": "8Â  Preliminary Data Exploration",
    "section": "8.6 pd.DataFrame preliminary exploration",
    "text": "8.6 pd.DataFrame preliminary exploration\nRun each of the following methods for prey in a different cell and write a brief description of what they do as a comment:\n\nhead()\ntail()\ninfo()\nnunique()\n\nFor example:\n# head()\n# returns the first five rows of the data frame\nprey.head()\nIf youâ€™re not sure about what the method does, try looking it up in the pandas.DataFrame documentation.\n\nCheck the documentation for head(). If this function has any optional parameters, change the default value to get a different output.\n\nPrint each of the following attributes of prey in a different cell and write a brief explanation of what they are as a comment:\n\nshape\ncolumns\ndtypes\n\nIf youâ€™re not sure about what info is the attribute showing, try looking it up in the pandas.DataFrame documentation.\n\ncheck git status -> stage changes -> check git status -> commit with message -> push changes"
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#update-some-column-names",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#update-some-column-names",
    "title": "8Â  Preliminary Data Exploration",
    "section": "8.7 Update some column names",
    "text": "8.7 Update some column names\nChange the column names of institutionCode and datasetKey to institution_code and dataset_key, respectively. Make sure youâ€™re actually updating the dataframe. HINT: yesterdayâ€™s class.\n\ncheck git status -> stage changes -> check git status -> commit with message -> push changes"
  }
]