[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "",
    "text": "Welcome!\nWelcome to the course materials for EDS 220 - Working with Environmental Datasets! This course is part of the UCSB Masters in Environmental Data Science.\nIn this website you will find the materials covered during lectures and discussion sections for the Fall 2023 term. This site will be under construction throughout the course."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "About",
    "text": "About\nThis hands-on course explores widely used environmental data formats and Python libraries for analyzing diverse environmental data. Students will gain experience working with popular open data repositories and cloud platforms to source and analyze real-world environmental datasets. The course will also serve as an introduction to Python programming and provide opportunities to practice effective communication of the strengths and weaknesses of students’ data products and analyses."
  },
  {
    "objectID": "index.html#instruction-team",
    "href": "index.html#instruction-team",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Instruction Team",
    "text": "Instruction Team\n\nInstructor\nCarmen Galaz García (she/her/hers)\nE-mail: galaz-garcia@nceas.ucsb.edu\nOffice hour: Wednesday 3:30-4:30 pm, NCEAS classroom\nBest way to contact me: e-mail\n\n\nTA\nYutian Fang (she/her/hers)\nE-mail: yutianfang@bren.ucsb.edu\nOffice Hours: Tuesday, 4-5 pm, Decker’s deck (Bren school).\nBest way to contact me: email"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Syllabus",
    "text": "Syllabus\nClick here to access the syllabus."
  },
  {
    "objectID": "index.html#calendar",
    "href": "index.html#calendar",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Calendar",
    "text": "Calendar\nThe following is our ideal calendar, the course content and calendar may be subject to change as the course progresses."
  },
  {
    "objectID": "index.html#about-this-website",
    "href": "index.html#about-this-website",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "About this website",
    "text": "About this website\nThis website is created with Quarto and is published through GitHub pages. Click here to go to the website’s GitHub repository."
  },
  {
    "objectID": "index.html#contribute",
    "href": "index.html#contribute",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "Contribute",
    "text": "Contribute\nIf you have suggestions on how to correct, improve, or expand these notes, please feel free to email galaz-garcia@nceas.ucsb.edu or file a GitHub issue."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "EDS 220 - Working with Environmental Datasets",
    "section": "License",
    "text": "License\nAll content in this book is licensed under the Creative Commons Attribution NonCommercial 4.0 International (CC BY-NC 4.0) license."
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#numpy",
    "href": "lectures/lesson-1-python-review.html#numpy",
    "title": "1  Python Review",
    "section": "1.1 numpy",
    "text": "1.1 numpy\nNumPy is one of the core packages for numerical computing in Python. Many of the packages we will use in this course use NumPy’s arrays as their building blocks. Additionally, numpy objects have been optimized for processing, so computations on them are really fast and use less memory than doing the equivalent using base Python.\nIn this lesson we will use numpy to review some core concepts in Python you’re already familiar with.\nFirst, let’s start by importing the library:\n\nimport numpy as np"
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#variables",
    "href": "lectures/lesson-1-python-review.html#variables",
    "title": "1  Python Review",
    "section": "1.2 Variables",
    "text": "1.2 Variables\nWe can think of a variable as a name we assign to a particular object in Python. For example:\n\n# assign a small array to variable a\na = np.array([[1,1,2],[3,5,8]])\n\nWhen we run the cell, we store the variables and their value. We can view a variable’s value in two ways:\n\nrunning a cell with the variable name\nusing the print function to print the value\n\n\n# show the value\na\n\narray([[1, 1, 2],\n       [3, 5, 8]])\n\n\n\n# print the value \nprint(a)\n\n[[1 1 2]\n [3 5 8]]\n\n\n\n\n\n\n\n\nR and Python\n\n\n\nIn Python we use the equal sign = to assign values to variables in the same way the left-arrow <- is used in R.\n\n\n\n\n\n\n\n\nNaming Variables\n\n\n\nThere are many ways of constructing multi-word variable names. In this course we will name variables using snake_case, where words are all in small caps and separated by underscores (ex: my_variable). This is the naming convention suggested by the Style Guide for Python Code."
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#variables-and-objects",
    "href": "lectures/lesson-1-python-review.html#variables-and-objects",
    "title": "1  Python Review",
    "section": "1.3 Variables and Objects",
    "text": "1.3 Variables and Objects\nYou will often encounter the word object in Python documentation and tutorials. Informally speaking, an object is a bundle of properties and actions about something specific. For example, an object could represent a data frame with properties such as number of rows, names of columns, and date created, and actions suchs as selecting a specific row or adding a new column.\nA variable is the name we give a specific object, and the same object can be referenced by different variables. An analogy for this is the following: the Sun (object) is called “sol” in Spanish and “soleil” in French, so two different names (variables) represent the same object. You can read more technical details about the difference between objects and variables in Python here.\nIn practice, we can often use the word variable and object interchangeably. I want to bring up what objects are so you’re not caught off-guard with vocabulary you’ll often encounter in the documentation, StackExchange, etc. We’ll often use the word object too (for example, in the next subsection!)."
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#types",
    "href": "lectures/lesson-1-python-review.html#types",
    "title": "1  Python Review",
    "section": "1.4 Types",
    "text": "1.4 Types\nEvery object in Python has a type, the type tells us what kind of object it is. We can also call the type of an object, the class of an object (so class and type both mean what kind of object we have).\nWe can see the type/class of a variable/object by using the type function:\n\nprint(a)\ntype(a)\n\n[[1 1 2]\n [3 5 8]]\n\n\nnumpy.ndarray\n\n\nThe numpy.ndarray is the core object/data type in the NumPy pakcage. We can check the type of an entry in the array by indexing:\n\nprint(a[0,0])\ntype(a[0,0])\n\n1\n\n\nnumpy.int64\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nHow would you access the value 5 in the array a? Remember indexing in Python starts from 0!\n\n\nNotice the type of the value 1 in the array is numpy.int64 and not just the core Python integer type int. The NumPy type numpy.int64 is telling us 1 is an integer stored as a 64-bit number. NumPy has its own data types to deal with numbers depending on memory storage and floating point precision, click here to know see all the types.\nSince “everything in Python is an object” and every object has a class, we will interact with SO MANY classes in this course. Often, knowing the type of an object is the first step to finding information to code what you want!"
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#functions",
    "href": "lectures/lesson-1-python-review.html#functions",
    "title": "1  Python Review",
    "section": "1.5 Functions",
    "text": "1.5 Functions\nprint was our first example of a Python function. Functions take in a set of arguments, separated by commas, and use those arguments to create some output. There are several built-in funcions in Python, most of them are for interacting with the basic Python data types. You can see a list of them here.\n\n\n\n\n\n\nArgument or Parameter?\n\n\n\nWe can interchangeably say arguments or parameters. You will see argument more often in the documentation.\n\n\nWe can ask for information about a function by executing ? followed by the function name:\n\n?print\n\n\nThe first line is always the function showing all of its arguments in parenthesis. Then there is a short description of what the function does. And finally a list of the arguments and a brief explanation about each of them.\nYou can see there are different types of arguments inside the parenthesis. Roughly speaking, a function has two types of arguments:\n\nnon-optional arguments: arguments you need to specify for the function to do something, and\noptional arguments: arguments that are pre-filled with a default value by the function, but you can override them. Optional arguments appear inside the parenthesis () in the form optional_argument = default_value.\n\nExample:\nend is an argument in print with the default value a new line. We can change this argument so that finishes the line with ^_^ instead:\n\n# notice we had always used print withough specifying any value for the `end` argument\nprint('I am changing the default end argument of the print function', end=' ^_^')\n\nI am changing the default end argument of the print function ^_^"
  },
  {
    "objectID": "lectures/lesson-1-python-review.html#attributes-methods",
    "href": "lectures/lesson-1-python-review.html#attributes-methods",
    "title": "1  Python Review",
    "section": "1.6 Attributes & Methods",
    "text": "1.6 Attributes & Methods\nAn object in Python has attributes and methods. An attribute is a property of the object, some piece of information about it. A method is a procedure associated with an object, so it is an action where the main ingredient is the object.\nFor example, these could be some attributes and methods a class cat:\n\n\n\n.\n\n\nMore formally, a method is a function that acts on the object it is part of.\nWe can access a variable’s attributes and methods by adding a period . at the end of the variable’s name. So we would write variable.variable_method() or variable.variable_attribute.\n\n\n\n\n\n\nCheck-in\n\n\n\nSuppose we have a class fish, make a diagram similar to the cat class diagram showing 3 attributes for the class and 3 methods.\n\n\nExample\nNumPy arrays have many methods and attributes. Let’s see some concrete examples.\n\n# define a 3x3 array\nvar = np.array([[1,2,3],[4,5,6],[7,8,9]])\nvar\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\n# T is an example of attribute, it returns the transpose of var\nprint(var.T)\nprint(type(var.T))\n\n[[1 4 7]\n [2 5 8]\n [3 6 9]]\n<class 'numpy.ndarray'>\n\n\n\n# shape, another attribute, tells us the shape of the array (3x3)\nprint(var.shape)\nprint(type(var.shape))\n\n(3, 3)\n<class 'tuple'>\n\n\n\n# ndim is an attribute holding the number of array dimensions\nprint(var.ndim)\nprint(type(var.ndim))\n\n2\n<class 'int'>\n\n\nNotice these attributes can have many different data types. Here we saw tuples and int, two of the core Python classes, and also a numpy array as attributes of var.\nNow some examples of methods:\n\n# the tolist method returns the array as a nested list of scalars\nvar.tolist()\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n\n# the min method returns the minimum value in the array along an axis\nvar.min(axis=0)\n\narray([1, 2, 3])\n\n\n\n\n\n\n\n\nCheck-in\n\n\n\nWe can also call the min method without any parameters:\n\nvar.min()\n\n1\n\n\nWhat kind of parameter is axis in our previous call of the var method?\n\n\nRemember, methods are functions associated to an object. We can check this!\n\ntype(var.tolist)\n\nbuiltin_function_or_method\n\n\n\ntype(var.min)\n\nbuiltin_function_or_method\n\n\nYou can see a complete list of NumPy array’s methods and attributes in the documentation.\n\n\n\n\n\n\nR and Python\n\n\n\nIn R we don’t use methods within an object. Rather, functions are extrinsic to (outside) the objects they are acting on. In R, for example, there would be two separate items: the variable var and a separate function min that gets var as a parameter:\n# this is R code\nvar <- array(c(1,4,7,2,5,8,3,6,9), dim =c(3,3))\nmin(var)\nUsing the pipe operator %>% in R’s tidyverse is closer to the dot . in Python:\n# this is R code\nvar <- array(c(1,4,7,2,5,8,3,6,9), dim =c(3,3))\nvar %>% min()\nWhat happens here is that the pipe %>% is passing var to the min() function as its first argument. This is essentially what happens in Python when a function is a method of a class:\n# this is Python code\nvar = np.array([[1,2,3],[4,5,6],[7,8,9]])\nvar.min()\nWhen working in Python, remember that methods are functions that are part of an object and a method uses the object it is part of to produce some information."
  },
  {
    "objectID": "lectures/lesson-2-series-dataframes.html#pandas",
    "href": "lectures/lesson-2-series-dataframes.html#pandas",
    "title": "2  pandas",
    "section": "2.1 pandas",
    "text": "2.1 pandas\npandas is a Python package to wrangle and analyze tabular data. It is built on top of NumPy and has become the core tool for doing data analysis in Python.\nThe convention to import it is:\n\nimport pandas as pd\n\n# we will also import numpy \nimport numpy as np\n\nThere is so much to learn about pandas. While we won’t be able to cover every single functionality of this package in the next three lecutres, the goal is to get you started with the basic tools for data wrangling and give you a solid basis on which you can explore further."
  },
  {
    "objectID": "lectures/lesson-2-series-dataframes.html#series",
    "href": "lectures/lesson-2-series-dataframes.html#series",
    "title": "2  pandas",
    "section": "2.2 Series",
    "text": "2.2 Series\nThe first core data structure of pandas is the series. A series is a one-dimensional array of indexed data. A pandas.Series having an index is the main difference between a pandas.Series and a numpy array. See the difference:\n\n# a numpy array\n# np.random.randn returns values from the std normal distribution\narr = np.random.randn(4) \nprint(type(arr))\nprint(arr, \"\\n\")\n\n# a pandas series made from the previous array\ns = pd.Series(arr)\nprint(type(s))\nprint(s)\n\n<class 'numpy.ndarray'>\n[ 1.20320302  2.77738979  0.68087609 -0.87104766] \n\n<class 'pandas.core.series.Series'>\n0    1.203203\n1    2.777390\n2    0.680876\n3   -0.871048\ndtype: float64\n\n\n\n2.2.1 Creating a pandas.Series\nThe basic method to create a pandas.Series is to call\ns = pd.Series(data, index=index)\nThe data parameter can be:\n\na numpy array or a list\na Python dictionary\na number\n\nThe index parameter is a list of index labels.\nFor now, we will create a pandas.Series from a numpy array or list. To use this method we need to pass a numpy array (or a list of objects that can be converted to NumPy types) as data and a list of indices of the same length as data.\n\n# a Series from a numpy array \npd.Series(np.arange(3), index=['a','b','c'])\n\na    0\nb    1\nc    2\ndtype: int64\n\n\nThe index parameter is optional. If we don’t include it, the default is to make the index equal to [0,...,len(data)-1]. For example:\n\n# a Series from a list of strings with default index\npd.Series(['EDS 220', 'EDS 222', 'EDS 223', 'EDS 242'])\n\n0    EDS 220\n1    EDS 222\n2    EDS 223\n3    EDS 242\ndtype: object\n\n\n\n2.2.1.1 From a dictionary\nRemember a dictionary is a set of key-value pairs. If we create a pandas.Series via a dictionary the keys will become the index and the values the corresponding data.\n\n# construct dictionary\nd = {'a':0, 'b':1, 'c':2}\n\n# initialize a sries using a dictionary\npd.Series(d)\n\na    0\nb    1\nc    2\ndtype: int64\n\n\n\n\n2.2.1.2 From a number\nIf we only provide a number as the data for the series, we need to provide an index. The number will be repeated to match the length of the index.\n\npd.Series(3.0, index = ['A', 'B', 'C'])\n\nA    3.0\nB    3.0\nC    3.0\ndtype: float64\n\n\n\n\n\n2.2.2 Simple operations\nArithmetic operations work on series and also most NumPy functions. For example:\n\n# define a series\ns = pd.Series([98,73,65],index=['Andrea', 'Beth', 'Carolina'])\n\n# divide each element in series by 10\nprint(s /10, '\\n')\n\n# take the exponential of each element in series\nprint(np.exp(s), '\\n')\n\n# notice this doesn't change the values of our series\nprint(s)\n\nAndrea      9.8\nBeth        7.3\nCarolina    6.5\ndtype: float64 \n\nAndrea      3.637971e+42\nBeth        5.052394e+31\nCarolina    1.694889e+28\ndtype: float64 \n\nAndrea      98\nBeth        73\nCarolina    65\ndtype: int64\n\n\nWe can also produce new pandas.Series with True/False values indicating whether the elements in a series satisfy a condition or not:\n\ns > 10\n\nAndrea      True\nBeth        True\nCarolina    True\ndtype: bool\n\n\nThis kind of simple conditions on pandas.Series will be key when we are selecting data from data frames.\n\n\n\n2.2.3 Attributes & Methods\npandas.Series have many attributes and methods, you can see a full list in the pandas documentation. For now we will cover two examples that have to do with identifying missing values.\npandas represents a missing or NA value with NaN, which stands for not a number. Let’s construct a small series with some NA values:\n\n# series with NAs in it\ns = pd.Series([1, 2, np.NaN, 4, np.NaN])\n\nA pandas.Series has an attribute called hasnans that returns True if there are any NaNs:\n\n# check if series has NAs\ns.hasnans\n\nTrue\n\n\nThen we might be intersted in knowing which elements in the series are NAs. We can do this using the isna method:\n\ns.isna()\n\n0    False\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n\n\nWe can see the ouput is a pd.Series of boolean values indicating if an element in the row at the given index is NA (True = is NA) or not (False = not NA).\n\n\n\n\n\n\nmoving on\n\n\n\nThere’s much more to say about pandas.Series, but this is enought to get us going. At this point, we mainly want to know about pandas.Series because pandas.Series are the columns of pandas.DataFrames.\n\n\n\n\n\n\n\n\nslicing with loc\n\n\n\nNotice that when use slicing with loc we get both the start and the end of the indices we indicated. This is different to slicing in numpy arrays or lists where we do not get the element at the end of the slice. Compare the following:\n\nx = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(x)\n\n# slicing will return elements at indices 2 trhough 4 (inclusive)\nx[2:5]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n[2, 3, 4]\n\n\n\n# define a np array with integers from 0 to 9\ny = np.arange(10)\nprint(y)\n\n# slicing will return elements at indices 2 trhough 4 (inclusive)\ny[2:5]\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\narray([2, 3, 4])\n\n\n\n z = pd.Series(y)\n print(z)\n\n# slicing will return elements with index labels 2 through 5 (inclusive)\n z.loc[2:5]\n\n0    0\n1    1\n2    2\n3    3\n4    4\n5    5\n6    6\n7    7\n8    8\n9    9\ndtype: int64\n\n\n2    2\n3    3\n4    4\n5    5\ndtype: int64"
  },
  {
    "objectID": "lectures/lesson-2-series-dataframes.html#data-frames",
    "href": "lectures/lesson-2-series-dataframes.html#data-frames",
    "title": "2  pandas",
    "section": "2.3 Data Frames",
    "text": "2.3 Data Frames\nThe Data Frame is the most used pandas object. It represents tabular data and we can think of it as a spreadhseet. Each column of a pandas.DataFrame is a pandas.Series.\n\n2.3.1 Creating a pandas.DataFrame\nThere are many ways of creating a pandas.DataFrame.\nWe already mentioned each column of a pandas.DataFrame is a pandas.Series. In fact, the pandas.DataFrame is a dictionary of pandas.Series, with each column name being the key and the column values being the key’s value. Thus, we can create a pandas.DataFrame in this way:\n\n# initialize dictionary with columns' data \nd = {'col_name_1' : pd.Series(np.arange(3)),\n     'col_name_2' : pd.Series([3.1, 3.2, 3.3]),\n     }\n\n# create data frame\ndf = pd.DataFrame(d)\ndf\n\n\n\n\n\n  \n    \n      \n      col_name_1\n      col_name_2\n    \n  \n  \n    \n      0\n      0\n      3.1\n    \n    \n      1\n      1\n      3.2\n    \n    \n      2\n      2\n      3.3\n    \n  \n\n\n\n\nWe can change the index and column names by changing the index and columns attributes in the data frame.\n\n# print original index\nprint(df.index)\n\n# change the index\ndf.index = ['a','b','c']\ndf\n\nRangeIndex(start=0, stop=3, step=1)\n\n\n\n\n\n\n  \n    \n      \n      col_name_1\n      col_name_2\n    \n  \n  \n    \n      a\n      0\n      3.1\n    \n    \n      b\n      1\n      3.2\n    \n    \n      c\n      2\n      3.3\n    \n  \n\n\n\n\n\n# print original column names\nprint(df.columns)\n\n# change column names \ndf.columns = ['C1','C2']\ndf\n\nIndex(['col_name_1', 'col_name_2'], dtype='object')\n\n\n\n\n\n\n  \n    \n      \n      C1\n      C2\n    \n  \n  \n    \n      a\n      0\n      3.1\n    \n    \n      b\n      1\n      3.2\n    \n    \n      c\n      2\n      3.3"
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html",
    "title": "3  * Lab: Preliminary data exploration",
    "section": "",
    "text": "4 References\nRachel King, Jenna Braun, Michael Westphal, & CJ Lortie. (2023). Compiled occurrence records for prey items of listed species found in California drylands with associated environmental data. Knowledge Network for Biocomplexity. doi:10.5063/F1VM49RH.\nLortie, C. J., Braun, J., King, R., & Westphal, M. (2023). The importance of open data describing prey item species lists for endangered species. Ecological Solutions and Evidence, 4(2), e12251. https://doi.org/10.1002/2688-8319.12251"
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#archive-exploration",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#archive-exploration",
    "title": "3  * Lab: Preliminary data exploration",
    "section": "3.1 Archive exploration",
    "text": "3.1 Archive exploration\nFor many datasets, data exploration begins at the data repository. Take some time to look through the dataset’s description in KNB. Discuss the following questions with your team:\n\nWhat is this data about?\nIs this data collected in-situ by the authors or is it a synthesis of multiple datasets?\nDuring what time frame were the observations in the dataset collected?\nDoes this dataset come with an associated metadata file?\nDoes the dataset contain sensitive data?\n\nIn your notebook: use a markdown cell to add a brief description of the dataset, including a citation, date of access, and a link to the archive.\n\ncheck git status -> stage changes -> check git status -> commit with message -> push changes"
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#xml-metadata-exploration",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#xml-metadata-exploration",
    "title": "3  * Lab: Preliminary data exploration",
    "section": "3.2 .xml metadata exploration",
    "text": "3.2 .xml metadata exploration\nYou may have noticed there are two metadata files: Compiled_occurrence_records_for_prey_items_of.xml and metadata_arth_occurrences.csv.\n\nIn the archive’s dataset description, notice the .xml document file type is EML which stands for EML: Ecological Metadata Language.\nOpen the .xml file: there’s a lot going on. This is a machine-readable file that has metadata about the whole dataset. You can proably identify some items like title and creators.\nClose the file and delete it - we won’t use it today.\nYou don’t need to write anything in your notebook about this section."
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#csv-metadata-exploration",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#csv-metadata-exploration",
    "title": "3  * Lab: Preliminary data exploration",
    "section": "3.3 .csv metadata exploration",
    "text": "3.3 .csv metadata exploration\nBack in your notebook, import the pandas package using standard abbreviation in a code cell. Then follow these steps to read in the metadata csv using the pandas.read_csv() function:\n\nNavigate to the data package site and copy the URL to access the metadata_arth_occurrences csv file. To copy the URL:\n\n\nhover over the Download button –> right click –> “Copy Link”.\n\n\nRead in the data from the URL using the pd.read_csv() function like this:\n# look at metadata\npd.read_csv('the URL goes here')\nTake a minute to look at the descriptions for the columns.\n\nNote: Not all datasets have column descriptions in a csv file. Often they come with a doc or txt file with information."
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#data-loading",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#data-loading",
    "title": "3  * Lab: Preliminary data exploration",
    "section": "3.4 Data loading",
    "text": "3.4 Data loading\n\nFollow steps (a) and (b) from the previous exercise to read in the drylands prey data file arth_occurrences_with_env.csv using pd.read_csv(). Store the dataframe to a variable called prey like this:\n\n# read in data\nprey = pd.read_csv('the URL goes here')\n\nUse a Python function to see what is the type of the prey variable.\n\n\ncheck git status -> stage changes -> check git status -> commit with message -> push changes\n\n\nCHECK IN WITH YOUR TEAM\n\n\nMAKE SURE YOU’VE ALL SUCCESSFULLY ACCESSED THE DATA BEFORE CONTINUING"
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#look-at-your-data",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#look-at-your-data",
    "title": "3  * Lab: Preliminary data exploration",
    "section": "3.5 Look at your data",
    "text": "3.5 Look at your data\n\nRun prey in a cell. What do you notice in the columns section?\nTo see all the column names in the same display we need to set a pandas option. Run the following command and then look at the prey data again:\n\npd.set_option(\"display.max.columns\", None)\n\nAdd a comment explaining what pd.set_option(\"display.max.columns\", None) does.\n\n\ncheck git status -> stage changes -> check git status -> commit with message -> push changes"
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#pd.dataframe-preliminary-exploration",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#pd.dataframe-preliminary-exploration",
    "title": "3  * Lab: Preliminary data exploration",
    "section": "3.6 pd.DataFrame preliminary exploration",
    "text": "3.6 pd.DataFrame preliminary exploration\nRun each of the following methods for prey in a different cell and write a brief description of what they do as a comment:\n\nhead()\ntail()\ninfo()\nnunique()\n\nFor example:\n# head()\n# returns the first five rows of the data frame\nprey.head()\nIf you’re not sure about what the method does, try looking it up in the pandas.DataFrame documentation.\n\nCheck the documentation for head(). If this function has any optional parameters, change the default value to get a different output.\n\nPrint each of the following attributes of prey in a different cell and write a brief explanation of what they are as a comment:\n\nshape\ncolumns\ndtypes\n\nIf you’re not sure about what info is the attribute showing, try looking it up in the pandas.DataFrame documentation.\n\ncheck git status -> stage changes -> check git status -> commit with message -> push changes"
  },
  {
    "objectID": "discussion-sections/ds1-anthr-prelim-data-exploration.html#update-some-column-names",
    "href": "discussion-sections/ds1-anthr-prelim-data-exploration.html#update-some-column-names",
    "title": "3  * Lab: Preliminary data exploration",
    "section": "3.7 Update some column names",
    "text": "3.7 Update some column names\nChange the column names of institutionCode and datasetKey to institution_code and dataset_key, respectively. Make sure you’re actually updating the dataframe. HINT: yesterday’s class.\n\ncheck git status -> stage changes -> check git status -> commit with message -> push changes"
  },
  {
    "objectID": "lectures/lesson-3-pandas-subsetting.html#subsetting-a-pandas.dataframe",
    "href": "lectures/lesson-3-pandas-subsetting.html#subsetting-a-pandas.dataframe",
    "title": "4  Subsetting",
    "section": "4.1 Subsetting a pandas.DataFrame",
    "text": "4.1 Subsetting a pandas.DataFrame\nLike it’s often the case when working with pandas, there are many ways in which we can subset a data frame. We will review the core methods to do this. \nFor all examples we will use simplified data (glacial_loss.csv) from the National Snow and Ice Data Center (Original dataset). The column descriptions are:\n\nyear: ​calendar year\neurope - antarctica: ​change in glacial volume (km3​ ​) in each region that year\nglobal_glacial_volume_change: ​cumulative​ global glacial volume change (km3), starting in 1961\nannual_sea_level_rise: ​annual rise in sea level (mm)\ncumulative_sea_level_rise:​ cumulative rise in sea level (mm) since 1961\n\nFirst, we read-in the file and get some baisc information about this data frame:\n\n# import pandas\nimport pandas as pd\n\n# read in file\ndf = pd.read_csv('data/lesson-1/glacial_loss.csv')\n\n# see the first five rows\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      0\n      1961\n      -5.128903\n      -108.382987\n      -18.721190\n      -32.350759\n      -14.359007\n      -4.739367\n      -35.116389\n      -220.823515\n      0.610010\n      0.610010\n    \n    \n      1\n      1962\n      5.576282\n      -173.252450\n      -24.324790\n      -4.675440\n      -2.161842\n      -13.694367\n      -78.222887\n      -514.269862\n      0.810625\n      1.420635\n    \n    \n      2\n      1963\n      -10.123105\n      -0.423751\n      -2.047567\n      -3.027298\n      -27.535881\n      3.419633\n      3.765109\n      -550.575640\n      0.100292\n      1.520927\n    \n    \n      3\n      1964\n      -4.508358\n      20.070148\n      0.477800\n      -18.675385\n      -2.248286\n      20.732633\n      14.853096\n      -519.589859\n      -0.085596\n      1.435331\n    \n    \n      4\n      1965\n      10.629385\n      43.695389\n      -0.115332\n      -18.414602\n      -19.398765\n      6.862102\n      22.793484\n      -473.112003\n      -0.128392\n      1.306939\n    \n  \n\n\n\n\n\n# get column names\ndf.columns\n\nIndex(['year', 'europe', 'arctic', 'alaska', 'asia', 'north_america',\n       'south_america', 'antarctica', 'global_glacial_volume_change',\n       'annual_sea_level_rise', 'cumulative_sea_level_rise'],\n      dtype='object')\n\n\n\n# check the data types of each column\ndf.dtypes\n\nyear                              int64\neurope                          float64\narctic                          float64\nalaska                          float64\nasia                            float64\nnorth_america                   float64\nsouth_america                   float64\nantarctica                      float64\nglobal_glacial_volume_change    float64\nannual_sea_level_rise           float64\ncumulative_sea_level_rise       float64\ndtype: object\n\n\n\n# data frame's shape: output is a tuple (# rows, # columns)\ndf.shape\n\n(43, 11)\n\n\n\n4.1.1 Selecting a single column…\n\n4.1.1.1 …by column name\nThis is the simplest case for selecting data. Suppose we are interested in the annual sea level rise. Then we can access that single column in this way:\n\n# seelect a single column by using square brackets []\nannual_rise = df['annual_sea_level_rise']\n\n# check the type of the ouput\nprint(type(annual_rise))\n\nannual_rise.head()\n\n<class 'pandas.core.series.Series'>\n\n\n0    0.610010\n1    0.810625\n2    0.100292\n3   -0.085596\n4   -0.128392\nName: annual_sea_level_rise, dtype: float64\n\n\nSince we only selected a single column the output is a pandas.Series.\n\n\n\n\n\n\npd.DataFrame = dictionary of columns\n\n\n\nRemember we can think of a pandas.DataFrame as a dictionary of its columns? Then we can access a single column using the column name as the key, just like we would do in a dictionary. That is the we just used: df['column_name'].\n\n\nThis is an example of selecting by label, which means we want to select data from our data frame using the names of the columns, not their position.\n\n\n4.1.1.2 … with attribute syntax\nWe can also access a single column by using attribute syntax:\n\nannual_rise_2 = df.annual_sea_level_rise\nannual_rise_2.head()\n\n0    0.610010\n1    0.810625\n2    0.100292\n3   -0.085596\n4   -0.128392\nName: annual_sea_level_rise, dtype: float64\n\n\n\n\n\n4.1.2 Selecting multiple columns…\n\n4.1.2.1 … using a list of column names\nThis is another example of selecting by labels. We just need to pass a list with the column names to the square brackets []. For example, say we want to look at the change in glacial volume in Europe and Asia, then we can select those columns like this:\n\n# select columns with names \"europe\" and \"asia\"\neurope_asia = df[['europe','asia']]\n\nNotice there are double square brackets. This is because we are passing the list of names ['europe','asia'] to the selection brakcets [].\n\n# check the type of the resulting selection\nprint(type(europe_asia))\n\n# check the shape of the selection\nprint((europe_asia.shape))\n\n<class 'pandas.core.frame.DataFrame'>\n(43, 2)\n\n\n\n\n4.1.2.2 … using a slice\nYet another example of selecting by label! In this case we will use the loc selection. The general syntax is\ndf.loc[ row-selection , column-selection]\nwhere row-selection and column-selection are the rows and columns we want to subset from the data frame.\nLet’s start by a simple example, where we want to select a slice of columns, say the change in glacial volume per year in all regions. This corresponds to all columns between europe and antarctica.\n\n# select all columns between 'arctic' and 'antarctica'\nall_regions = df.loc[:,'europe':'antarctica']\nall_regions.head()\n\n\n\n\n\n  \n    \n      \n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n    \n  \n  \n    \n      0\n      -5.128903\n      -108.382987\n      -18.721190\n      -32.350759\n      -14.359007\n      -4.739367\n      -35.116389\n    \n    \n      1\n      5.576282\n      -173.252450\n      -24.324790\n      -4.675440\n      -2.161842\n      -13.694367\n      -78.222887\n    \n    \n      2\n      -10.123105\n      -0.423751\n      -2.047567\n      -3.027298\n      -27.535881\n      3.419633\n      3.765109\n    \n    \n      3\n      -4.508358\n      20.070148\n      0.477800\n      -18.675385\n      -2.248286\n      20.732633\n      14.853096\n    \n    \n      4\n      10.629385\n      43.695389\n      -0.115332\n      -18.414602\n      -19.398765\n      6.862102\n      22.793484\n    \n  \n\n\n\n\nNotice two things:\n\nwe used the colon : as the row-selection parameter, which means “select all the rows”\nthe slice of the data frame we got includes both endpoints of the slice 'arctic':'antarctica'. In other words we get the europe column and the antarctica column. This is different from how slicing works in base Python and NumPy, where the endpoint is not included.\n\n\n\n\n4.1.3 Selecting rows…\nNow that we are familiar with some methods for selecting columns, let’s move on to selecting rows.\n\n4.1.3.1 … using a condition\nSelecting which rows satisfy a particular condition is, in my experience, the most usual kind of row subsetting. The general syntax for this type of selection is df[condition_on_rows]. For example, suppose we are intersted in all data after 1996. We can select those rows in this way:\n\n# select all rows with year > 1996\nafter_96 = df[df['year']>1996]\nafter_96\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      36\n      1997\n      -13.724106\n      -24.832246\n      -167.229145\n      -34.406403\n      -27.680661\n      -38.213286\n      -20.179090\n      -4600.686013\n      0.909625\n      12.709077\n    \n    \n      37\n      1998\n      -13.083338\n      -110.429302\n      -107.879027\n      -58.115702\n      30.169987\n      -3.797978\n      -48.129928\n      -4914.831966\n      0.867807\n      13.576884\n    \n    \n      38\n      1999\n      -8.039555\n      -64.644068\n      -87.714653\n      -26.211723\n      5.888512\n      -8.038630\n      -40.653001\n      -5146.368231\n      0.639603\n      14.216487\n    \n    \n      39\n      2000\n      -17.008590\n      -96.494055\n      -44.445000\n      -37.518173\n      -29.191986\n      -2.767698\n      -58.873830\n      -5435.317175\n      0.798202\n      15.014688\n    \n    \n      40\n      2001\n      -8.419109\n      -145.415483\n      -55.749505\n      -35.977022\n      -0.926134\n      7.553503\n      -86.774675\n      -5764.039931\n      0.908074\n      15.922762\n    \n    \n      41\n      2002\n      -3.392361\n      -48.718943\n      -87.120000\n      -36.127226\n      -27.853498\n      -13.484593\n      -30.203960\n      -6013.225500\n      0.688358\n      16.611120\n    \n    \n      42\n      2003\n      -3.392361\n      -48.718943\n      -67.253634\n      -36.021991\n      -75.066475\n      -13.223430\n      -30.203960\n      -6289.640976\n      0.763579\n      17.374699\n    \n  \n\n\n\n\nLet’s break down what is happening here. In this case the condition for our rows is df['year']>1996, this checks which rows have a value greater than 1996 in the year column. Let’s see this explicitely:\n\n# check the type of df['year']>1996\nprint(type(df['year']>1996))\n\ndf['year']>1996\n\n<class 'pandas.core.series.Series'>\n\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13    False\n14    False\n15    False\n16    False\n17    False\n18    False\n19    False\n20    False\n21    False\n22    False\n23    False\n24    False\n25    False\n26    False\n27    False\n28    False\n29    False\n30    False\n31    False\n32    False\n33    False\n34    False\n35    False\n36     True\n37     True\n38     True\n39     True\n40     True\n41     True\n42     True\nName: year, dtype: bool\n\n\nThe output is a pandas.Series with boolean values (True or False) indicating which rows satisfy the condition year>1996. When we pass such a series of boolean values to the selection brackets [] we keep only those rows with a True value.\nHere’s another example of using a condition. Suppose we want to look at data from years 1970 to 1979. One way of doing this is to use the in operator in our condition:\n\nseventies = df[df['year'].isin(range(1970,1980))]\nseventies\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      9\n      1970\n      -6.452316\n      -24.494667\n      -0.125296\n      -36.120199\n      11.619790\n      11.636911\n      4.400377\n      -999.018177\n      0.110225\n      2.759719\n    \n    \n      10\n      1971\n      0.414711\n      -42.904189\n      28.103328\n      -8.702938\n      -9.964542\n      1.061299\n      -6.735536\n      -1038.104459\n      0.107973\n      2.867692\n    \n    \n      11\n      1972\n      -5.144729\n      -27.004031\n      -22.143350\n      -40.883357\n      32.363730\n      -14.968034\n      -6.223849\n      -1122.885506\n      0.234202\n      3.101894\n    \n    \n      12\n      1973\n      4.081090\n      9.839444\n      22.985188\n      -31.432594\n      -20.883232\n      2.103649\n      10.539823\n      -1125.677743\n      0.007713\n      3.109607\n    \n    \n      13\n      1974\n      1.545615\n      -40.126998\n      -29.517874\n      -43.861622\n      -23.991402\n      -21.338825\n      4.419343\n      -1279.964287\n      0.426206\n      3.535813\n    \n    \n      14\n      1975\n      7.431192\n      -32.410467\n      -44.094084\n      -43.357442\n      -30.858810\n      -2.368842\n      -7.775315\n      -1434.818037\n      0.427773\n      3.963586\n    \n    \n      15\n      1976\n      3.986753\n      21.686639\n      -28.234725\n      -67.292125\n      -12.534421\n      -19.465358\n      19.250607\n      -1518.185129\n      0.230296\n      4.193882\n    \n    \n      16\n      1977\n      4.891410\n      -33.123010\n      -5.662139\n      -62.165684\n      -15.905332\n      2.654950\n      -23.727249\n      -1652.453400\n      0.370907\n      4.564788\n    \n    \n      17\n      1978\n      8.404591\n      -77.561015\n      -12.503384\n      -22.858040\n      -31.097609\n      7.127708\n      -9.140167\n      -1791.355022\n      0.383706\n      4.948495\n    \n    \n      18\n      1979\n      3.916703\n      -88.351684\n      -63.938851\n      -49.242043\n      -12.076624\n      -17.718503\n      -9.578557\n      -2030.537848\n      0.660726\n      5.609221\n    \n  \n\n\n\n\nLet’s break it down:\n\ndf['year'] is the column with the year values, a pandas.Series,\nin df['year'].isin(), we have that isin is a method for the pandas.Series and we are calling it using the dot ..\nrange(1970,1980) constructs consecutive integers from 1970 to 1979 - remember the right endopoint (1980) is not included!\ndf['year'].isin(range(1970,1980)) is then a pandas.Series of boolean values indicating which rows have year equal to 1970, …, 1979.\nwhen we put df['year'].isin(range(1970,1980)) inside the selection brackets [] we obtain the rows of the data frame with year equal to 1970, …, 1979.\n\n\n\n\n\n\n\nloc for row selection\n\n\n\nIt is equivalent to write\n# select rows with year<1965\ndf[df['year'] < 1965]\nand\n# select rows with year<1965 using love\ndf.loc[ df['year'] <1965 , :]\nIn the second one:\n\nwe are using the df.loc[ row-selection , column-selection] syntax\nthe row-selection parameter is the condition df['year']<1965\nthe column-selection parameter is a colon :, which indicates we want all columns for the rows we are selecting.\n\nWe prefer the first syntax when we are selecting rows and not columns since it is simpler.\n\n\n\n\n4.1.3.2 … using multiple conditions\nWe can combine multipe conditions by surrounding each one in parenthesis () and using the or operator | and the and operator &.\nor example:\n\n# select rows with \n# annual_sea_level_rise<0.5 mm OR annual_sea_level_rise>0.8 mm\n\ndf[ (df['annual_sea_level_rise']<0.5) | (df['annual_sea_level_rise']>0.8)]\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      0\n      1961\n      -5.128903\n      -108.382987\n      -18.721190\n      -32.350759\n      -14.359007\n      -4.739367\n      -35.116389\n      -220.823515\n      0.610010\n      0.610010\n    \n    \n      1\n      1962\n      5.576282\n      -173.252450\n      -24.324790\n      -4.675440\n      -2.161842\n      -13.694367\n      -78.222887\n      -514.269862\n      0.810625\n      1.420635\n    \n    \n      2\n      1963\n      -10.123105\n      -0.423751\n      -2.047567\n      -3.027298\n      -27.535881\n      3.419633\n      3.765109\n      -550.575640\n      0.100292\n      1.520927\n    \n    \n      3\n      1964\n      -4.508358\n      20.070148\n      0.477800\n      -18.675385\n      -2.248286\n      20.732633\n      14.853096\n      -519.589859\n      -0.085596\n      1.435331\n    \n    \n      4\n      1965\n      10.629385\n      43.695389\n      -0.115332\n      -18.414602\n      -19.398765\n      6.862102\n      22.793484\n      -473.112003\n      -0.128392\n      1.306939\n    \n  \n\n\n\n\nand example\n\n# select rows with cumulative_sea_level_rise>10 AND  global_glacial_volume_change<-300\ndf[ (df['cumulative_sea_level_rise']>10) & (df['global_glacial_volume_change']<-300)]\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      32\n      1993\n      16.685013\n      -73.666274\n      -43.702040\n      -65.995130\n      -33.151246\n      -20.578403\n      -20.311577\n      -3672.582082\n      0.671126\n      10.145254\n    \n    \n      33\n      1994\n      0.741751\n      -3.069084\n      -59.962273\n      -59.004710\n      -89.506142\n      -15.258449\n      -8.168498\n      -3908.977191\n      0.653025\n      10.798280\n    \n    \n      34\n      1995\n      -2.139665\n      -58.167778\n      -74.141762\n      3.500155\n      -0.699374\n      -19.863392\n      -25.951496\n      -4088.082873\n      0.494767\n      11.293047\n    \n    \n      35\n      1996\n      -6.809834\n      -4.550205\n      -74.847017\n      -67.436591\n      4.867530\n      -21.080115\n      -11.781489\n      -4271.401594\n      0.506405\n      11.799452\n    \n    \n      36\n      1997\n      -13.724106\n      -24.832246\n      -167.229145\n      -34.406403\n      -27.680661\n      -38.213286\n      -20.179090\n      -4600.686013\n      0.909625\n      12.709077\n    \n    \n      37\n      1998\n      -13.083338\n      -110.429302\n      -107.879027\n      -58.115702\n      30.169987\n      -3.797978\n      -48.129928\n      -4914.831966\n      0.867807\n      13.576884\n    \n    \n      38\n      1999\n      -8.039555\n      -64.644068\n      -87.714653\n      -26.211723\n      5.888512\n      -8.038630\n      -40.653001\n      -5146.368231\n      0.639603\n      14.216487\n    \n    \n      39\n      2000\n      -17.008590\n      -96.494055\n      -44.445000\n      -37.518173\n      -29.191986\n      -2.767698\n      -58.873830\n      -5435.317175\n      0.798202\n      15.014688\n    \n    \n      40\n      2001\n      -8.419109\n      -145.415483\n      -55.749505\n      -35.977022\n      -0.926134\n      7.553503\n      -86.774675\n      -5764.039931\n      0.908074\n      15.922762\n    \n    \n      41\n      2002\n      -3.392361\n      -48.718943\n      -87.120000\n      -36.127226\n      -27.853498\n      -13.484593\n      -30.203960\n      -6013.225500\n      0.688358\n      16.611120\n    \n    \n      42\n      2003\n      -3.392361\n      -48.718943\n      -67.253634\n      -36.021991\n      -75.066475\n      -13.223430\n      -30.203960\n      -6289.640976\n      0.763579\n      17.374699\n    \n  \n\n\n\n\n\n\n4.1.3.3 … by position\nAll the selections we have done so far have been using labels or using a condition. Sometimes we might want to select certain rows depending on their actual position in the data frame. In this case we use iloc selection with the syntax df.iloc[row-indices]. iloc stands for integer-location based indexing. Let’s see some examples:\n\n# select the fifht row = index 4\ndf.iloc[4]\n\nyear                            1965.000000\neurope                            10.629385\narctic                            43.695389\nalaska                            -0.115332\nasia                             -18.414602\nnorth_america                    -19.398765\nsouth_america                      6.862102\nantarctica                        22.793484\nglobal_glacial_volume_change    -473.112003\nannual_sea_level_rise             -0.128392\ncumulative_sea_level_rise          1.306939\nName: 4, dtype: float64\n\n\n\n# select rows 23 through 30, inclduing 30\ndf.iloc[23:31]\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n      arctic\n      alaska\n      asia\n      north_america\n      south_america\n      antarctica\n      global_glacial_volume_change\n      annual_sea_level_rise\n      cumulative_sea_level_rise\n    \n  \n  \n    \n      23\n      1984\n      8.581427\n      -5.755672\n      -33.466092\n      -20.528535\n      -20.734676\n      -8.267686\n      -3.261011\n      -2569.339802\n      0.232609\n      7.097624\n    \n    \n      24\n      1985\n      -5.970980\n      -49.651089\n      12.065473\n      -31.571622\n      -33.833985\n      10.072906\n      -13.587886\n      -2682.857926\n      0.313586\n      7.411210\n    \n    \n      25\n      1986\n      -5.680642\n      22.900847\n      7.557447\n      -18.920773\n      -33.014743\n      -4.652030\n      30.482473\n      -2684.197632\n      0.003701\n      7.414911\n    \n    \n      26\n      1987\n      8.191477\n      12.387780\n      -24.007862\n      -41.121970\n      -48.560996\n      1.670733\n      3.130190\n      -2773.325568\n      0.246210\n      7.661120\n    \n    \n      27\n      1988\n      -11.117228\n      -31.066489\n      49.897712\n      -21.300712\n      -46.545435\n      13.460422\n      -37.986834\n      -2858.767621\n      0.236028\n      7.897148\n    \n    \n      28\n      1989\n      14.863220\n      -23.462392\n      -36.112726\n      -46.528372\n      -57.756422\n      -21.687470\n      -10.044757\n      -3041.169131\n      0.503872\n      8.401020\n    \n    \n      29\n      1990\n      -1.226009\n      -27.484542\n      -92.713339\n      -35.553433\n      -56.563056\n      -31.077022\n      -29.893352\n      -3318.220397\n      0.765335\n      9.166355\n    \n    \n      30\n      1991\n      -14.391425\n      -34.898689\n      -8.822063\n      -15.338299\n      -31.458010\n      -7.162909\n      -35.968429\n      -3467.630284\n      0.412734\n      9.579089\n    \n  \n\n\n\n\nNotice since we are back to indexing by position the right endpoint of the slice (6) is not included in the ouput.\n\n\n\n4.1.4 Selecting rows and columns simultaneously…\nSelecting rows and columns simultaneously can be done using loc (labels or conditions) or iloc (integer position).\n\n4.1.4.1 …by labels or conditions\nWhen we want to select rows and columns simultaneously by labels or conditions we can use loc selection with the syntax\ndf.loc[ row-selection , column-selection]\nspecifying both paratmers: row-selection and column-selection. These parameters can be a condition (which generates a boolean array) or a subset of labels from the index or the column names. Let’s see an examples:\n\n# select change in glacial volume in Europe per year after 2000\ndf.loc[df['year']>2000,['year','europe']]\n\n\n\n\n\n  \n    \n      \n      year\n      europe\n    \n  \n  \n    \n      40\n      2001\n      -8.419109\n    \n    \n      41\n      2002\n      -3.392361\n    \n    \n      42\n      2003\n      -3.392361\n    \n  \n\n\n\n\nLet’s break it down:\n\nwe are using the df.loc[ row-selection , column-selection] syntax\nthe row-selection parameter is the condition df['year']>1990, which is a boolean array saying which years are greater than 1990\nthe column-selection parameter is ['year','europe'] which is a list with the names of the two columns we are intersted in.\n\n\n\n4.1.4.2 … by position\nWhen we want to select rows and columns simultaneously by position we use iloc selection with the syntax:\ndf.iloc[ row-indices , column-indices]\nFor example,\n\n# select rows 3-7 (including 7) and columns 3 and 4\ndf.iloc[ 3:8, [3,4] ]\n\n\n\n\n\n  \n    \n      \n      alaska\n      asia\n    \n  \n  \n    \n      3\n      0.477800\n      -18.675385\n    \n    \n      4\n      -0.115332\n      -18.414602\n    \n    \n      5\n      0.224762\n      -14.630284\n    \n    \n      6\n      -7.174030\n      -39.013695\n    \n    \n      7\n      -0.660556\n      7.879589\n    \n  \n\n\n\n\nLet’s break it down:\n\nwe are using the df.iloc[ row-indices , column-indices] syntax\nthe row-indices parameter is the slice of integer indices 3:8. Remember the right endpoint (8) won’t be included.\nthe column-indices parameter is the list of integer indices 3 and 4. This means we are selecting the fourth and fifth column.\n\n\n\n\n4.1.5 Notes about loc and iloc\n\n\n\n\n\n\niloc vs. loc\n\n\n\nAt the beginning, the difference between iloc and loc can be confusing. Remember the i in iloc stands for integer-location, this reminds us iloc only uses integer indexing to retrieve information from the data frames in the same way as indexing for Python lists.\nIf you want to dive deeper, this is a great discussion about the difference between iloc and loc: Stackoverflow - How are iloc and loc different?\nAnd, as always, the documentation will provide you with more information: pandas.DataFrame.loc and pandas.DataFrame.iloc.\n\n\n\n\n\n\n\n\niloc for column selection? Avoid it!\n\n\n\nWe can also access columns by position using iloc - but it is best not to if possible.\nSuppose we want to access the 10th column in the data frame - then we want to select a column by position. In this case the 10th column is the annual sea level rise data and the 10th position corresponds to the index 9. We can select this column by position using the iloc selection:\n\n# select column by position using iloc\n# the syntax is iloc[row-indices, column-indices]\n# [:,9] means \"select all rows from the 10th column\"\nannual_rise_3 = df.iloc[:,9]\nannual_rise_3.head()\n\n0    0.610010\n1    0.810625\n2    0.100292\n3   -0.085596\n4   -0.128392\nName: annual_sea_level_rise, dtype: float64\n\n\nUnless you are really looking for information about the 10th column, do not access a column by position. This is bound to break in many ways:\n\nit relies on a person correctly counting the position of a column. Even with a small dataset this can be prone to error.\nit is not explicit: if we want information about sea level rise df.annual_sea_level_rise or df['annual_sea_level_rise'] are explicitely telling us we are accessing that information. df.iloc[:,9] is obscure and uninformative.\ndatastets can get updated. Maybe a new column was added before annual_sea_level_rise, this would change the position of the column, which would make any code depending on df.iloc[:,9] invalid. Accessing by label helps reproducibility!"
  },
  {
    "objectID": "lectures/lesson-3-pandas-subsetting.html#summary",
    "href": "lectures/lesson-3-pandas-subsetting.html#summary",
    "title": "4  Subsetting",
    "section": "4.2 Summary",
    "text": "4.2 Summary\n\n\n\npandas.DataFrame selection flow chart"
  },
  {
    "objectID": "lectures/lesson-3-pandas-subsetting.html#resources",
    "href": "lectures/lesson-3-pandas-subsetting.html#resources",
    "title": "4  Subsetting",
    "section": "4.3 Resources",
    "text": "4.3 Resources\nWhat is presented in this section is a comprehensive, but not an exhaustive list of methods to select data in pandas.DataFrames. There are so many ways to subset data to get the same result. Some of the content from this lesson is adapted from the following resources and I encourage you to read them to learn more!\n📖 Pandas getting started tutorials - How to I select a subset of a DataFrame\n📖 Pandas documentation - User Guide - Indexing and Selecting Data\n📖 Python for Data Analysis, 3E - Getting started with pandas"
  },
  {
    "objectID": "lectures/lesson-3-pandas-subsetting.html#acknowledgements",
    "href": "lectures/lesson-3-pandas-subsetting.html#acknowledgements",
    "title": "4  Subsetting",
    "section": "4.4 Acknowledgements",
    "text": "4.4 Acknowledgements\nThe simplified glacial_loss.csv dataset was created by Dr. Allison Horst as part of her course materials on environmental data science."
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#plot-method",
    "href": "lectures/lesson-4-basic-plotting.html#plot-method",
    "title": "5  Plotting",
    "section": "5.1 plot() method",
    "text": "5.1 plot() method\nA pandas.DataFrame has a built-in method plot() for plotting. When we call it without specifying any other parameters plot() creates one line plot for each of the columns with numeric data.\n\n# one line plot per column with numeric data - a mess\ndf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nAs we can see, this doesn’t make any sense! In particular, look at the x-axis. The default for plot is to use the values of the index as the x-axis values. Let’s see some examples about how to improve this situation."
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#line-plots",
    "href": "lectures/lesson-4-basic-plotting.html#line-plots",
    "title": "5  Plotting",
    "section": "5.2 Line plots",
    "text": "5.2 Line plots\nWe can make a line plot of one column against another by using the following syntax:\ndf.plot(x='x_values_column', y='y_values_column')\nFor example,\n\n# change in glacial volume per year in Europe\ndf.plot(x='year', y='europe')\n\n<AxesSubplot:xlabel='year'>\n\n\n\n\n\nWe can do some basic customization specifying other arguments of the plot function. Some basic ones are:\n\ntitle: Title to use for the plot.\nxlabel: Name to use for the xlabel on x-axis\nylabel: Name to use for the ylabel on y-axis\ncolor: change the color of our plot\n\nIn action:\n\ndf.plot(x='year', \n        y='europe',\n        title='Change in glacial volume per year in Europe',\n        xlabel='Year',\n        ylabel='​Change in glacial volume (km3​)',\n        color='green'\n        )\n\n<AxesSubplot:title={'center':'Change in glacial volume per year in Europe'}, xlabel='Year', ylabel='\\u200bChange in glacial volume (km3\\u200b)'>\n\n\n\n\n\nYou can see all the optional arguments for the plot() function in the documentation.\n\n5.2.1 Multiple line plots\nLet’s say we want to graph the change in glacial volume in the Arctic and Alaska. We can do it by updating these arguments:\n\ny : a list of column names that will be plotted against x\ncolor: specify the color of each column’s line with a dictionary {'col_1' : 'color_1', 'col_2':'color_2}\n\n\ndf.plot(x='year', \n        y=['arctic', 'alaska'],\n        title = 'Change in glacial volume per year in Alaska and the Arctic',\n        xlabel='Year',\n        ylabel='​Change in glacial volume (km3​)',        \n        color = {'arctic':'#F48FB1',\n                 'alaska': '#AB47BC'\n                 }\n        )\n\n<AxesSubplot:title={'center':'Change in glacial volume per year in Alaska and the Arctic'}, xlabel='Year', ylabel='\\u200bChange in glacial volume (km3\\u200b)'>\n\n\n\n\n\nNotice that for specifying the colors we used a HEX code, this gives us more control over how our graph looks.\nWe can also create separate plots for each column by setting the subset to True.\n\ndf.plot(x='year', \n        y=['arctic', 'alaska'],\n        title = 'Change in glacial volume per year in Alaska and the Arctic',\n        xlabel='Year',\n        ylabel='​Change in glacial volume (km3​)',        \n        color = {'arctic':'#F48FB1',\n                 'alaska': '#AB47BC'\n                 },\n        subplots=True\n        )\n\narray([<AxesSubplot:xlabel='Year', ylabel='\\u200bChange in glacial volume (km3\\u200b)'>,\n       <AxesSubplot:xlabel='Year', ylabel='\\u200bChange in glacial volume (km3\\u200b)'>],\n      dtype=object)\n\n\n\n\n\n\n\n5.2.2 Check-in\n\nPlot a graph of the annual sea level rise with respect to the years.\nWhat information is the columns variable retrieving from the data frame? Describe in a sentence what is being plotted.\n\ncolumns = df.loc[:,'europe':'antarctica'].columns\ndf.plot(x='year', \n        y=columns, \n        subplots=True)\nWe will move on to another dataset for the rest of the lecture. The great…"
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#palmer-penguins-dataset",
    "href": "lectures/lesson-4-basic-plotting.html#palmer-penguins-dataset",
    "title": "5  Plotting",
    "section": "5.3 Palmer penguins dataset",
    "text": "5.3 Palmer penguins dataset\nFor the next plots we will use the Palmer Penguins dataset (Horst et al., 2020). This contains size measurements for three penguin species in the Palmer Archipelago, Antarctica.\n\n\n\nThe Palmer Archipelago penguins. Artwork by @allison_horst.\n\n\nThe data is usually accessed through the palmerpenguins R data package. Today we will access the csv directly into Python using the URL: https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv\nThe Palmer penguins dataset has the following columns:\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_lenght_mm\nbody_mass_g\nsex\nyear\n\nLet’s start by reading in the data.\n\n# read in data\npenguins = pd.read_csv('https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv')\n\n# look at dataframe's head\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      2007\n    \n  \n\n\n\n\n\n# check column data types and NA values\npenguins.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB\n\n\n\n# simple statistics about numeric columns\npenguins.describe()\n\n\n\n\n\n  \n    \n      \n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      year\n    \n  \n  \n    \n      count\n      342.000000\n      342.000000\n      342.000000\n      342.000000\n      344.000000\n    \n    \n      mean\n      43.921930\n      17.151170\n      200.915205\n      4201.754386\n      2008.029070\n    \n    \n      std\n      5.459584\n      1.974793\n      14.061714\n      801.954536\n      0.818356\n    \n    \n      min\n      32.100000\n      13.100000\n      172.000000\n      2700.000000\n      2007.000000\n    \n    \n      25%\n      39.225000\n      15.600000\n      190.000000\n      3550.000000\n      2007.000000\n    \n    \n      50%\n      44.450000\n      17.300000\n      197.000000\n      4050.000000\n      2008.000000\n    \n    \n      75%\n      48.500000\n      18.700000\n      213.000000\n      4750.000000\n      2009.000000\n    \n    \n      max\n      59.600000\n      21.500000\n      231.000000\n      6300.000000\n      2009.000000\n    \n  \n\n\n\n\nWe can also subset the dataframe to get information about a particular column or groups of columns.\n\n# get count unique values in categorical columns and year\npenguins[['species', 'island', 'sex', 'year']].nunique()\n\nspecies    3\nisland     3\nsex        2\nyear       3\ndtype: int64\n\n\n\n# get unique values in species column\nprint(penguins.species.unique())\n\n['Adelie' 'Gentoo' 'Chinstrap']\n\n\n\n# species unique value counts \nprint(penguins.species.value_counts())\n\nAdelie       152\nGentoo       124\nChinstrap     68\nName: species, dtype: int64"
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#kind-argument-in-plot",
    "href": "lectures/lesson-4-basic-plotting.html#kind-argument-in-plot",
    "title": "5  Plotting",
    "section": "5.4 kind argument in plot()",
    "text": "5.4 kind argument in plot()\nWe talked about how the plot() function creates by default a line plot. The parameter that controls this behaviour is plot()’s kind parameter. By changing the value of kind we can create different kinds of plots. Let’s look at the documentation to see what these values are:\n\n\n\npandas.DataFrame.plot documentation extract - accessed Oct 10,2023\n\n\nNotice the default value of kind is 'line'.\nLet’s change the kind parameter to create some different plots."
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#scatter-plots",
    "href": "lectures/lesson-4-basic-plotting.html#scatter-plots",
    "title": "5  Plotting",
    "section": "5.5 Scatter plots",
    "text": "5.5 Scatter plots\nSuppose we want to visualy compare the flipper length against the body mass, we can do this with a scatterplot.\nExample:\n\npenguins.plot(kind='scatter',\n        x='flipper_length_mm', \n        y='body_mass_g')\n\n<AxesSubplot:xlabel='flipper_length_mm', ylabel='body_mass_g'>\n\n\n\n\n\nWe can update some other arguments to customize the graph:\n\npenguins.plot(kind='scatter',\n        x='flipper_length_mm', \n        y='body_mass_g',\n        title='Flipper length and body mass for Palmer penguins',\n        xlabel='Flipper length (mm)',\n        ylabel='Body mass (g)',\n        color='#ff3b01',\n        alpha=0.4  # controls transparency\n        )\n\n<AxesSubplot:title={'center':'Flipper length and body mass for Palmer penguins'}, xlabel='Flipper length (mm)', ylabel='Body mass (g)'>"
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#bar-plots",
    "href": "lectures/lesson-4-basic-plotting.html#bar-plots",
    "title": "5  Plotting",
    "section": "5.6 Bar plots",
    "text": "5.6 Bar plots\nWe can create bar plots of our data setting kind='bar' in the plot() method.\nFor example, let’s say we want to get data about the 10 penguins with lowest body mass. We can first select this data using the nsmallest() method for series:\n\nsmallest = penguins.body_mass_g.nsmallest(10).sort_values()\nsmallest\n\n314    2700.0\n58     2850.0\n64     2850.0\n54     2900.0\n98     2900.0\n116    2900.0\n298    2900.0\n104    2925.0\n47     2975.0\n44     3000.0\nName: body_mass_g, dtype: float64\n\n\nWe can then plot this data as a bar plot\n\nsmallest.plot(kind='bar')\n\n<AxesSubplot:>\n\n\n\n\n\nIf we wanted to look at other data for these smallest penguins we can use the index of the smallest pandas.Series to select those rows in the original penguins data frame using loc:\n\npenguins.loc[smallest.index]\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      314\n      Chinstrap\n      Dream\n      46.9\n      16.6\n      192.0\n      2700.0\n      female\n      2008\n    \n    \n      58\n      Adelie\n      Biscoe\n      36.5\n      16.6\n      181.0\n      2850.0\n      female\n      2008\n    \n    \n      64\n      Adelie\n      Biscoe\n      36.4\n      17.1\n      184.0\n      2850.0\n      female\n      2008\n    \n    \n      54\n      Adelie\n      Biscoe\n      34.5\n      18.1\n      187.0\n      2900.0\n      female\n      2008\n    \n    \n      98\n      Adelie\n      Dream\n      33.1\n      16.1\n      178.0\n      2900.0\n      female\n      2008\n    \n    \n      116\n      Adelie\n      Torgersen\n      38.6\n      17.0\n      188.0\n      2900.0\n      female\n      2009\n    \n    \n      298\n      Chinstrap\n      Dream\n      43.2\n      16.6\n      187.0\n      2900.0\n      female\n      2007\n    \n    \n      104\n      Adelie\n      Biscoe\n      37.9\n      18.6\n      193.0\n      2925.0\n      female\n      2009\n    \n    \n      47\n      Adelie\n      Dream\n      37.5\n      18.9\n      179.0\n      2975.0\n      NaN\n      2007\n    \n    \n      44\n      Adelie\n      Dream\n      37.0\n      16.9\n      185.0\n      3000.0\n      female\n      2007"
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#histograms",
    "href": "lectures/lesson-4-basic-plotting.html#histograms",
    "title": "5  Plotting",
    "section": "5.7 Histograms",
    "text": "5.7 Histograms\nWe can create a histogram of our data setting kind='hist' in plot().\n\n# using plot without subsetting data - a mess again\npenguins.plot(kind='hist')\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\nTo gain actual information, let’s subset the data before plotting it. For example, suppose we want to look at the distribution of flipper length. We could do it in this way:\n\n# distribution of flipper length measurements\n# first select data, then plot\npenguins.flipper_length_mm.plot(kind='hist',\n                                title='Penguin flipper lengths',\n                                xlabel='Flipper length (mm)',\n                                grid=True)\n\n<AxesSubplot:title={'center':'Penguin flipper lengths'}, ylabel='Frequency'>\n\n\n\n\n\n\n5.7.1 Check-in\n\nSelect the bill_length_mm and bill_depth_mm columns in the penguins dataframe and then update the kind parameter to box to make boxplots of the bill length and bill depth. \nSelect both rows and columns to create a histogram of the flipper length of gentoo penguins."
  },
  {
    "objectID": "lectures/lesson-4-basic-plotting.html#references",
    "href": "lectures/lesson-4-basic-plotting.html#references",
    "title": "5  Plotting",
    "section": "5.8 References",
    "text": "5.8 References\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi:10.5281/zenodo.3960218."
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#data-palmer-penguins",
    "href": "lectures/lesson-5-updating-dataframes.html#data-palmer-penguins",
    "title": "6  Updating data frames",
    "section": "6.1 Data: Palmer penguins",
    "text": "6.1 Data: Palmer penguins\nWe will use the Palmer penguins dataset (Horst et al., 2020). This time we will import it via the seaborn package since it is included as one of seaborn’s example datasets.\n\n\n\n\n\n\nseaborn\n\n\n\nseaborn is a library to make nice looking graphs in Python. We’ve been using plot() to make basic exploratory graphs. seaborn is the next step if you’re interested in making publication-level statistical graphics.\n\n\n\n# standard libraries\nimport pandas as pd\nimport numpy as np\n\n# importing seaborn with its standard abbreviation\nimport seaborn as sns\n\n# will use the random library to create some random numbers\nimport random\n\npenguins = sns.load_dataset(\"penguins\")\n\n# look at dataframe's head\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#adding-a-single-column",
    "href": "lectures/lesson-5-updating-dataframes.html#adding-a-single-column",
    "title": "6  Updating data frames",
    "section": "6.2 Adding a single column",
    "text": "6.2 Adding a single column\nThe general syntax to adding a new column to a pd.DataFrame is\ndf['new_col_name'] = new_column_values\nwhere new_column values could be:\n\na pd.Series or numpy.array of the same length as the data frame\na single scalar\n\nExample\nSuppose we want to create a new column where the body mass is in kilograms instead of grams, so we need to divide the body_mass_g by 1000.\n\n# add a new column body_mass_kg \n# sane syntax as adding a new key to a dictionary\npenguins['body_mass_kg'] = penguins.body_mass_g/1000\n\n# confirm the new column is in the data frame\nprint('body_mass_kg' in penguins.columns)\n\n# take a look at the new column\npenguins.head()\n\nTrue\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      body_mass_kg\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n      3.75\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n      3.80\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n      3.25\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female\n      3.45\n    \n  \n\n\n\n\n\n\n\n\n\n\npd.DataFrames and dictionaries\n\n\n\nRemember we talked about how a pandas.DataFrame could be seen as a dictionary of its columns. The most basic way of adding a new column to a data frame is the same way of adding a new key-value pair to a dictionary:\n# adding a new key-value pair to a dictionary\n# same as adding a new column in pandas\ndict[new_key] = new_value\n\n\nNotice the new column got added at the end. If we want to create a new column and insert it at a particular position we use the data frame method insert():\ndf.insert(loc = integer_index,  # location of new column\n          column = 'new_col_name', \n          value = new_col_values)\nExample\nSuppose each penguin observation gets a unique identifier as a three digit number. We want to add this column, at the beginning of the data frame. We can use insert to do this:\n\n# create random 3-digit codes\n# random.sample used for random sampling wo replacement\ncodes = random.sample(range(100,1000), len(penguins))\n\n# insert codes at the front of data frame = index 0\npenguins.insert(loc=0, \n                column = 'code',\n                value = codes)\n        \npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      body_mass_kg\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n      3.75\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n      3.80\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n      3.25\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female\n      3.45"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#adding-multiple-columns",
    "href": "lectures/lesson-5-updating-dataframes.html#adding-multiple-columns",
    "title": "6  Updating data frames",
    "section": "6.3 Adding multiple columns",
    "text": "6.3 Adding multiple columns\nWe can assign multiple columns in the same call by using the data frame’s assign() method. The general syntax is:\ndf = df.assign( new_col1_name = new_col1_values, \n           new_col2_name = new_col2_values)\nNotice the new column names are not strings, we declare them as if we were creating variables.\nExample\nSuppose we want to add these new columns:\n\nflipper length converted from mm to cm, and\na code representing the observer.\n\nWe can add these columns to penguins using assign():\n\n# create new columns in the data frame\n# random.choices used for random sampling with replacement\n# need to reassign output of assign() to update the data frame\npenguins = penguins.assign( flipper_length_cm = penguins.flipper_length_mm /10, \n                            observer =   random.choices(['A','B','C'], k=len(penguins)))\n# look at result\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n      3.75\n      18.1\n      C\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n      3.80\n      18.6\n      C\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n      3.25\n      19.5\n      B\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female\n      3.45\n      19.3\n      A"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#removing-columns",
    "href": "lectures/lesson-5-updating-dataframes.html#removing-columns",
    "title": "6  Updating data frames",
    "section": "6.4 Removing columns",
    "text": "6.4 Removing columns\nWe can remove columns using the drop() method for data frames, the syntax is:\ndf = df.drop(columns = col_names)\nwhere col_names can be a single column name (str) or a list of column names. The drop() method does not modify the data frame in place, so you need to reassign the output.\nExample\nNow that we updated the units for flipper length and body mass, it makes sense to remove the previous columns to avoid duplicate information. We can do this using drop():\n\n# use a list of column names\n# reassign output of drop() to dataframe to update it\npenguins = penguins.drop(columns=['flipper_length_mm','body_mass_g'])\n\n# check columns\nprint(penguins.columns)\n\nIndex(['code', 'species', 'island', 'bill_length_mm', 'bill_depth_mm', 'sex',\n       'body_mass_kg', 'flipper_length_cm', 'observer'],\n      dtype='object')"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#updating-values",
    "href": "lectures/lesson-5-updating-dataframes.html#updating-values",
    "title": "6  Updating data frames",
    "section": "6.5 Updating values",
    "text": "6.5 Updating values\nSometimes we want to update certain value within our data frame. We’ll review some methods and best practices to do that in this section.\n\n6.5.1 A single value\nWe can access a single value in a pd.DataFrame using the locators\n\nat[] to select by labels, or\niat[] to select by position.\n\nThe syntax for at[] is:\ndf.at[single_index_value, 'column_name']\nThink of at[] as the equivalent to loc[] when trying to access a single value.\nExample\nLet’s say we want to know what was the bill length of the penguin in the fourth row. We can access that using at[]:\n\n# access value at row with index=3 and column='bill_length_mm'\npenguins.at[3,'bill_length_mm']\n\nnan\n\n\nWe get this is an NA. Maybe we want to update it to 38.3 mm. We can do this with at[] too:\n\n# update NA to 38.3\npenguins.at[3,'bill_length_mm'] = 38.3\n\n# check it was updated\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      Male\n      3.75\n      18.1\n      C\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      Female\n      3.80\n      18.6\n      C\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      Female\n      3.25\n      19.5\n      B\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      38.3\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      Female\n      3.45\n      19.3\n      A\n    \n  \n\n\n\n\nIf we want to access or update a single value by position we use the iat[] locator.\n\n\n6.5.2 Multiple values in a column\nWhat if we want to update multiple values in a column? We’ll cover two cases: with a condition on the column values and by selecting a few values to update.\n\n6.5.2.1 By condition\nOften we want to create a new column where the new values depend on conditions on another column’s values.\nExample\nSuppose we want to classify all penguins with body mass less than 3kg as small, penguins with body mass greater or equal than 3kg but less than 5kg as medium, and those with body mass greater or equal than 5kg as large. One way to add this information in a new column using numpy.select():\n\n# create a list with the conditions\nconditions = [penguins.body_mass_kg < 3, \n              (3 <= penguins.body_mass_kg) & (penguins.body_mass_kg < 5),\n              5 <= penguins.body_mass_kg]\n\n# create a list with the choices\nchoices = [\"small\",\n           \"medium\",\n           \"large\"]\n\n# add the selections using np.select\n# default = value for anything that falls outside conditions\npenguins['size'] = np.select(conditions, choices, default=np.nan)\n\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n      size\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      Male\n      3.75\n      18.1\n      C\n      medium\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      Female\n      3.80\n      18.6\n      C\n      medium\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      Female\n      3.25\n      19.5\n      B\n      medium\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      38.3\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n      nan\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      Female\n      3.45\n      19.3\n      A\n      medium\n    \n  \n\n\n\n\n\n\n\n\n6.5.2.2 By selecting values\nWhen we only want to update some values in a column we can do this by selecting this data using loc (if selecting by labels) or iloc (if selecting by position). The general sytanx for updating data with loc is:\n# modifies data in place\ndf.loc[row_selection, col_name] = new_values\nwhere\n\nrow_selection is the rows we want to update,\ncol_name is a single column name, and\nnew_values is the new value or values we want. If using multiple values, then new_values must be of the same length as the number of rows selected,\n\nExample\nSuppose we want to update the “Male” value in the sex column to “M”.\n\n# select rows with sex=male and update the values in the sex column\npenguins.loc[penguins.sex=='Male', 'sex'] = 'M'\n\n# check changes\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n      size\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      M\n      3.75\n      18.1\n      C\n      medium\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      Female\n      3.80\n      18.6\n      C\n      medium\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      Female\n      3.25\n      19.5\n      B\n      medium\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      38.3\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n      nan\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      Female\n      3.45\n      19.3\n      A\n      medium"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#settingwithcopywarning",
    "href": "lectures/lesson-5-updating-dataframes.html#settingwithcopywarning",
    "title": "6  Updating data frames",
    "section": "6.6 SettingWithCopyWarning",
    "text": "6.6 SettingWithCopyWarning\nSuppose we want to similarly update the “Female” values in the sex column to “F”. This is an example of another way we might try to do it, but it won’t update our data frame:\n\n# select rows and columns with two selection brackets [][] (chained indexing), instead of loc[]\n# then trying to update the values\npenguins[penguins.sex=='Female']['sex'] = 'F'\n\n/opt/python/3.7.13/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nWhen we select the data we want to update using chained indexing ([][]) instead of loc[] we get a SettingWithCopyWarning. Wit this warning, pandas is trying to alert us to a potential bug. In this case that we did not update our data frame:\n\n# no values were updated\npenguins.sex.unique()\n\narray(['M', 'Female', nan], dtype=object)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe SettingWithCopyWarning is letting us know our code is ambiguous and we need to double check the code is doing what we intended. In the words of the pandas documentation:\n\nassigning to the product of chained indexing has inherently unpredictable results.\n\n\n\n\nTo see what is going on we need to understand that some pandas operations return a view to your data, while others return a copy of your data.\n\nViews are actual subsets of the original data, when we update them, we are modifying the original data frame.\nCopies are unique objects, independent of our original data frames. When we update a copy we are not modifying the original data frame.\n\nDepending on what we are trying to do we might want to modify the original data frame or we might want to modify a copy.\nCheck-in\nUpdate the “Female” values in the penguins data frame to “F”. Don’t use chained indexing\n\n# no chained indexing in assignment = no warning\npenguins.loc[penguins.sex=='Female','sex'] = 'F'\n\n# notice the values were updated now\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n      size\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      M\n      3.75\n      18.1\n      C\n      medium\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      F\n      3.80\n      18.6\n      C\n      medium\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      F\n      3.25\n      19.5\n      B\n      medium\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      38.3\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n      nan\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      F\n      3.45\n      19.3\n      A\n      medium\n    \n  \n\n\n\n\n\n6.6.0.1 Another SettingWithCopyWarning\nAnother common situation where SettingWithCopyWarning comes up is when we try to update a subset of a data frame we have already stored in a new variable.\nExample\nSuppose we only want to use data from Biscoe island and, after doing some analyses, we want to add a new column to it:\n\n# select penguins from Biscoe island\nbiscoe = penguins[penguins.island=='Biscoe']\n\n# 50 lines of code here\n\n# add a column, we get a warning\nbiscoe['sample_col'] = 100\n\n/opt/python/3.7.13/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nIt might not seem like it, but we have a chained assignment here too. We essentialy wrote:\npenguins[penguins.island=='Biscoe']['sample_col'] = 100\nTo fix this we can take control of the copy-view situation and explicitely ask for a copy of the dataset when subsetting the data. Use the copy() method to do this:\n\n# make sure you get a new data frame with penguins from Biscoe island\nbiscoe = penguins[penguins.island=='Biscoe'].copy()\n\n# add a column, no warning\nbiscoe['sample_col'] = 100\n\nNow we are sure we did not modify our initial data, but rather the biscoe data frame:\n\n# new column is there\nbiscoe.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n      size\n      sample_col\n    \n  \n  \n    \n      20\n      666\n      Adelie\n      Biscoe\n      37.8\n      18.3\n      F\n      3.40\n      17.4\n      A\n      medium\n      100\n    \n    \n      21\n      348\n      Adelie\n      Biscoe\n      37.7\n      18.7\n      M\n      3.60\n      18.0\n      C\n      medium\n      100\n    \n    \n      22\n      753\n      Adelie\n      Biscoe\n      35.9\n      19.2\n      F\n      3.80\n      18.9\n      C\n      medium\n      100\n    \n    \n      23\n      513\n      Adelie\n      Biscoe\n      38.2\n      18.1\n      M\n      3.95\n      18.5\n      C\n      medium\n      100\n    \n    \n      24\n      893\n      Adelie\n      Biscoe\n      38.8\n      17.2\n      M\n      3.80\n      18.0\n      A\n      medium\n      100\n    \n  \n\n\n\n\n\n# and we are sure our original df wasn't modified\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      code\n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      sex\n      body_mass_kg\n      flipper_length_cm\n      observer\n      size\n    \n  \n  \n    \n      0\n      330\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      M\n      3.75\n      18.1\n      C\n      medium\n    \n    \n      1\n      128\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      F\n      3.80\n      18.6\n      C\n      medium\n    \n    \n      2\n      530\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      F\n      3.25\n      19.5\n      B\n      medium\n    \n    \n      3\n      796\n      Adelie\n      Torgersen\n      38.3\n      NaN\n      NaN\n      NaN\n      NaN\n      A\n      nan\n    \n    \n      4\n      692\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      F\n      3.45\n      19.3\n      A\n      medium\n    \n  \n\n\n\n\nThe SettingWithCopyWarning can often be tricky, there are also false positives and false negatives. Avoiding chained indexing and making a copy of your data frame subset whenever possible will save you from the usual pitfalls.\nTo learn more about the SettingWithCopyWarning, these are some articles that go into more depth:\n📖 pandas Documentation - Returning a view versus a copy\n📖 Real Python- SettingWithCopyWarning in pandas: Views vs Copies\n📖 Dataquest - SettingwithCopyWarning: How to Fix This Warning in Pandas"
  },
  {
    "objectID": "lectures/lesson-5-updating-dataframes.html#references",
    "href": "lectures/lesson-5-updating-dataframes.html#references",
    "title": "6  Updating data frames",
    "section": "6.7 References",
    "text": "6.7 References\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi:10.5281/zenodo.3960218."
  },
  {
    "objectID": "lectures/lesson-6-groupby.html#summary-statistics",
    "href": "lectures/lesson-6-groupby.html#summary-statistics",
    "title": "7  groupby()",
    "section": "7.1 Summary statistics",
    "text": "7.1 Summary statistics\nIn pandas it is easy to get summary statistics for each column in a dataframe by using methods such as\n\nsum(): sum values in each column,\ncount(): count non-NA values in each column,\nmin() and max(): get the minimum and maximum value in each column,\n\nmean() and median(): get the mean and median value in each column,\nstd() and var(): get the standard deviation and variance in each column.\n\nExample\n\n# get the number of non-NA values in each column \npenguins.count()\n\nspecies              344\nisland               344\nbill_length_mm       342\nbill_depth_mm        342\nflipper_length_mm    342\nbody_mass_g          342\nsex                  333\nyear                 344\ndtype: int64\n\n\n\n# get the minimum value in each column\npenguins.select_dtypes('number').min()\n\nbill_length_mm         32.1\nbill_depth_mm          13.1\nflipper_length_mm     172.0\nbody_mass_g          2700.0\nyear                 2007.0\ndtype: float64"
  },
  {
    "objectID": "lectures/lesson-6-groupby.html#grouping",
    "href": "lectures/lesson-6-groupby.html#grouping",
    "title": "7  groupby()",
    "section": "7.2 Grouping",
    "text": "7.2 Grouping\nOur penguins data is naturally split into different groups: there are three different species, two sexes and three islands. Often, we want to calculate a certain statistic for each group. For example, suppose we want to calculate the average flipper length per species. How would we do this “by hand”?\n\nWe start with our data and notice there are multiple species in the species column.\nWe split our original table to group all observations from the same species together.\nWe calculate the average flipper length for each of the groups we formed.\nThen we combine the values for average flipper length per species into a single table.\n\nThis is known as the Split-Apply-Combine strategy. This strategy follows the three steps we explained above:\n\nSplit: Split the data into logical groups (e.g. species, sex, island, etc.)\nApply: Calculate some summary statistic on each group (e.g. average flipper length by species, number of individuals per island, body mass by sex, etc.)\nCombine: Combine the statistic calculated on each group back together.\n\n\n\n\nSplit-apply-combine to calculate mean flipper length\n\n\nIn Python we can use the groupby() method to split (i.e. group) the data into different categories. The general syntax for groupby() is\ndf.groupby(columns_to_group_by)\nwhere most often we will have columns_to_group_by = a single column name (string) or a list of column names, the unique values of the column (or columns) will be used as the groups of the data frame\nExample\nFirst, if we don’t use the groupby() method, we obtain the average for the whole flipper length column:\n\npenguins.flipper_length_mm.mean()\n\n200.91520467836258\n\n\nTo get the mean flipper length by species we first group our dataset by the species column’s data:\n\n# average flipper length per species\npenguins.groupby(\"species\").flipper_length_mm.mean()\n\nspecies\nAdelie       189.953642\nChinstrap    195.823529\nGentoo       217.186992\nName: flipper_length_mm, dtype: float64\n\n\nThere’s a lot going on there, let’s break it down (remember the . can be read as “and then…”)\n\nstart with the penguins data frame, and then…\nuse groupby() to group the data frame by species values, and then…\nselect the 'flipper_length_mm' column, and then…\ncalculate the mean()\n\nWe can store our new data frame as avg_flipper and then graph it as a bar plot:\n\n# average flipper length per species\navg_flipper = penguins.groupby(\"species\").flipper_length_mm.mean()\n\n# sort in descending order\n# remember to reassign to actually update data\navg_flipper = avg_flipper.sort_values(ascending=False)\n\n# plot as a bar plot\navg_flipper.plot(kind='bar',\n                 title='Average flipper length by species',\n                 ylabel='Flipper length (mm)',\n                 rot=360  # rotation for ticks \n                 ) \n\n<AxesSubplot:title={'center':'Average flipper length by species'}, xlabel='species', ylabel='Flipper length (mm)'>\n\n\n\n\n\nWe can also group by combinations of columns.\nExample\nSuppose we want to know what was the number of penguins in every island on different years. We can use the count() method to count the number of non-NA values in each column like this:\n\npenguins.count()\n\nspecies              344\nisland               344\nbill_length_mm       342\nbill_depth_mm        342\nflipper_length_mm    342\nbody_mass_g          342\nsex                  333\nyear                 344\ndtype: int64\n\n\nWhen we group by island and year we get the count of non-NA values for each column divided for each combination of island and year:\n\npenguins.groupby(['island','year']).count()\n\n\n\n\n\n  \n    \n      \n      \n      species\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n    \n      island\n      year\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      2007\n      44\n      44\n      44\n      44\n      44\n      43\n    \n    \n      2008\n      64\n      64\n      64\n      64\n      64\n      63\n    \n    \n      2009\n      60\n      59\n      59\n      59\n      59\n      57\n    \n    \n      Dream\n      2007\n      46\n      46\n      46\n      46\n      46\n      45\n    \n    \n      2008\n      34\n      34\n      34\n      34\n      34\n      34\n    \n    \n      2009\n      44\n      44\n      44\n      44\n      44\n      44\n    \n    \n      Torgersen\n      2007\n      20\n      19\n      19\n      19\n      19\n      15\n    \n    \n      2008\n      16\n      16\n      16\n      16\n      16\n      16\n    \n    \n      2009\n      16\n      16\n      16\n      16\n      16\n      16\n    \n  \n\n\n\n\nLet’s say we want to plot the surveyed population per year and island. We could then use a “one liner” to do it in this way:\n\npenguins.groupby(['island','year']).count().species.sort_values().plot(kind='barh')\n\n<AxesSubplot:ylabel='island,year'>\n\n\n\n\n\n\n\n7.2.1 Check-in\n\nUse groupby and the max() method for data frames to calculate the maximum value of penguin’s body mass by year and species.\n\n\n\nUse (1) to display the highest body masses per year and species as a bar plot in descending order."
  },
  {
    "objectID": "lectures/lesson-6-groupby.html#acknowledgmenets",
    "href": "lectures/lesson-6-groupby.html#acknowledgmenets",
    "title": "7  groupby()",
    "section": "7.3 Acknowledgmenets",
    "text": "7.3 Acknowledgmenets\nThis section is based on this NCEAS Learning Hub R lesson.\nHalina Do-Linh, Carmen Galaz García, Matthew B. Jones, Camila Vargas Poulsen. 2023. Open Science Synthesis training Week 1. NCEAS Learning Hub & Delta Stewardship Council."
  },
  {
    "objectID": "discussion-sections/ds2-hares.html",
    "href": "discussion-sections/ds2-hares.html",
    "title": "8  * Lab: Data cleaning",
    "section": "",
    "text": "9 References\nKielland, K., F.S. Chapin, R.W. Ruess, and Bonanza Creek LTER. 2017. Snowshoe hare physical data in Bonanza Creek Experimental Forest: 1999-Present ver 22. Environmental Data Initiative. https://doi.org/10.6073/pasta/03dce4856d79b91557d8e6ce2cbcdc14 (Accessed 2023-10-18)."
  },
  {
    "objectID": "discussion-sections/ds2-hares.html#archive-exploration",
    "href": "discussion-sections/ds2-hares.html#archive-exploration",
    "title": "8  * Lab: Data cleaning",
    "section": "8.1 Archive exploration",
    "text": "8.1 Archive exploration\nTake some time to look through the dataset’s description in EDI and click around. Discuss the following questions with your team:\n\nWhat is this data about?\nDuring what time frame were the observations in the dataset collected?\nDoes the dataset contain sensitive data?\nIs there a publication associated with this dataset?\n\nIn your notebook: use a markdown cell to add a brief description of the dataset, including a citation, date of access, and a link to the archive."
  },
  {
    "objectID": "discussion-sections/ds2-hares.html#adding-an-image",
    "href": "discussion-sections/ds2-hares.html#adding-an-image",
    "title": "8  * Lab: Data cleaning",
    "section": "8.2 Adding an image",
    "text": "8.2 Adding an image\nFollow these steps to add an image of a hare using a URL:\n\nGo to https://commons.wikimedia.org/wiki/File:SNOWSHOE_HARE_%28Lepus_americanus%29_%285-28-2015%29_quoddy_head,washington_co,maine-01%2818988734889%29.jpg\nGet the URL of the hare image. To do this:\n\n\nhover over the image –> right click –> “Copy Image Address”.\n\n\nAt the end of the markdown cell with the dataset description, use markdown sytanx to add the image from its URL: ![image description](URL-goes-here)\nDo you need to add an attribution in the image description? Check the license at the bottom of wikimedia page.\n\n\ncommit and push changes"
  },
  {
    "objectID": "discussion-sections/ds2-hares.html#data-loading",
    "href": "discussion-sections/ds2-hares.html#data-loading",
    "title": "8  * Lab: Data cleaning",
    "section": "8.3 Data loading",
    "text": "8.3 Data loading\nBack in your notebook, import the pandas package in a code cell and import the 55_Hare_Data_2012.txt from its URL using the pandas.read_csv(). Store it in a variable named hares. Take a look at the head of the dataframe.\n\ncommit and push changes\n\n\nCHECK IN WITH YOUR TEAM\n\n\nMAKE SURE YOU’VE ALL SUCCESSFULLY ACCESSED THE DATA BEFORE CONTINUING"
  },
  {
    "objectID": "discussion-sections/ds2-hares.html#metadata-exploration",
    "href": "discussion-sections/ds2-hares.html#metadata-exploration",
    "title": "8  * Lab: Data cleaning",
    "section": "8.4 Metadata exploration",
    "text": "8.4 Metadata exploration\nBack in the EDI repository, click on View Full Metadata to access more information.\nGo to the “Detailed Metadata” section and click on “Data Entities”. Take a minute to look at the descriptions for the dataset’s columns."
  },
  {
    "objectID": "discussion-sections/ds2-hares.html#detecting-messy-values",
    "href": "discussion-sections/ds2-hares.html#detecting-messy-values",
    "title": "8  * Lab: Data cleaning",
    "section": "8.5 Detecting messy values",
    "text": "8.5 Detecting messy values\n\nGet the number of teams each unique non-NA value in the sex column appears by running hares.sex.value_counts().\nCheck the documentation of value_counts(). What is the purpose of the dropna=False parameter? Do step 2 again, this time adding the dropna=False parameter to value_counts().\nDiscuss with your team the output of the unique value counts. Notice anything odd?\nYou likely noticed there seem to be some repeated values, for example m appears twice. Use the unique() method on the sex column to see the unique non-NA values in this column. Discuss with your team what was the cause of the seemingly repeated values.\nIn the metadata section of the EDI repository, find which are the allowed values for the sex column. Discuss with your team whether these values correspond to the values present in the dataset.\n\n\ncommit and push changes"
  },
  {
    "objectID": "discussion-sections/ds2-hares.html#clean-values",
    "href": "discussion-sections/ds2-hares.html#clean-values",
    "title": "8  * Lab: Data cleaning",
    "section": "8.6 Clean values",
    "text": "8.6 Clean values\n\nUse np.select like we did on Monday to create a new column called sex_simple\n\n\n‘F’, ‘f’, and ‘f’ get assigned to ‘female’,\n‘M’, ‘m’, and ‘m’ get assigned to ‘male’, and\nanything else gets assigned np.nan\n\nHINTS:\n\nYou need to create a list with two conditions and a list with two choices.\nTo write the condition think about what does (hares.sex=='F') | (hares.sex=='f') mean? Do you need to add anything else?\n\n\nCheck the counts of unique values (including NAs) in the new sex_simple column.\n\n\ncommit and push changes"
  },
  {
    "objectID": "discussion-sections/ds2-hares.html#calculate-mean-weight",
    "href": "discussion-sections/ds2-hares.html#calculate-mean-weight",
    "title": "8  * Lab: Data cleaning",
    "section": "8.7 Calculate mean weight",
    "text": "8.7 Calculate mean weight\n\nUse groupby() to calculate the mean weight by sex (use the new column).\n\n\ncommit and push changes"
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#timestamps",
    "href": "lectures/lesson-7-time-series.html#timestamps",
    "title": "9  Basic time series",
    "section": "9.1 Timestamps",
    "text": "9.1 Timestamps\npandas represents an instant in time using the pandas.Timestamp class. For example:\n\nimport pandas as pd\n\n# create a timestamp\npd.Timestamp(year=2020, month=10, day=18, hour=12, minute=30, second=15)\n\nTimestamp('2020-10-18 12:30:15')\n\n\nWhen we store multiple Timestamps in a pd.Series (for example when we have a column of dates) the data type of the column is set to datetime64[ns]:\n\n# Notice the dtype of the column is datetime64\npd.Series([pd.Timestamp(2020,10,18), \n           pd.Timestamp(2020,10,17),\n           pd.Timestamp(2020,10,16)])\n\n0   2020-10-18\n1   2020-10-17\n2   2020-10-16\ndtype: datetime64[ns]\n\n\nThis is enough to get us started!"
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#data-precipitation-in-boulder-co",
    "href": "lectures/lesson-7-time-series.html#data-precipitation-in-boulder-co",
    "title": "9  Basic time series",
    "section": "9.2 Data: Precipitation in Boulder, CO",
    "text": "9.2 Data: Precipitation in Boulder, CO\nTo exemplify some of the basic time series functionalities we’ll be using data about hourly precipitation in the county of Boulder, Colorado from 2000 to 2014. In September 2013, an unusual weather pattern led to some of the most intense precipitation ever recorded in this region, causing devastating floods throughout the Colorado Front Range. Our goal is to visualize precipitation data in 2013 and identify this unusual weather event.\nThis data was obtained via the National Oceanic and Atmosperic Administration (NOAA) Climate Data Online service. This dataset is a csv and can be acceses at this link. You can view the full documentation here. The following is a summary of the column descriptions:\n\nSTATION: identification number indentifying the station.\nSTATION_NAME: optional field, name identifying the station location.\nDATE: this is the year of the record (4 digits), followed by month (2 digits), followed by day of the month (2 digits), followed by a space and ending with a time of observation that is a two digit indication of the local time hour, followed by a colon (:) followed by a two digit indication of the minute which for this dataset will always be 00. Note: The subsequent data value will be for the hour ending at the time specified here. Hour 00:00 will be listed as the first hour of each date, however since this data is by definition an accumulation of the previous 60 minutes, it actually occurred on the previous day.\nHPCP: The amount of precipitation recorded at the station for the hour ending at the time specified for DATE above given in inches. The values 999.99 means the data value is missing. Hours with no precipitation are not shown."
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#data-preparation",
    "href": "lectures/lesson-7-time-series.html#data-preparation",
    "title": "9  Basic time series",
    "section": "9.3 Data preparation",
    "text": "9.3 Data preparation\nLet’s start by reading in the data and taking a look at it.\n\n# read in data \nurl = 'https://raw.githubusercontent.com/carmengg/eds-220-book/main/data/boulder_colorado_2013_hourly_precipitation.csv'\nprecip = pd.read_csv(url)\n\n# check df's head\nprecip.head()\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      DATE\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n  \n  \n    \n      0\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000101 00:00\n      999.99\n      ]\n      \n    \n    \n      1\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000101 01:00\n      0.00\n      g\n      \n    \n    \n      2\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000102 20:00\n      0.00\n      \n      q\n    \n    \n      3\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000103 01:00\n      0.00\n      \n      q\n    \n    \n      4\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000103 05:00\n      0.00\n      \n      q\n    \n  \n\n\n\n\nLet’s make a first attempt at plotting the precipitation:\n\nprecip.HPCP.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nThere’s a few things going on with this graph:\n\nThere are many jumps close to 1000. This is clearly not right and these are outliers. Looking at the column description we can see 999.99 indicates the HPCP data is missing.\nThe x-axis values are given by the index of the dataframe and not relative to time.\nWe are only intersted in the precipitation data from 2013, this graph is trying to plot all our data.\n\nLet’s fix each one of these issues separately."
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#indicate-missing-data",
    "href": "lectures/lesson-7-time-series.html#indicate-missing-data",
    "title": "9  Basic time series",
    "section": "9.4 Indicate missing data",
    "text": "9.4 Indicate missing data\nThe metadata states the missing values are indicated by the number 999.99. We can use this information to reload the dataframe indicating 999.99 is the missing value. To do this, we add the na_values parameter to the pd.read_csv() function to indicitate additional values that should be recognized as NA:\n\n# read in csv indicating NA values\nprecip = pd.read_csv(url, na_values=[999.99])\n\n# check updated df\nprecip.head()\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      DATE\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n  \n  \n    \n      0\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000101 00:00\n      NaN\n      ]\n      \n    \n    \n      1\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000101 01:00\n      0.0\n      g\n      \n    \n    \n      2\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000102 20:00\n      0.0\n      \n      q\n    \n    \n      3\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000103 01:00\n      0.0\n      \n      q\n    \n    \n      4\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      20000103 05:00\n      0.0\n      \n      q\n    \n  \n\n\n\n\n\n\n\n\n\n\nDefault NA values\n\n\n\nCheck the na_values parameter in the pd.read_csv() documentation to learn more about which values are identified as NA by default.\n\n\nWe can try making our plot again:\n\nprecip.HPCP.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nThis looks much better and we can already see there is something going on close to the end of the time series. We still need to set the index as the time."
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#strings-into-dates",
    "href": "lectures/lesson-7-time-series.html#strings-into-dates",
    "title": "9  Basic time series",
    "section": "9.5 Strings into dates",
    "text": "9.5 Strings into dates\nNotice that the DATE column in our dataframe is not of type datetime. We can check this using the dtypes attribute for dataframes:\n\n# check the dtypes of all columns\nprecip.dtypes\n\nSTATION              object\nSTATION_NAME         object\nDATE                 object\nHPCP                float64\nMeasurement Flag     object\nQuality Flag         object\ndtype: object\n\n\n\nThe object dtype means that (most likely) all values in that column are strings. We can convert strings to datetime objects using the to_datetime() pandas function:\n\npd.to_datetime() input: a pd.Series with strings that can be converted to dates\npd.to_datetime() output: a pd.Series with the strings converted to datetime objects\n\nExample\n\n# covert precip.DATE column to timedate objects\npd.to_datetime(precip.DATE)\n\n0      2000-01-01 00:00:00\n1      2000-01-01 01:00:00\n2      2000-01-02 20:00:00\n3      2000-01-03 01:00:00\n4      2000-01-03 05:00:00\n               ...        \n9001   2013-12-22 01:00:00\n9002   2013-12-23 00:00:00\n9003   2013-12-23 02:00:00\n9004   2013-12-29 01:00:00\n9005   2013-12-31 00:00:00\nName: DATE, Length: 9006, dtype: datetime64[ns]\n\n\nWe can overwrite the DATE column with this output:\n\n# convert DATE column from string to timestamps\nprecip.DATE = pd.to_datetime(precip.DATE)\n\n# check column data type is updated\nprint(precip.dtypes)\n\n# check new values\nprecip.head()\n\nSTATION                     object\nSTATION_NAME                object\nDATE                datetime64[ns]\nHPCP                       float64\nMeasurement Flag            object\nQuality Flag                object\ndtype: object\n\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      DATE\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n  \n  \n    \n      0\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      2000-01-01 00:00:00\n      NaN\n      ]\n      \n    \n    \n      1\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      2000-01-01 01:00:00\n      0.0\n      g\n      \n    \n    \n      2\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      2000-01-02 20:00:00\n      0.0\n      \n      q\n    \n    \n      3\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      2000-01-03 01:00:00\n      0.0\n      \n      q\n    \n    \n      4\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      2000-01-03 05:00:00\n      0.0\n      \n      q\n    \n  \n\n\n\n\nAnd we can make another try at plotting our precipitation data:\n\nprecip.plot(x='DATE', y='HPCP')\n\n<AxesSubplot:xlabel='DATE'>\n\n\n\n\n\nNotice the x-axis is now neatly organized into years."
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#timestamp-index",
    "href": "lectures/lesson-7-time-series.html#timestamp-index",
    "title": "9  Basic time series",
    "section": "9.6 Timestamp index",
    "text": "9.6 Timestamp index\nAt its core, we have time series data if our data frame is indexed by moments in time. Using our DATE column as the index will allows us to do many operations with respect to time, including subsetting and resampling.\nUpdating the index of a dataframe is simple, we use the set_index() method. It’s general syntax is\ndf = df.set_index(new_index)\nwhere new_index is:\n\nthe name of the column (string) we want to set as index, or\nif our new index is not a column in the dataframe, an array or pd.Series of the same length as our data frame (we need one index per row!).\n\nNotice that set_index does not modify the data frame in place.\nExample\nIn our precipitation data this is:\n\n# set DATE coumn as index\nprecip = precip.set_index('DATE')\n\n# check new index\nprecip.head()\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n    \n      DATE\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      NaN\n      ]\n      \n    \n    \n      2000-01-01 01:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.0\n      g\n      \n    \n    \n      2000-01-02 20:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.0\n      \n      q\n    \n    \n      2000-01-03 01:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.0\n      \n      q\n    \n    \n      2000-01-03 05:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.0\n      \n      q\n    \n  \n\n\n\n\nSince we know the default behaviour of plot() is to use the index as the x-axis and make a line plot for each numeric column, we can simplify our plot making like this:\n\nprecip.plot()\n\n<AxesSubplot:xlabel='DATE'>"
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#subsetting-by-date",
    "href": "lectures/lesson-7-time-series.html#subsetting-by-date",
    "title": "9  Basic time series",
    "section": "9.7 Subsetting by date",
    "text": "9.7 Subsetting by date\npandas has great functionality to subset a dataframe when using a time index. These are some examples:\n\n# select data from a given month and year\n# use loc['year-month']\nprecip.loc['2013-09']\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n    \n      DATE\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-09-01 00:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      NaN\n      ]\n      \n    \n    \n      2013-09-01 01:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      NaN\n      [\n      \n    \n    \n      2013-09-01 00:00:00\n      COOP:050183\n      ALLENSPARK 2 SE CO US\n      NaN\n      ]\n      \n    \n    \n      2013-09-01 01:00:00\n      COOP:050183\n      ALLENSPARK 2 SE CO US\n      NaN\n      [\n      \n    \n    \n      2013-09-01 00:00:00\n      COOP:055121\n      LONGMONT 6 NW CO US\n      NaN\n      }\n      \n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2013-09-23 02:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.2\n      \n      \n    \n    \n      2013-09-27 10:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.1\n      \n      \n    \n    \n      2013-09-27 15:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.1\n      \n      \n    \n    \n      2013-09-27 17:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.1\n      \n      \n    \n    \n      2013-09-27 18:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.1\n      \n      \n    \n  \n\n128 rows × 5 columns\n\n\n\n\n# select data from a given year\n# use loc['year']\nprecip.loc['2013']\n\n\n\n\n\n  \n    \n      \n      STATION\n      STATION_NAME\n      HPCP\n      Measurement Flag\n      Quality Flag\n    \n    \n      DATE\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2013-01-01 01:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.0\n      g\n      \n    \n    \n      2013-01-10 02:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      NaN\n      [\n      \n    \n    \n      2013-01-13 00:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      NaN\n      ]\n      \n    \n    \n      2013-01-26 20:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.1\n      \n      \n    \n    \n      2013-01-28 23:00:00\n      COOP:055881\n      NEDERLAND 5 NNW CO US\n      0.1\n      \n      \n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2013-12-22 01:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      NaN\n      [\n      \n    \n    \n      2013-12-23 00:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      NaN\n      ]\n      \n    \n    \n      2013-12-23 02:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      0.1\n      \n      \n    \n    \n      2013-12-29 01:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      NaN\n      [\n      \n    \n    \n      2013-12-31 00:00:00\n      COOP:050843\n      BOULDER 2 CO US\n      NaN\n      ]\n      \n    \n  \n\n662 rows × 5 columns\n\n\n\nWe can use this selection to plot data as usual. Notice we have a lot of gaps due to missing data.\n\nprecip.loc['2013'].plot()\n\n<AxesSubplot:xlabel='DATE'>"
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#resample",
    "href": "lectures/lesson-7-time-series.html#resample",
    "title": "9  Basic time series",
    "section": "9.8 Resample",
    "text": "9.8 Resample\nResampling a time series means converting a time series from one frequency to another. For example, monthly to yearly (downsampling) or weekly to daily (upsampling). We can resample with the resample() method. The simplest use is to call\ndf.resample(new_frequency).aggregator_function()\nwhere: - new_frequency is a string representing the new frequence to resample the data, for example 'D' for day, w for week, M for month, Y for year, and - aggregator_function() is the function we will use to aggregate the data into the new frequence. For example, max(), min(), sum() or average().\nresample() works similarly to groupby() in the sense that you need to specify a way to aggregate the data to get any output.\nExample\nOur 2013 precipitation data has hourly frequency and we want to resample it to daily frequency.\n\n# resample 2013 hourly data to daily frequency\n# nothing happens\nprecip.loc['2013'].resample('D')\n\n<pandas.core.resample.DatetimeIndexResampler object at 0x7f2b11ceb510>\n\n\nTo get an output we need to add an aggregator function that indicates how we want to summarize the data that falls on each day. In this case we want the total precipitation on a day, so we will aggreagte using sum():\n\n# total daily precipitation in 2013\ndaily_precip_2013 = precip.loc['2013'].resample('D').sum()\n\ndaily_precip_2013\n\n\n\n\n\n  \n    \n      \n      HPCP\n    \n    \n      DATE\n      \n    \n  \n  \n    \n      2013-01-01\n      0.0\n    \n    \n      2013-01-02\n      0.0\n    \n    \n      2013-01-03\n      0.0\n    \n    \n      2013-01-04\n      0.0\n    \n    \n      2013-01-05\n      0.0\n    \n    \n      ...\n      ...\n    \n    \n      2013-12-27\n      0.0\n    \n    \n      2013-12-28\n      0.1\n    \n    \n      2013-12-29\n      0.0\n    \n    \n      2013-12-30\n      0.1\n    \n    \n      2013-12-31\n      0.0\n    \n  \n\n365 rows × 1 columns\n\n\n\nNotice the index now is every day in 2013. We should rename that column since it is not longer hourly precipitation:\n\n# rename column\ndaily_precip_2013 = daily_precip_2013.rename(columns={'HPCP':'daily_precipitation'})\n\nFinally, we can plot our data:\n\ndaily_precip_2013.plot(ylabel='daily precipitation (in)', \n                       xlabel='date',\n                       title='Precipitation in Boulder, CO during 2013',\n                       legend=False)\n\n<AxesSubplot:title={'center':'Precipitation in Boulder, CO during 2013'}, xlabel='date', ylabel='daily precipitation (in)'>"
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#more-info",
    "href": "lectures/lesson-7-time-series.html#more-info",
    "title": "9  Basic time series",
    "section": "9.9 More info",
    "text": "9.9 More info\nAs usual, there’s so much more to learn about time series data. These documents will allow you to dive deeper:\n📖 pandas getting started tutorials - How to handle time series data with ease\n📖 Time Series Chapter, Python for Data Analysis, Wes McKinney\n📖 pandas User Guide - Time series/date functionality"
  },
  {
    "objectID": "lectures/lesson-7-time-series.html#acknowledgements",
    "href": "lectures/lesson-7-time-series.html#acknowledgements",
    "title": "9  Basic time series",
    "section": "9.10 Acknowledgements",
    "text": "9.10 Acknowledgements\nThis lesson was adapted from Dr. Sam Stevenson’s lecture on Data quality control and outliers: 1D time series and Earth Lab’s Lesson 1. Work With Datetime Format in Python - Time Series Data."
  },
  {
    "objectID": "lectures/lesson-8-vector-data.html#points-lines-and-polygons",
    "href": "lectures/lesson-8-vector-data.html#points-lines-and-polygons",
    "title": "10  Data formats",
    "section": "10.1 Points, lines, and polygons",
    "text": "10.1 Points, lines, and polygons\nVector data represents specific features on the Earth’s surface. There are three types of vector data:\n\nPoints: each point has a single x,y location. Examples of data that can be represented as point vector data are sampling locations or animal sightings.\nLines: a line is composed of at least two points that are connected. Roads and streams are commonly depicted as line vector data.\nPolygons: polygons are sets of three or more vertices that are connected and form a closed region. Political boundaries (outlines of countries, states, cities, etc) are examples of polygon vector data.\n\nEach item in the vector data is usually referred to as a feature. So each point would be a feature, each polygon is a feature, etc.\n\n\n\nImage Source: National Ecological Observatory Network (NEON)\n\n\nIn addition to the geospatial information stored, vector data can include attributes that describe each feature. For example, a vector dataset where each feature is a polygon representing the boundary of a state could have as attributes the population and are of the state.\n\n\n\nImage Source: National Ecological Observatory Network (NEON)"
  },
  {
    "objectID": "lectures/lesson-8-vector-data.html#shapefiles",
    "href": "lectures/lesson-8-vector-data.html#shapefiles",
    "title": "10  Data formats",
    "section": "10.2 Shapefiles",
    "text": "10.2 Shapefiles\nOne of the most popular formats to store vector data is the shapefile data format. The shapefile format is developed and maintained by the Environmental Systems Research Institute (Esri).\nSo far we’ve been working with data that comes stored in a single file, like a csv or txt file for tabular data. A shapefile is actually a collection of files that interact together to create a single data file. All the files that make up a shapefile need to have the same name (different extensions) and be in the same directory. For our shapefiles to work we need at least these three files:\n\n.shp: shape format, this file has the geometries for all features.\n.shx: shape index format, this file indexes the features\n.dbf: attribute format, this file stores the attributes for features as a table\n\nSometimes shapefiles will have additional files, including:\n\n.prj: a file containing information about the projection and coordinate reference system\n.sbn and .sbx: files that contain a spatial index of the features\n.shp.xml: geospatial metadata in XML format.\n\nCheck the Wikipedia page about shapefiles to see a more extensive list of files associated to shapefiles.\n\n\n\n\n\n\nFile management\n\n\n\nRemember: when working with a shapefile all the associated files must have the same name (different extensions) and be located in the same directory.\n\n\n\n\n\n\n\n\nDifferent vector types = separate shapefiles\n\n\n\nEach shapefile can only hold one type of vector data. So only points, only lines, or only polygons can be inside a single shapefile."
  },
  {
    "objectID": "lectures/lesson-8-vector-data.html#geojson",
    "href": "lectures/lesson-8-vector-data.html#geojson",
    "title": "10  Data formats",
    "section": "10.3 GeoJSON",
    "text": "10.3 GeoJSON\nGeoJSON, which stands for Geographic JavaScript Object Notation, is an open format for encoding vector data (points, lines, polygons, and multipolygons) and their attributes. It is a popular format for web mapping applications. The GeoJSON format uses a single file, with extension .json or .geojson.\nWhile shapefiles can be in any coordinate reference system, the GeoJSON specification requires GeoJSON files to use the World Geodetic System 1984 (WGS84) and all points being expressed as longitude and latitude units of decimal degrees.\nData in a GeoJSON is stored as attribute-value pairs (think dictionaries!) and lists. The following are examples of how points, lines, and polygons are represented as GeoJSON features:\n\n\n\nSource: Wikipedia\n\n\nThe followng is an example of a full GeoJSON file. Notice multiple types of geometries can be mixed within the same file:\n# Source: Wikipedia\n{\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"Point\",\n        \"coordinates\": [102.0, 0.5]\n      },\n      \"properties\": {\n        \"prop0\": \"value0\"\n      }\n    },\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"LineString\",\n        \"coordinates\": [\n          [102.0, 0.0],\n          [103.0, 1.0],\n          [104.0, 0.0],\n          [105.0, 1.0]\n        ]\n      },\n      \"properties\": {\n        \"prop0\": \"value0\",\n        \"prop1\": 0.0\n      }\n    },\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [100.0, 0.0],\n            [101.0, 0.0],\n            [101.0, 1.0],\n            [100.0, 1.0],\n            [100.0, 0.0]\n          ]\n        ]\n      },\n      \"properties\": {\n        \"prop0\": \"value0\",\n        \"prop1\": { \"this\": \"that\" }\n      }\n    }\n  ]\n}\nThe website https://geojson.io/ is a nice tool to easily create GeoJSON files."
  },
  {
    "objectID": "lectures/lesson-8-vector-data.html#references",
    "href": "lectures/lesson-8-vector-data.html#references",
    "title": "10  Data formats",
    "section": "10.4 References",
    "text": "10.4 References\nData Carpentires - Introduction to Geospatial Concepts"
  },
  {
    "objectID": "lectures/lesson-9-geopandas-intro.html#data",
    "href": "lectures/lesson-9-geopandas-intro.html#data",
    "title": "11  geopandas",
    "section": "11.1 Data",
    "text": "11.1 Data\nIn this lesson we will use simplified point data about wild pigs (Sus scrofa) sightings in California, USA from the Global Biodiversity Information Facility.\nWe can read in a shapefile with geopandas by using the gpd.read_file() function.\n\npigs = gpd.read_file('data/gbif_sus_scroga_california/gbif_sus_scroga_california.shp')\npigs.head()\n\n\n\n\n\n  \n    \n      \n      gbifID\n      species\n      state\n      individual\n      day\n      month\n      year\n      inst\n      collection\n      catalogNum\n      identified\n      geometry\n    \n  \n  \n    \n      0\n      899953814\n      Sus scrofa\n      California\n      NaN\n      22.0\n      3.0\n      2014.0\n      iNaturalist\n      Observations\n      581956\n      edwardrooks\n      POINT (-121.53812 37.08846)\n    \n    \n      1\n      899951348\n      Sus scrofa\n      California\n      NaN\n      9.0\n      6.0\n      2007.0\n      iNaturalist\n      Observations\n      576047\n      Bruce Freeman\n      POINT (-120.54942 35.47354)\n    \n    \n      2\n      896560733\n      Sus scrofa\n      California\n      NaN\n      20.0\n      12.0\n      1937.0\n      MVZ\n      Hild\n      MVZ:Hild:195\n      Museum of Vertebrate Zoology, University of Ca...\n      POINT (-122.27063 37.87610)\n    \n    \n      3\n      896559958\n      Sus scrofa\n      California\n      NaN\n      1.0\n      4.0\n      1969.0\n      MVZ\n      Hild\n      MVZ:Hild:1213\n      Museum of Vertebrate Zoology, University of Ca...\n      POINT (-121.82297 38.44543)\n    \n    \n      4\n      896559722\n      Sus scrofa\n      California\n      NaN\n      1.0\n      1.0\n      1961.0\n      MVZ\n      Hild\n      MVZ:Hild:1004\n      Museum of Vertebrate Zoology, University of Ca...\n      POINT (-121.74559 38.54882)\n    \n  \n\n\n\n\n\n\n\n\n\n\nOne shapefile = multiple files\n\n\n\nAlthough the parameter for gpd.read_file() is only the .shp file, remember that we need to have at least the .shx and .dbf files in the same directory as the .shp to read in the data."
  },
  {
    "objectID": "lectures/lesson-9-geopandas-intro.html#geoseries-and-geodataframe",
    "href": "lectures/lesson-9-geopandas-intro.html#geoseries-and-geodataframe",
    "title": "11  geopandas",
    "section": "11.2 GeoSeries and GeoDataFrame",
    "text": "11.2 GeoSeries and GeoDataFrame\nThe core data structure in GeoPandas is the geopandas.GeoDataFrame. We can think of it as a pandas.DataFrame with a dedicated geometry column that can perform spatial operations.\nThe geometry column in a gpd.GeoDataFrame holds the geometry (point, polygon, etc) of each spatial feature. Columns in the gpd.GeoDataFrame with attributes about the features are pandas.Series like in a regular pd.DataFrame.\n\n\n\nImage source: GeoPandas documentation\n\n\nExample\nFirst of all, notice that the leftmost column of pigs is a column named geometry whose values indicate points.\n\npigs.head(3)\n\n\n\n\n\n  \n    \n      \n      gbifID\n      species\n      state\n      individual\n      day\n      month\n      year\n      inst\n      collection\n      catalogNum\n      identified\n      geometry\n    \n  \n  \n    \n      0\n      899953814\n      Sus scrofa\n      California\n      NaN\n      22.0\n      3.0\n      2014.0\n      iNaturalist\n      Observations\n      581956\n      edwardrooks\n      POINT (-121.53812 37.08846)\n    \n    \n      1\n      899951348\n      Sus scrofa\n      California\n      NaN\n      9.0\n      6.0\n      2007.0\n      iNaturalist\n      Observations\n      576047\n      Bruce Freeman\n      POINT (-120.54942 35.47354)\n    \n    \n      2\n      896560733\n      Sus scrofa\n      California\n      NaN\n      20.0\n      12.0\n      1937.0\n      MVZ\n      Hild\n      MVZ:Hild:195\n      Museum of Vertebrate Zoology, University of Ca...\n      POINT (-122.27063 37.87610)\n    \n  \n\n\n\n\nAs usual, we can check the type of our objects using the type Python function:\n\n# type of the pigs dataframe \nprint(type(pigs))\n\n# type of the geometry column\nprint(type(pigs.geometry))\n\n# type of the gbifID column\nprint(type(pigs.gbifID))\n\n<class 'geopandas.geodataframe.GeoDataFrame'>\n<class 'geopandas.geoseries.GeoSeries'>\n<class 'pandas.core.series.Series'>\n\n\nThe new data type of the geometry column is also reflected when we look at the data types of the columns in the data frame:\n\npigs.dtypes\n\ngbifID           int64\nspecies         object\nstate           object\nindividual     float64\nday            float64\nmonth          float64\nyear           float64\ninst            object\ncollection      object\ncatalogNum      object\nidentified      object\ngeometry      geometry\ndtype: object\n\n\nWe can also check the type of each element in the geometry column using the geom_type attribute of a gpd.GeoDataFrame:\n\npigs.geom_type\n\n0       Point\n1       Point\n2       Point\n3       Point\n4       Point\n        ...  \n1041    Point\n1042    Point\n1043    Point\n1044    Point\n1045    Point\nLength: 1046, dtype: object"
  },
  {
    "objectID": "lectures/lesson-9-geopandas-intro.html#geometric-information",
    "href": "lectures/lesson-9-geopandas-intro.html#geometric-information",
    "title": "11  geopandas",
    "section": "11.3 Geometric information",
    "text": "11.3 Geometric information\nTwo other important attributes of a gpd.GeoDataFrame are its coordinate reference system (CRS) and its extent.\nWe can think of the coordinate reference system (CRS) as the instructions to locate each feature in our dataframe on the surface of the Earth. We access the CRS of a gpd.GeoDataFrame using the crs attribute:\n\n# access the CRS of the GeoDataFrame\npigs.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nThe extent of the geo-dataframe is the bounding box covering all the features in our geo-dataframe. This is formed by finding the points that are furthest west, east, south and north.\n\n\n\nSpatial extent of different vector data. Image Source: National Ecological Observatory Network (NEON)\n\n\nWe access the extent of a gpd.GeoDataFrame using the total_bounds attribute:\n\npigs.total_bounds\n\narray([-124.29448 ,   32.593433, -115.4356  ,   40.934296])"
  },
  {
    "objectID": "lectures/lesson-9-geopandas-intro.html#data-wrangling",
    "href": "lectures/lesson-9-geopandas-intro.html#data-wrangling",
    "title": "11  geopandas",
    "section": "11.4 Data wrangling",
    "text": "11.4 Data wrangling\nGeoPandas is conveniently built on top of pandas, so we may use everything we have learned about data selection, wrangling, and modification for a pd.DataFrame.\nExample\nSuppose we only want to use recent data for wild pig observations. A quick check shows that this dataframe has data since 1818:\n\n# use sort_index() method to order the index\npigs.year.value_counts().sort_index()\n\n1818.0     31\n1910.0      1\n1925.0      1\n1927.0      4\n1929.0      3\n         ... \n2019.0    101\n2020.0    159\n2021.0    164\n2022.0    185\n2023.0     98\nName: year, Length: 61, dtype: int64\n\n\nWe can use our usual data selection to get data from 2020 onwards:\n\n# selet data from 2020 onwards\npigs_recent = pigs[pigs.year>=2020]\n\n# print length of original dataframe\nprint(len(pigs))\n\n# check length of new dataframe\nlen(pigs_recent)\n\n1046\n\n\n606"
  },
  {
    "objectID": "lectures/lesson-9-geopandas-intro.html#plotting",
    "href": "lectures/lesson-9-geopandas-intro.html#plotting",
    "title": "11  geopandas",
    "section": "11.5 Plotting",
    "text": "11.5 Plotting\n\n11.5.1 plot()\nSimilarly to a pd.DataFrame, a gpd.GeoDataFrame has a plot() method that we can call directly to create a quick view of our data. The geospatial information of the gpd.GeoDataFrame will be used to create the axes of the plot.\nExample\nThis is a quick look at our recent pigs data:\n\npigs_recent.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n11.5.2 matplotlib’s fig and ax\nGoing forward, we will often want to make more complex visualizations where we add different layers to a graph and customize it. To do this we will use the matplotlib Python library for creating visualizations. We can interact with matplotlib via its pyplot interface, which we imported at the top of the notebook as\n# import matplotlib with standard abbreviation\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "lectures/lesson-9-geopandas-intro.html#best-practices-for-importing-packages",
    "href": "lectures/lesson-9-geopandas-intro.html#best-practices-for-importing-packages",
    "title": "11  geopandas",
    "section": "11.6 Best practices for importing packages",
    "text": "11.6 Best practices for importing packages\nWe always import libraries at the top of our notebook in a single cell. If halfway through our analysis we realize we need to import a new library, we still need to go back to that first cell and import it there!"
  },
  {
    "objectID": "lectures/lesson-9-geopandas-intro.html#simple-map",
    "href": "lectures/lesson-9-geopandas-intro.html#simple-map",
    "title": "11  geopandas",
    "section": "11.7 Simple map",
    "text": "11.7 Simple map\nWe can add multiple layers of shapefiles to our maps by plotting multiple geo-dataframes on the same axis.\nExample\nWe will import a shapefile with the outline of the state of CA. The original shapefile was obtained through the California’s government Open Data Portal, and the one used here was updated to match the CRS of the points (we’ll talk more about matching CRSs later).\n\n# import CA boundary\nca_boundary = gpd.read_file('data/ca-boundary/ca-boundary.shp')\nca_boundary\n\n\n\n\n\n  \n    \n      \n      REGION\n      DIVISION\n      STATEFP\n      STATENS\n      GEOID\n      STUSPS\n      NAME\n      LSAD\n      MTFCC\n      FUNCSTAT\n      ALAND\n      AWATER\n      INTPTLAT\n      INTPTLON\n      geometry\n    \n  \n  \n    \n      0\n      4\n      9\n      06\n      01779778\n      06\n      CA\n      California\n      00\n      G4000\n      A\n      403501101370\n      20466718403\n      +37.1551773\n      -119.5434183\n      MULTIPOLYGON (((-119.63473 33.26545, -119.6363...\n    \n  \n\n\n\n\nWe can see this geo-dataframe has a single feature whose geometry is MultiPolygon. Plotting the California outline with the point data is easy: just add it as a layer in your axis.\n\n# initialize an empty figure \nfig, ax = plt.subplots()\n\n# add layers\n# add CA boundary\nca_boundary.plot(ax = ax,\n                color = 'none',\n                edgecolor = '#362312')\n\n# add pig point data\npigs_recent.plot(ax = ax,\n                alpha = .5,\n                color = '#FF5768',\n                edgecolor = '#FFBF65')\n\n# customization\nax.set_title('Reported \"Sus scrofa\" sightings in CA (2020-2023)')\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\n\n# display figure\nplt.show()"
  },
  {
    "objectID": "lectures/lesson-9-geopandas-intro.html#references",
    "href": "lectures/lesson-9-geopandas-intro.html#references",
    "title": "11  geopandas",
    "section": "11.8 References",
    "text": "11.8 References\nGBIG data: GBIF.org (23 October 2023) GBIF Occurrence Download https://doi.org/10.15468/dl.qavhwp\nGeopandas Documentation - Introduction to GeoPandas\nMatplotlib Documentation - Basic Usage\nNEON tutorials - Intro to Vector Data in R"
  },
  {
    "objectID": "lectures/lesson-10-merge-data.html#types-of-joins",
    "href": "lectures/lesson-10-merge-data.html#types-of-joins",
    "title": "12  Merging data",
    "section": "12.1 Types of Joins",
    "text": "12.1 Types of Joins\nFrequently, analysis of data will require merging separate dataframes. There are multiple ways to merge observations. When conceptualizing merges, we think of two tables, one on the left and one on the right.\n\n\n\nImage source: Data Modeling Essentials, NCEAS Learning Hub\n\n\n\n12.1.1 Inner Join\nAn inner join is when you merge the subset of rows that have matches in both the left table and the right table.\n\n\n\nImage source: Data Modeling Essentials, NCEAS Learning Hub\n\n\n\n\n12.1.2 Left Join\nA left join takes all of the rows from the left table, and merges on the data from matching rows in the right table. Keys that don’t match from the left table are still provided with a missing value (na) from the right table.\n\n\n\nImage source: Data Modeling Essentials, NCEAS Learning Hub\n\n\n\n\n12.1.3 Right Join\nA right join is the same as a left join, except that all of the rows from the right table are included with matching data from the left, or a missing value. Notice that left and right joins can ultimately be the same depending on the positions of the tables.\n\n\n\nImage source: Data Modeling Essentials, NCEAS Learning Hub\n\n\n\n\n12.1.4 Full Join\nFinally, a full outer join (or just full join) includes all data from all rows in both tables, and includes missing values wherever necessary.\n\n\n\nImage source: Data Modeling Essentials, NCEAS Learning Hub\n\n\nSometimes people represent joins as Venn diagrams, showing which parts of the left and right tables are included in the results for each join. This representation is useful, however, they miss part of the story related to where the missing value comes from in each result.\n\n\n\nImage source: R for Data Science, Wickham & Grolemund."
  },
  {
    "objectID": "lectures/lesson-10-merge-data.html#goal",
    "href": "lectures/lesson-10-merge-data.html#goal",
    "title": "12  Merging data",
    "section": "12.2 Goal",
    "text": "12.2 Goal\nOur goal in this lesson will be to join two datasets, one with demographic information and another with country outlines, to create the following choropleth map showing the number of Arctic communities by country and their location in Scandinavia:"
  },
  {
    "objectID": "lectures/lesson-10-merge-data.html#data",
    "href": "lectures/lesson-10-merge-data.html#data",
    "title": "12  Merging data",
    "section": "12.3 Data",
    "text": "12.3 Data\nWe will use two datasets in this lesson. The first dataset is Natural Earth’s medium scale cultural boundaries data (1:50m). We can obtain this dataset by downloading the shapefile. Natural Earth is a public domain dataset with ready-to-use data for creating maps.\nThe second dataset we will use is a list of Arctic communities and their location (Brook, 2023) which can be accessed through the DataONE repository. This is a GeoJSON file with the following attributes:\n\nname: name of Arctic community,\npopulation: population of Arctic community, as of 2022\ncountry: country that the Arctic community falls within (see dataset metadata for the codes)\ngeoname-id: numeric codes that uniquely identify all administrative/legal and statistical geographic areas for which the Census Bureau tabulates data"
  },
  {
    "objectID": "lectures/lesson-10-merge-data.html#data-preparation",
    "href": "lectures/lesson-10-merge-data.html#data-preparation",
    "title": "12  Merging data",
    "section": "12.4 Data preparation",
    "text": "12.4 Data preparation\nWe start our analysis by importing the necessary libraries:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport geopandas as gpd\n\n\nThe Natural Earth dataset has many columns, so we need to update the pandas display settings to show all columns:\n\n# display all column when looking at dataframes\npd.set_option(\"display.max.columns\", None)\n\n\n12.4.1 Countries\nNow we move on to preparing the polygons for the Scandinavian countries. To import the Natural Earth countries polygons we use the geopandas.read_file() function again:\n\n# import countries polygons\ncountries = gpd.read_file('data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp')\ncountries.head()\n\n\n\n\n\n  \n    \n      \n      featurecla\n      scalerank\n      LABELRANK\n      SOVEREIGNT\n      SOV_A3\n      ADM0_DIF\n      LEVEL\n      TYPE\n      TLC\n      ADMIN\n      ADM0_A3\n      GEOU_DIF\n      GEOUNIT\n      GU_A3\n      SU_DIF\n      SUBUNIT\n      SU_A3\n      BRK_DIFF\n      NAME\n      NAME_LONG\n      BRK_A3\n      BRK_NAME\n      BRK_GROUP\n      ABBREV\n      POSTAL\n      FORMAL_EN\n      FORMAL_FR\n      NAME_CIAWF\n      NOTE_ADM0\n      NOTE_BRK\n      NAME_SORT\n      NAME_ALT\n      MAPCOLOR7\n      MAPCOLOR8\n      MAPCOLOR9\n      MAPCOLOR13\n      POP_EST\n      POP_RANK\n      POP_YEAR\n      GDP_MD\n      GDP_YEAR\n      ECONOMY\n      INCOME_GRP\n      FIPS_10\n      ISO_A2\n      ISO_A2_EH\n      ISO_A3\n      ISO_A3_EH\n      ISO_N3\n      ISO_N3_EH\n      UN_A3\n      WB_A2\n      WB_A3\n      WOE_ID\n      WOE_ID_EH\n      WOE_NOTE\n      ADM0_ISO\n      ADM0_DIFF\n      ADM0_TLC\n      ADM0_A3_US\n      ADM0_A3_FR\n      ADM0_A3_RU\n      ADM0_A3_ES\n      ADM0_A3_CN\n      ADM0_A3_TW\n      ADM0_A3_IN\n      ADM0_A3_NP\n      ADM0_A3_PK\n      ADM0_A3_DE\n      ADM0_A3_GB\n      ADM0_A3_BR\n      ADM0_A3_IL\n      ADM0_A3_PS\n      ADM0_A3_SA\n      ADM0_A3_EG\n      ADM0_A3_MA\n      ADM0_A3_PT\n      ADM0_A3_AR\n      ADM0_A3_JP\n      ADM0_A3_KO\n      ADM0_A3_VN\n      ADM0_A3_TR\n      ADM0_A3_ID\n      ADM0_A3_PL\n      ADM0_A3_GR\n      ADM0_A3_IT\n      ADM0_A3_NL\n      ADM0_A3_SE\n      ADM0_A3_BD\n      ADM0_A3_UA\n      ADM0_A3_UN\n      ADM0_A3_WB\n      CONTINENT\n      REGION_UN\n      SUBREGION\n      REGION_WB\n      NAME_LEN\n      LONG_LEN\n      ABBREV_LEN\n      TINY\n      HOMEPART\n      MIN_ZOOM\n      MIN_LABEL\n      MAX_LABEL\n      LABEL_X\n      LABEL_Y\n      NE_ID\n      WIKIDATAID\n      NAME_AR\n      NAME_BN\n      NAME_DE\n      NAME_EN\n      NAME_ES\n      NAME_FA\n      NAME_FR\n      NAME_EL\n      NAME_HE\n      NAME_HI\n      NAME_HU\n      NAME_ID\n      NAME_IT\n      NAME_JA\n      NAME_KO\n      NAME_NL\n      NAME_PL\n      NAME_PT\n      NAME_RU\n      NAME_SV\n      NAME_TR\n      NAME_UK\n      NAME_UR\n      NAME_VI\n      NAME_ZH\n      NAME_ZHT\n      FCLASS_ISO\n      TLC_DIFF\n      FCLASS_TLC\n      FCLASS_US\n      FCLASS_FR\n      FCLASS_RU\n      FCLASS_ES\n      FCLASS_CN\n      FCLASS_TW\n      FCLASS_IN\n      FCLASS_NP\n      FCLASS_PK\n      FCLASS_DE\n      FCLASS_GB\n      FCLASS_BR\n      FCLASS_IL\n      FCLASS_PS\n      FCLASS_SA\n      FCLASS_EG\n      FCLASS_MA\n      FCLASS_PT\n      FCLASS_AR\n      FCLASS_JP\n      FCLASS_KO\n      FCLASS_VN\n      FCLASS_TR\n      FCLASS_ID\n      FCLASS_PL\n      FCLASS_GR\n      FCLASS_IT\n      FCLASS_NL\n      FCLASS_SE\n      FCLASS_BD\n      FCLASS_UA\n      geometry\n    \n  \n  \n    \n      0\n      Admin-0 country\n      1\n      3\n      Zimbabwe\n      ZWE\n      0\n      2\n      Sovereign country\n      1\n      Zimbabwe\n      ZWE\n      0\n      Zimbabwe\n      ZWE\n      0\n      Zimbabwe\n      ZWE\n      0\n      Zimbabwe\n      Zimbabwe\n      ZWE\n      Zimbabwe\n      NaN\n      Zimb.\n      ZW\n      Republic of Zimbabwe\n      NaN\n      Zimbabwe\n      NaN\n      NaN\n      Zimbabwe\n      NaN\n      1\n      5\n      3\n      9\n      14645468.0\n      14\n      2019\n      21440\n      2019\n      5. Emerging region: G20\n      5. Low income\n      ZI\n      ZW\n      ZW\n      ZWE\n      ZWE\n      716\n      716\n      716\n      ZW\n      ZWE\n      23425004\n      23425004\n      Exact WOE match as country\n      ZWE\n      NaN\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      ZWE\n      -99\n      -99\n      Africa\n      Africa\n      Eastern Africa\n      Sub-Saharan Africa\n      8\n      8\n      5\n      -99\n      1\n      0.0\n      2.5\n      8.0\n      29.925444\n      -18.911640\n      1159321441\n      Q954\n      زيمبابوي\n      জিম্বাবুয়ে\n      Simbabwe\n      Zimbabwe\n      Zimbabue\n      زیمبابوه\n      Zimbabwe\n      Ζιμπάμπουε\n      זימבבואה\n      ज़िम्बाब्वे\n      Zimbabwe\n      Zimbabwe\n      Zimbabwe\n      ジンバブエ\n      짐바브웨\n      Zimbabwe\n      Zimbabwe\n      Zimbábue\n      Зимбабве\n      Zimbabwe\n      Zimbabve\n      Зімбабве\n      زمبابوے\n      Zimbabwe\n      津巴布韦\n      辛巴威\n      Admin-0 country\n      NaN\n      Admin-0 country\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      POLYGON ((31.28789 -22.40205, 31.19727 -22.344...\n    \n    \n      1\n      Admin-0 country\n      1\n      3\n      Zambia\n      ZMB\n      0\n      2\n      Sovereign country\n      1\n      Zambia\n      ZMB\n      0\n      Zambia\n      ZMB\n      0\n      Zambia\n      ZMB\n      0\n      Zambia\n      Zambia\n      ZMB\n      Zambia\n      NaN\n      Zambia\n      ZM\n      Republic of Zambia\n      NaN\n      Zambia\n      NaN\n      NaN\n      Zambia\n      NaN\n      5\n      8\n      5\n      13\n      17861030.0\n      14\n      2019\n      23309\n      2019\n      7. Least developed region\n      4. Lower middle income\n      ZA\n      ZM\n      ZM\n      ZMB\n      ZMB\n      894\n      894\n      894\n      ZM\n      ZMB\n      23425003\n      23425003\n      Exact WOE match as country\n      ZMB\n      NaN\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      ZMB\n      -99\n      -99\n      Africa\n      Africa\n      Eastern Africa\n      Sub-Saharan Africa\n      6\n      6\n      6\n      -99\n      1\n      0.0\n      3.0\n      8.0\n      26.395298\n      -14.660804\n      1159321439\n      Q953\n      زامبيا\n      জাম্বিয়া\n      Sambia\n      Zambia\n      Zambia\n      زامبیا\n      Zambie\n      Ζάμπια\n      זמביה\n      ज़ाम्बिया\n      Zambia\n      Zambia\n      Zambia\n      ザンビア\n      잠비아\n      Zambia\n      Zambia\n      Zâmbia\n      Замбия\n      Zambia\n      Zambiya\n      Замбія\n      زیمبیا\n      Zambia\n      赞比亚\n      尚比亞\n      Admin-0 country\n      NaN\n      Admin-0 country\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      POLYGON ((30.39609 -15.64307, 30.25068 -15.643...\n    \n    \n      2\n      Admin-0 country\n      1\n      3\n      Yemen\n      YEM\n      0\n      2\n      Sovereign country\n      1\n      Yemen\n      YEM\n      0\n      Yemen\n      YEM\n      0\n      Yemen\n      YEM\n      0\n      Yemen\n      Yemen\n      YEM\n      Yemen\n      NaN\n      Yem.\n      YE\n      Republic of Yemen\n      NaN\n      Yemen\n      NaN\n      NaN\n      Yemen, Rep.\n      NaN\n      5\n      3\n      3\n      11\n      29161922.0\n      15\n      2019\n      22581\n      2019\n      7. Least developed region\n      4. Lower middle income\n      YM\n      YE\n      YE\n      YEM\n      YEM\n      887\n      887\n      887\n      RY\n      YEM\n      23425002\n      23425002\n      Exact WOE match as country\n      YEM\n      NaN\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      YEM\n      -99\n      -99\n      Asia\n      Asia\n      Western Asia\n      Middle East & North Africa\n      5\n      5\n      4\n      -99\n      1\n      0.0\n      3.0\n      8.0\n      45.874383\n      15.328226\n      1159321425\n      Q805\n      اليمن\n      ইয়েমেন\n      Jemen\n      Yemen\n      Yemen\n      یمن\n      Yémen\n      Υεμένη\n      תימן\n      यमन\n      Jemen\n      Yaman\n      Yemen\n      イエメン\n      예멘\n      Jemen\n      Jemen\n      Iémen\n      Йемен\n      Jemen\n      Yemen\n      Ємен\n      یمن\n      Yemen\n      也门\n      葉門\n      Admin-0 country\n      NaN\n      Admin-0 country\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      MULTIPOLYGON (((53.08564 16.64839, 52.58145 16...\n    \n    \n      3\n      Admin-0 country\n      3\n      2\n      Vietnam\n      VNM\n      0\n      2\n      Sovereign country\n      1\n      Vietnam\n      VNM\n      0\n      Vietnam\n      VNM\n      0\n      Vietnam\n      VNM\n      0\n      Vietnam\n      Vietnam\n      VNM\n      Vietnam\n      NaN\n      Viet.\n      VN\n      Socialist Republic of Vietnam\n      NaN\n      Vietnam\n      NaN\n      NaN\n      Vietnam\n      NaN\n      5\n      6\n      5\n      4\n      96462106.0\n      16\n      2019\n      261921\n      2019\n      5. Emerging region: G20\n      4. Lower middle income\n      VM\n      VN\n      VN\n      VNM\n      VNM\n      704\n      704\n      704\n      VN\n      VNM\n      23424984\n      23424984\n      Exact WOE match as country\n      VNM\n      NaN\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      VNM\n      -99\n      -99\n      Asia\n      Asia\n      South-Eastern Asia\n      East Asia & Pacific\n      7\n      7\n      5\n      2\n      1\n      0.0\n      2.0\n      7.0\n      105.387292\n      21.715416\n      1159321417\n      Q881\n      فيتنام\n      ভিয়েতনাম\n      Vietnam\n      Vietnam\n      Vietnam\n      ویتنام\n      Viêt Nam\n      Βιετνάμ\n      וייטנאם\n      वियतनाम\n      Vietnám\n      Vietnam\n      Vietnam\n      ベトナム\n      베트남\n      Vietnam\n      Wietnam\n      Vietname\n      Вьетнам\n      Vietnam\n      Vietnam\n      В'єтнам\n      ویتنام\n      Việt Nam\n      越南\n      越南\n      Admin-0 country\n      NaN\n      Admin-0 country\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      MULTIPOLYGON (((104.06396 10.39082, 104.08301 ...\n    \n    \n      4\n      Admin-0 country\n      5\n      3\n      Venezuela\n      VEN\n      0\n      2\n      Sovereign country\n      1\n      Venezuela\n      VEN\n      0\n      Venezuela\n      VEN\n      0\n      Venezuela\n      VEN\n      0\n      Venezuela\n      Venezuela\n      VEN\n      Venezuela\n      NaN\n      Ven.\n      VE\n      Bolivarian Republic of Venezuela\n      República Bolivariana de Venezuela\n      Venezuela\n      NaN\n      NaN\n      Venezuela, RB\n      NaN\n      1\n      3\n      1\n      4\n      28515829.0\n      15\n      2019\n      482359\n      2014\n      5. Emerging region: G20\n      3. Upper middle income\n      VE\n      VE\n      VE\n      VEN\n      VEN\n      862\n      862\n      862\n      VE\n      VEN\n      23424982\n      23424982\n      Exact WOE match as country\n      VEN\n      NaN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      VEN\n      -99\n      -99\n      South America\n      Americas\n      South America\n      Latin America & Caribbean\n      9\n      9\n      4\n      -99\n      1\n      0.0\n      2.5\n      7.5\n      -64.599381\n      7.182476\n      1159321411\n      Q717\n      فنزويلا\n      ভেনেজুয়েলা\n      Venezuela\n      Venezuela\n      Venezuela\n      ونزوئلا\n      Venezuela\n      Βενεζουέλα\n      ונצואלה\n      वेनेज़ुएला\n      Venezuela\n      Venezuela\n      Venezuela\n      ベネズエラ\n      베네수엘라\n      Venezuela\n      Wenezuela\n      Venezuela\n      Венесуэла\n      Venezuela\n      Venezuela\n      Венесуела\n      وینیزویلا\n      Venezuela\n      委内瑞拉\n      委內瑞拉\n      Admin-0 country\n      NaN\n      Admin-0 country\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      MULTIPOLYGON (((-60.82119 9.13838, -60.94141 9...\n    \n  \n\n\n\n\nTaking a quick look at this dataset:\n\n# quick view\ncountries.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nNotice the column names are in all caps. It is easier to work with column names in small caps because we don’t need to be pressing shift or capslock. We can do this update like this:\n\n# re-assign the column names: .str.lower() makes them lower case\ncountries.columns = countries.columns.str.lower()\nprint(countries.columns)\n\nIndex(['featurecla', 'scalerank', 'labelrank', 'sovereignt', 'sov_a3',\n       'adm0_dif', 'level', 'type', 'tlc', 'admin',\n       ...\n       'fclass_tr', 'fclass_id', 'fclass_pl', 'fclass_gr', 'fclass_it',\n       'fclass_nl', 'fclass_se', 'fclass_bd', 'fclass_ua', 'geometry'],\n      dtype='object', length=169)\n\n\nFinally, we have too many columns, so let’s only keep a few*:\n\n# remeber: the geometry column has the polygons for each country\ncountries_sub = countries[['admin','type','geometry']]\ncountries_sub.head()\n\n\n\n\n\n  \n    \n      \n      admin\n      type\n      geometry\n    \n  \n  \n    \n      0\n      Zimbabwe\n      Sovereign country\n      POLYGON ((31.28789 -22.40205, 31.19727 -22.344...\n    \n    \n      1\n      Zambia\n      Sovereign country\n      POLYGON ((30.39609 -15.64307, 30.25068 -15.643...\n    \n    \n      2\n      Yemen\n      Sovereign country\n      MULTIPOLYGON (((53.08564 16.64839, 52.58145 16...\n    \n    \n      3\n      Vietnam\n      Sovereign country\n      MULTIPOLYGON (((104.06396 10.39082, 104.08301 ...\n    \n    \n      4\n      Venezuela\n      Sovereign country\n      MULTIPOLYGON (((-60.82119 9.13838, -60.94141 9...\n    \n  \n\n\n\n\n\n\n12.4.2 Arctic communities\nIn the same way as we previously used pandas.read_csv(), we can read in the Arctic communities data directly from the data repository using geopandas.read_file():\n\n# read in Arctic communities data\ncommunities = gpd.read_file('https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Aed7718ae-fb0d-43dd-9270-fbfe80bfc7a4')\ncommunities.head()\n\n\n\n\n\n  \n    \n      \n      name\n      population\n      country\n      geoname-id\n      geometry\n    \n  \n  \n    \n      0\n      Udomlya\n      32373\n      RU\n      452949\n      POINT (34.99250 57.87944)\n    \n    \n      1\n      Valmiera\n      26963\n      LV\n      453754\n      POINT (25.42751 57.54108)\n    \n    \n      2\n      Ventspils\n      42644\n      LV\n      454310\n      POINT (21.57288 57.38988)\n    \n    \n      3\n      Vec-Liepāja\n      85260\n      LV\n      454432\n      POINT (21.01667 56.53333)\n    \n    \n      4\n      Tukums\n      18348\n      LV\n      454768\n      POINT (23.15528 56.96694)\n    \n  \n\n\n\n\nNotice that the countries and communities GeoDataFrames both have the same crs:\n\ncountries.crs == communities.crs\n\nTrue\n\n\nThis makes it easy to take a quick look at our communities data by plotting it on top of the countries dataframe:\n\nfig, ax = plt.subplots()\ncountries.plot(ax=ax)\ncommunities.plot(ax=ax, color='red')\nplt.show()\n\n\n\n\nNext, we want to calculate the number of Arctic communities by country.\n\n# calculate number of communities by country\n\n# extract number of communities by country as a pd.Series\nn_comms = communities.groupby('country').count().name\n\n# convert the pd.Series into a pd.DataFrame and update it\nn_comms = pd.DataFrame(n_comms).rename(columns={'name':'n_communities'}).reset_index()\n\nLet’s break this down a bit:\n\nWe start with our communities dataframe and use groupby('country') to group by country code,\nthen we use count() as an aggregator function to count how many rows belong to each country code.\nThe result of this operation is a dataframe (run communities.groupby('country').count() to check) and we select a single column with the counts by selecting the name column.\nThe result is a single pd.Series in the variable n_comms.\nWe then convert this pd.Series into a pd.DataFrame and clean it up a bit.\n\n\n# number of communities per country\nn_comms\n\n\n\n\n\n  \n    \n      \n      country\n      n_communities\n    \n  \n  \n    \n      0\n      AX\n      1\n    \n    \n      1\n      BY\n      8\n    \n    \n      2\n      CA\n      7\n    \n    \n      3\n      DK\n      72\n    \n    \n      4\n      EE\n      14\n    \n    \n      5\n      FI\n      98\n    \n    \n      6\n      FO\n      1\n    \n    \n      7\n      GB\n      96\n    \n    \n      8\n      GL\n      1\n    \n    \n      9\n      IS\n      5\n    \n    \n      10\n      LT\n      26\n    \n    \n      11\n      LV\n      25\n    \n    \n      12\n      NO\n      48\n    \n    \n      13\n      RU\n      774\n    \n    \n      14\n      SE\n      133\n    \n    \n      15\n      US\n      115\n    \n  \n\n\n\n\nSince we only want data from Scandinavia, we can use the codes for these countries to locate these rows:\n\n# select Scandinavia data\nscandi_codes = ['DK','NO','SE','FO','FI','IS','AX']\nscandi_n_comms = n_comms[n_comms.country.isin(scandi_codes)].copy()\nscandi_n_comms\n\n\n\n\n\n  \n    \n      \n      country\n      n_communities\n    \n  \n  \n    \n      0\n      AX\n      1\n    \n    \n      3\n      DK\n      72\n    \n    \n      5\n      FI\n      98\n    \n    \n      6\n      FO\n      1\n    \n    \n      9\n      IS\n      5\n    \n    \n      12\n      NO\n      48\n    \n    \n      14\n      SE\n      133"
  },
  {
    "objectID": "lectures/lesson-10-merge-data.html#merge-datasets",
    "href": "lectures/lesson-10-merge-data.html#merge-datasets",
    "title": "12  Merging data",
    "section": "12.5 Merge datasets",
    "text": "12.5 Merge datasets\nTo merge two datasets they need to have at least one column in common. Currently our datasets do not have any columns in common:\n\ncountries_sub.head(2)\n\n\n\n\n\n  \n    \n      \n      admin\n      type\n      geometry\n    \n  \n  \n    \n      0\n      Zimbabwe\n      Sovereign country\n      POLYGON ((31.28789 -22.40205, 31.19727 -22.344...\n    \n    \n      1\n      Zambia\n      Sovereign country\n      POLYGON ((30.39609 -15.64307, 30.25068 -15.643...\n    \n  \n\n\n\n\n\nscandi_n_comms.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      n_communities\n    \n  \n  \n    \n      0\n      AX\n      1\n    \n    \n      3\n      DK\n      72\n    \n  \n\n\n\n\nWe can easily fix this by adding an admin column to scandi_n_comms:\n\n# Add country names \nscandi_names = ['Aland Islands',\n                'Denmark',\n                'Finland',\n                'Faroe Islands',\n                'Iceland',\n                'Norway',\n                'Sweden']\nscandi_n_comms['admin'] = scandi_names\nscandi_n_comms\n\n\n\n\n\n  \n    \n      \n      country\n      n_communities\n      admin\n    \n  \n  \n    \n      0\n      AX\n      1\n      Aland Islands\n    \n    \n      3\n      DK\n      72\n      Denmark\n    \n    \n      5\n      FI\n      98\n      Finland\n    \n    \n      6\n      FO\n      1\n      Faroe Islands\n    \n    \n      9\n      IS\n      5\n      Iceland\n    \n    \n      12\n      NO\n      48\n      Norway\n    \n    \n      14\n      SE\n      133\n      Sweden\n    \n  \n\n\n\n\nTo merge dataframes we can use the pandas.merge() function. The basic syntax for it is:\noutput_df = pd.merge(left_df,\n                     right_df, \n                     how = type_of_join, \n                     on = column_to_join)\nwhere\n\noutput_df is the dataframe resulting from the merge,\nleft_df is the dataframe we have “on the left side”,\nright_df is the dataframe we have “on the right side”,\nhow specifies the type of join between the left and right dataframes, (check the options here), the default is to do an inner join,\non specifies the column to join on, this column must be present in both our dataframes.\n\nIn our case we want to perform an inner join between our dataframes. This will merge the subset of rows that have matches in both the left table and the right table.\n\n\n\nImage source: Data Modeling Essentials, NCEAS Learning Hub\n\n\n\n# merge dataframes \nscandi_countries = pd.merge(countries_sub,\n                            scandi_n_comms,\n                            how='inner',\n                            on='admin')\n# update index\nscandi_countries = scandi_countries.set_index('admin')\nscandi_countries\n\n\n\n\n\n  \n    \n      \n      type\n      geometry\n      country\n      n_communities\n    \n    \n      admin\n      \n      \n      \n      \n    \n  \n  \n    \n      Sweden\n      Sovereign country\n      MULTIPOLYGON (((19.07646 57.83594, 18.99375 57...\n      SE\n      133\n    \n    \n      Norway\n      Sovereign country\n      MULTIPOLYGON (((20.62217 69.03687, 20.49199 69...\n      NO\n      48\n    \n    \n      Iceland\n      Sovereign country\n      POLYGON ((-15.54312 66.22852, -15.42847 66.224...\n      IS\n      5\n    \n    \n      Finland\n      Country\n      MULTIPOLYGON (((24.15547 65.80527, 24.04902 65...\n      FI\n      98\n    \n    \n      Faroe Islands\n      Dependency\n      MULTIPOLYGON (((-6.62319 61.80596, -6.64277 61...\n      FO\n      1\n    \n    \n      Denmark\n      Country\n      MULTIPOLYGON (((12.56875 55.78506, 12.57119 55...\n      DK\n      72\n    \n  \n\n\n\n\nNotice that the row for Aland Islands is not present in the merged dataframe. We can verify the value ‘Aland Islands’ was nowhere in our original countries dataframe like this:\n\n# check Aland Islands is nowhere in data frame\n'Aland Islands' in countries.values\n\nFalse\n\n\nThe values attribute of a dataframe returns all the values in the dataframe as an array:\n\n# the underlying values of the dataframe\ncountries.values\n\narray([['Admin-0 country', 1, 3, ..., nan, nan,\n        <POLYGON ((31.288 -22.402, 31.197 -22.345, 31.073 -22.308, 30.916 -22.291, 3...>],\n       ['Admin-0 country', 1, 3, ..., nan, nan,\n        <POLYGON ((30.396 -15.643, 30.251 -15.643, 29.995 -15.644, 29.73 -15.645, 29...>],\n       ['Admin-0 country', 1, 3, ..., nan, nan,\n        <MULTIPOLYGON (((53.086 16.648, 52.581 16.47, 52.448 16.391, 52.328 16.294, ...>],\n       ...,\n       ['Admin-0 country', 3, 4, ..., nan, nan,\n        <MULTIPOLYGON (((-45.718 -60.521, -45.5 -60.546, -45.386 -60.583, -45.357 -6...>],\n       ['Admin-0 country', 3, 6, ..., nan, nan,\n        <POLYGON ((-63.123 18.069, -63.011 18.069, -63.012 18.045, -63.023 18.019, -...>],\n       ['Admin-0 country', 5, 6, ..., nan, nan,\n        <POLYGON ((179.214 -8.524, 179.201 -8.535, 179.196 -8.535, 179.201 -8.512, 1...>]],\n      dtype=object)\n\n\nThe Aland Islands is an autonomous region of Finland and there is one Arctic community registered in this region. We will directly add one to Finland to not lose this piece of data:\n\nscandi_countries.at['Finland', 'n_communities'] += 1\n\nprint(scandi_countries.at['Finland', 'n_communities'])\n\n99"
  },
  {
    "objectID": "lectures/lesson-10-merge-data.html#choropleth-map",
    "href": "lectures/lesson-10-merge-data.html#choropleth-map",
    "title": "12  Merging data",
    "section": "12.6 Choropleth map",
    "text": "12.6 Choropleth map\nA choropleth map is an efficient way to visualize aggregate data per region.\nTo make a choropleth map from our polygons GeoDataFrame we need to specify the column parameter in plot() and make it equal to the column with the values we want to plot in each country.\n\nscandi_countries.plot(column='n_communities',\n                      legend=True)\n\n<AxesSubplot:>\n\n\n\n\n\nTo finish, we can use matplotlib to customize our map:\n\nfig, ax = plt.subplots(figsize=(5, 5))\n#countries.plot(ax=ax)\nscandi_countries.plot(ax=ax,\n                      column='n_communities',\n                       cmap='BuPu',\n                       legend=True,\n                       edgecolor=\"0.8\",\n                       legend_kwds={\"shrink\":.8,\n                                    'label': \"Number of Arctic communities\"\n                                    }\n                       )\n\nax.set_title('Arctic communities in Scandinavia',  fontsize=20)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\n\nplt.show()\n\n\n\n\nCheck-in\nAdd the scandinavian communities as dots on the choropleth map."
  },
  {
    "objectID": "lectures/lesson-10-merge-data.html#complete-workflow",
    "href": "lectures/lesson-10-merge-data.html#complete-workflow",
    "title": "12  Merging data",
    "section": "12.7 Complete workflow",
    "text": "12.7 Complete workflow\n# import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport geopandas as gpd\n\n# ======= IMPORT DATA ========\n# read in Arctic communities data\ncommunities = gpd.read_file('https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Aed7718ae-fb0d-43dd-9270-fbfe80bfc7a4')\n\n# import countries polygons\ncountries = gpd.read_file('ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp')\ncountries.head()\n\n# ======= COUNTRIES PREPARATION =======\n# make column names lower case\ncountries.columns = countries.columns.str.lower()\n\n# select a subset of the columns\ncountries_sub = countries[['admin','type','geometry']]\n\n\n# ======= COMMUNITIES PREPARATION =======\n# extract number of communities by country as a pd.Series\nn_comms = communities.groupby('country').count().name\n\n# convert the pd.Series into a pd.DataFrame and update it\nn_comms = pd.DataFrame(n_comms).rename(columns={'name':'n_communities'}).reset_index()\n\n# select Scandinavia data\nscandi_codes = ['DK','NO','SE','FO','FI','IS','AX']\nscandi_n_comms = n_comms[n_comms.country.isin(scandi_codes)].copy()\n\n# select communities from Scandinavian countries\nscandi_communities = communities[communities.country.isin(scandi_codes)]\nscandi_communities\n\n# ======= MERGE DATASETS =======\n# add names as admin column to scandi_n_comms\nscandi_names = ['Aland Islands',\n                'Denmark',\n                'Finland',\n                'Faroe Islands',\n                'Iceland',\n                'Norway',\n                'Sweden']\nscandi_n_comms['admin'] = scandi_names\n# merge dataframes \nscandi_countries = pd.merge(countries_sub,\n                            scandi_n_comms,\n                            how='inner',\n                            on='admin')\n# update index\nscandi_countries = scandi_countries.set_index('admin')\n\n# ======= CREATE MAP =======\nfig, ax = plt.subplots()\n#countries.plot(ax=ax)\nscandi_countries.plot(ax=ax,\n                      column='n_communities',\n                       cmap='BuPu',\n                       legend=True,\n                       edgecolor=\"0.8\",\n                       legend_kwds={\"shrink\":.8,\n                                    'label': \"Number of Arctic communities\"\n                                    }\n                       )\n\nscandi_communities.plot(ax=ax, \n                        edgecolor='red',\n                        color='white')\n\nax.set_title('Arctic communities in Scandinavia',  fontsize=20)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\n\nplt.show()"
  },
  {
    "objectID": "lectures/lesson-10-merge-data.html#acknowledgments",
    "href": "lectures/lesson-10-merge-data.html#acknowledgments",
    "title": "12  Merging data",
    "section": "12.8 Acknowledgments",
    "text": "12.8 Acknowledgments\nThe section about merging data is based on the Data Modeling Essentials R lesson from the NCEAS Learning Hub.\nHalina Do-Linh, Carmen Galaz García, Matthew B. Jones, Camila Vargas Poulsen. 2023. Open Science Synthesis training Week 1. NCEAS Learning Hub & Delta Stewardship Council."
  },
  {
    "objectID": "lectures/lesson-10-merge-data.html#references",
    "href": "lectures/lesson-10-merge-data.html#references",
    "title": "12  Merging data",
    "section": "12.9 References",
    "text": "12.9 References\nMike Brook. (2023). Approximate Arctic Communities and Populations, (latitude >= 55, 2022). Arctic Data Center. doi:10.18739/A28S4JQ80."
  },
  {
    "objectID": "lectures/lesson-11-crs.html#what-is-a-crs",
    "href": "lectures/lesson-11-crs.html#what-is-a-crs",
    "title": "13  Coordinate Reference Systems",
    "section": "13.1 What is a CRS?",
    "text": "13.1 What is a CRS?\nSince the Earth does not comme with lines and numbers etched on it, us humans need to create a system to locate places on it. A way to solve this is to use a coordinate reference system (CRS). A CRS is, at its core, a way to represent places on the Earth as coordinates.\nTo understand what are the components of a CRS it can be useful to think of the process we would undergo to match places on the Earth to numbers (coordinates) that represent them.\n\n13.1.1 1. Ellipsoid: Pick a shape for the Earth\nStart with the Earth. Our planet is not a perfect spehere. Apart from its rugged surface with high mountains and deep ocean trenches, the shape of the Earth is not round. Due to the Earth’s rotation, the planet is slightly wider at the Equator. Thus, a sphere is not the best way to model the shape of the Earth. Instead, we use an ellipsoid. Think of an ellipsoid as a sphere that has been flattened a bit, thus making it wider across one axis. This is the first step for creating a CRS: picking an ellipsoid, a.k.a picking a shape for the Earth. The ellipsoid used to represent the Earth’s shape has changed across time and it also changes by application.\n\n\n\n13.1.2 2. Datum: Allign the ellipsoid with Earth’s locations\nThe ellipsoid is an abstract mathematical object on which we can locate points on it by using angles. The coordinate system on the ellipsoid is (angles from x, angles form y).\n\n\n\nLatitude and longtide as angular coordinates on the ellipsoid. Source: Coordinate systems and map projections - University of Twente\n\n\nThe next step is thus to make a decision on how to allign the ellipsoid so that it’s surface corresponds to actual places on the Earth. We could think, for example of how to place the ellipsoid’s center relative to Earth’s mass center (is it the same or is it off?) or how to allign the ellipsoid’s axes with the Earth (which meridian will be our prime meridian?). Generally speaking, a geodetic datum, or just datum, is how we are alligning our abstract ellipsoid to match the Earth’s surface.\n\n\n\nRegional datums to optimize location accuracy. Source: GPS for Land Surveyors\n\n\n\n\n\n\n\n\nMany datums\n\n\n\nGeographers and cartographers have created and used many datums across time and applications. Sometimes datum refers both to a choice of ellipsoid and how to allign it to the Earth. More importantly, different datums are used to increase the accuracy of representing different regions of Earth. At the GIS wiki you can see a list of names of different datums optimized for different regions.\n\n\n\n\n13.1.3 3. Geographic CRS: Lat/lon coordinates for all the world\nAt this point, we have successfully made a correspondence between the Earth’s surface of and our ellipsoid. This means we can now use the lat,lon coordinates of the ellipsoid to locate any point on the Earth’s sruface. What we have now is a geographic CRS. Geographic coordinate reference systems use degrees of latitude and longitude to locate points on the Earth, still thinking of it as a 3D object.\n\n\n\nLatitude and longited as angular coordiantes on the Earth - we use a geographic CRS to achieve this.\n\n\n\n\n13.1.4 4. Projection: Flatten a portion of your ellipsoid to make a planar map\nA map is a flat (2-dimensional) representation of a region of the Earth. In a geographic CRS we use a 3-dimensional model of the Earth (the ellipsoid) to locate places on it. To transform our 3D ellipsoid to make a 2D representation we need to use a projection. A projection is a method to convert between angular coordinates (i.e. lat, long coordiantes in 3D) and planar coordnates (i.e. x, y coordinates in 2D).\n\n\n\nImage Source: Battersby, S. (2017). Map Projections. The Geographic Information Science & Technology Body of Knowledge\n\n\n\n\n\n\n\n\nMany projections\n\n\n\nThere are many ways to project an ellipsoid into a 2-dimensional region. However, no matter what projection we use, the area, distances, or angles of the actual regions on Earth will be distorted when we project. Ultimately, the choice of projection relies on whether we want to optimze calculations involving area, angle, or distance measurements, as well as aesthetics when creating a map.\n\n\nMore about projections:\n🎥 Vox - Why all world maps are wrong\n📖 QGIS - A gentle introduction to GIS - Coordinate Reference Systems\n\n\n13.1.5 5. Projected CRS: x,y coordinates for a specific region\nWhen we project a region of the ellipsoid to represent a region of Earth on the plane we obtain a new set of planar coordinates, thus creating a projected CRS. A projected CRS must also include a unit of measure for its axes. These new coordinates are usually called eastings, when referring to the eastward-measured distance (x-coordinate), and northings, when referring to the northward-measured distance (y-coordinate).\n\n\n\nA projected CRS gives planar coordinates."
  },
  {
    "objectID": "lectures/lesson-12-csv-to-geo.html#data",
    "href": "lectures/lesson-12-csv-to-geo.html#data",
    "title": "14  csv to GeoDataFrame",
    "section": "14.1 Data",
    "text": "14.1 Data\nWe will use two datasets in this lesson. The first one is a reprojection of this dataset from the U.S. Energy Information Administration (EIA) with information about operable electric generating plants in the United States by energy source, as of May 2023.\nFollow these steps to download the reprojected datset for this lesson:\n\nGo to https://github.com/carmengg/eds-220-book/blob/main/data/power_plants_epsg4269.csv\nDownload the raw file and move it into your working directory\n\nYou can access the metadata for this dataset here.\nThe second dataset is a TIGER shapefile from the United States Census Bureau. TIGER stands for Topologically Integrated Geographic Encoding and Referencing. This used to be the data format the US Census distributed geospatial data, but since 2008 TIGER files are converted to shapefiles. We will use the shapefiles for the US states. Follow these steps to download shapefile with the United States’ states:\n\nAt the bottom of the 2022 page, under Download, click on “Web Interface”\nFor year, select 2022, and for layer type select “States (and equivalent)”. Click submit.\nClick on “Download national file”.\n\nYou can check the metadata for all the TIGER shapefiles here. The columns for this shapefile are:\n\n\n\nSource: TIGER/Line Shapefiles Technical Documentation\n\n\nFile management: Both datasets must be in a data directory inside your working directory."
  },
  {
    "objectID": "lectures/lesson-12-csv-to-geo.html#dataframe-to-geodataframe",
    "href": "lectures/lesson-12-csv-to-geo.html#dataframe-to-geodataframe",
    "title": "14  csv to GeoDataFrame",
    "section": "14.2 DataFrame to GeoDataFrame",
    "text": "14.2 DataFrame to GeoDataFrame\nLet’s start by importing the necessary libraries:\n\nimport geopandas as gpd\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nAnd update pandas display options:\n\n# display all column when looking at dataframes\npd.set_option(\"display.max.columns\", None)\n\nOur first step is to import the power plants dataset. Notice this is a csv, geopandas doesn’t have a way to extract a geometry column from a csv, so we will need to create this geometry manually. We start by reading in the data using pandas.\n\n# import power plants data\npower_plants = pd.read_csv('data/power_plants_epsg4269.csv')\npower_plants.head(3)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      objectid\n      plant_code\n      plant_name\n      utility_id\n      utility_name\n      sector_name\n      street_address\n      city\n      county\n      state\n      zip\n      primsource\n      source_desc\n      tech_desc\n      install_mw\n      total_mw\n      bat_mw\n      bio_mw\n      coal_mw\n      geo_mw\n      hydro_mw\n      hydrops_mw\n      ng_mw\n      nuclear_mw\n      crude_mw\n      solar_mw\n      wind_mw\n      other_mw\n      source\n      period\n      longitude\n      latitude\n    \n  \n  \n    \n      0\n      0.0\n      11570\n      1\n      Sand Point\n      63560\n      TDX Sand Point Generating, LLC\n      Electric Utility\n      100 Power Plant Way\n      Sand Point\n      Aleutians East\n      Alaska\n      99661.0\n      petroleum\n      Petroleum = 1.3 MW, Wind = 0.4 MW\n      Petroleum Liquids; Onshore Wind Turbine;\n      3.7\n      1.7\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1.3\n      NaN\n      0.4\n      NaN\n      EIA-860, EIA-860M and EIA-923\n      202305.0\n      -160.497222\n      55.339722\n    \n    \n      1\n      1.0\n      11571\n      2\n      Bankhead Dam\n      195\n      Alabama Power Co\n      Electric Utility\n      19001 Lock 17 Road\n      Northport\n      Tuscaloosa\n      Alabama\n      35476.0\n      hydroelectric\n      Hydroelectric = 53 MW\n      Conventional Hydroelectric\n      53.9\n      53.0\n      NaN\n      NaN\n      NaN\n      NaN\n      53.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      EIA-860, EIA-860M and EIA-923\n      202305.0\n      -87.356823\n      33.458665\n    \n    \n      2\n      2.0\n      11572\n      3\n      Barry\n      195\n      Alabama Power Co\n      Electric Utility\n      North Highway 43\n      Bucks\n      Mobile\n      Alabama\n      36512.0\n      natural gas\n      Coal = 1118.5 MW, Natural Gas = 1296.2 MW\n      Conventional Steam Coal; Natural Gas Fired Com...\n      2569.5\n      2414.7\n      NaN\n      NaN\n      1118.5\n      NaN\n      NaN\n      NaN\n      1296.2\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      EIA-860, EIA-860M and EIA-923\n      202305.0\n      -88.010300\n      31.006900\n    \n  \n\n\n\n\n\n# update column names to small caps\npower_plants.columns = power_plants.columns.str.lower()\n\nThis csv has longitude and latitude columns, which indicate the location of the power plants in the NAD83 CRS (EPSG:4269). We use this information to create a new gpd.GeoDataFrame from the pd.DataFrame using the GeoPandas function points_from_xy() like this:\n\npower_plants = gpd.GeoDataFrame(power_plants, # data\n                                    # specify geometry column\n                                    geometry=gpd.points_from_xy(power_plants.longitude, \n                                             power_plants.latitude),\n                                    # specify CRS\n                                    crs='EPSG:4269'\n                    )\n\nCheck we now have a geometry column:\n\npower_plants.head(3)\n\n\n\n\n\n  \n    \n      \n      unnamed: 0\n      objectid\n      plant_code\n      plant_name\n      utility_id\n      utility_name\n      sector_name\n      street_address\n      city\n      county\n      state\n      zip\n      primsource\n      source_desc\n      tech_desc\n      install_mw\n      total_mw\n      bat_mw\n      bio_mw\n      coal_mw\n      geo_mw\n      hydro_mw\n      hydrops_mw\n      ng_mw\n      nuclear_mw\n      crude_mw\n      solar_mw\n      wind_mw\n      other_mw\n      source\n      period\n      longitude\n      latitude\n      geometry\n    \n  \n  \n    \n      0\n      0.0\n      11570\n      1\n      Sand Point\n      63560\n      TDX Sand Point Generating, LLC\n      Electric Utility\n      100 Power Plant Way\n      Sand Point\n      Aleutians East\n      Alaska\n      99661.0\n      petroleum\n      Petroleum = 1.3 MW, Wind = 0.4 MW\n      Petroleum Liquids; Onshore Wind Turbine;\n      3.7\n      1.7\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1.3\n      NaN\n      0.4\n      NaN\n      EIA-860, EIA-860M and EIA-923\n      202305.0\n      -160.497222\n      55.339722\n      POINT (-160.49722 55.33972)\n    \n    \n      1\n      1.0\n      11571\n      2\n      Bankhead Dam\n      195\n      Alabama Power Co\n      Electric Utility\n      19001 Lock 17 Road\n      Northport\n      Tuscaloosa\n      Alabama\n      35476.0\n      hydroelectric\n      Hydroelectric = 53 MW\n      Conventional Hydroelectric\n      53.9\n      53.0\n      NaN\n      NaN\n      NaN\n      NaN\n      53.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      EIA-860, EIA-860M and EIA-923\n      202305.0\n      -87.356823\n      33.458665\n      POINT (-87.35682 33.45867)\n    \n    \n      2\n      2.0\n      11572\n      3\n      Barry\n      195\n      Alabama Power Co\n      Electric Utility\n      North Highway 43\n      Bucks\n      Mobile\n      Alabama\n      36512.0\n      natural gas\n      Coal = 1118.5 MW, Natural Gas = 1296.2 MW\n      Conventional Steam Coal; Natural Gas Fired Com...\n      2569.5\n      2414.7\n      NaN\n      NaN\n      1118.5\n      NaN\n      NaN\n      NaN\n      1296.2\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      EIA-860, EIA-860M and EIA-923\n      202305.0\n      -88.010300\n      31.006900\n      POINT (-88.01030 31.00690)\n    \n  \n\n\n\n\nLet’s see some information about the CRS of our power plants dataset:\n\n# print information about the \nprint('is geographic?: ', power_plants.crs.is_geographic)\nprint('is projected?: ', power_plants.crs.is_projected)\nprint('datum: ', power_plants.crs.datum)\nprint('ellipsoid: ', power_plants.crs.ellipsoid)\n\npower_plants.crs\n\nis geographic?:  True\nis projected?:  False\ndatum:  North American Datum 1983\nellipsoid:  GRS 1980\n\n\n<Geographic 2D CRS: EPSG:4269>\nName: NAD83\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: North America - onshore and offshore: Canada - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon. Puerto Rico. United States (USA) - Alabama; Alaska; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming. US Virgin Islands. British Virgin Islands.\n- bounds: (167.65, 14.92, -47.74, 86.46)\nDatum: North American Datum 1983\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nNow that we have a geometry column and a CRS, we can plot our dataset:\n\npower_plants.plot()\n\n<AxesSubplot:>"
  },
  {
    "objectID": "lectures/lesson-12-csv-to-geo.html#tiger-shapefile",
    "href": "lectures/lesson-12-csv-to-geo.html#tiger-shapefile",
    "title": "14  csv to GeoDataFrame",
    "section": "14.3 TIGER shapefile",
    "text": "14.3 TIGER shapefile\nNext, we import the TIGER shapefile:\n\n# read-in data\nstates = gpd.read_file('data/tl_2022_us_state/tl_2022_us_state.shp')\n# update column names to small caps\nstates.columns = states.columns.str.lower()\n\nstates.head()\n\n\n\n\n\n  \n    \n      \n      region\n      division\n      statefp\n      statens\n      geoid\n      stusps\n      name\n      lsad\n      mtfcc\n      funcstat\n      aland\n      awater\n      intptlat\n      intptlon\n      geometry\n    \n  \n  \n    \n      0\n      3\n      5\n      54\n      01779805\n      54\n      WV\n      West Virginia\n      00\n      G4000\n      A\n      62266456923\n      489045863\n      +38.6472854\n      -080.6183274\n      POLYGON ((-77.75438 39.33346, -77.75422 39.333...\n    \n    \n      1\n      3\n      5\n      12\n      00294478\n      12\n      FL\n      Florida\n      00\n      G4000\n      A\n      138962819934\n      45971472526\n      +28.3989775\n      -082.5143005\n      MULTIPOLYGON (((-83.10874 24.62949, -83.10711 ...\n    \n    \n      2\n      2\n      3\n      17\n      01779784\n      17\n      IL\n      Illinois\n      00\n      G4000\n      A\n      143778515726\n      6216539665\n      +40.1028754\n      -089.1526108\n      POLYGON ((-87.89243 38.28285, -87.89334 38.282...\n    \n    \n      3\n      2\n      4\n      27\n      00662849\n      27\n      MN\n      Minnesota\n      00\n      G4000\n      A\n      206244837557\n      18937184315\n      +46.3159573\n      -094.1996043\n      POLYGON ((-95.31989 48.99892, -95.31747 48.998...\n    \n    \n      4\n      3\n      5\n      24\n      01714934\n      24\n      MD\n      Maryland\n      00\n      G4000\n      A\n      25151771744\n      6979295311\n      +38.9466584\n      -076.6744939\n      POLYGON ((-75.75600 39.24607, -75.75579 39.243...\n    \n  \n\n\n\n\nLet’s see some information about the CRS of our states geodataframe:\n\n# print information about the CRS\nstates.crs\n\n<Geographic 2D CRS: EPSG:4269>\nName: NAD83\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: North America - onshore and offshore: Canada - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon. Puerto Rico. United States (USA) - Alabama; Alaska; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming. US Virgin Islands. British Virgin Islands.\n- bounds: (167.65, 14.92, -47.74, 86.46)\nDatum: North American Datum 1983\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nAnd plot it:\n\nstates.plot()\n\n<AxesSubplot:>\n\n\n\n\n\nNotice the map covers a big extension, this is because, according to the TIGER shapefiles metadata:\n\nIn addition to the fifty states, the Census Bureau treats the District of Columbia, Puerto Rico, and the Island areas (American Samoa, the Commonwealth of the Northern Mariana Islands, Guam, and the U.S. Virgin Islands) as statistical equivalents of states for the purpose of data presentation.\n\nIn this US Census Bureau file we can see what each code for the region, division, and state corresponds to. These should be numeric codes, so we can start by updating the corresponding columns in the states geo-dataframe:\n\n # notice region, division, and statefp are strings (object) types\n states.dtypes\n\nregion        object\ndivision      object\nstatefp       object\nstatens       object\ngeoid         object\nstusps        object\nname          object\nlsad          object\nmtfcc         object\nfuncstat      object\naland          int64\nawater         int64\nintptlat      object\nintptlon      object\ngeometry    geometry\ndtype: object\n\n\n\n # update dtypes of code columns\nstates.region = states.region.astype('int')\nstates.division = states.division.astype('int')\nstates.statefp = states.statefp.astype('int')\n\nStates corresponds to regions 1 through 4. However, there’s also a region code 9. These rows correspond to non-state regions:\n\nprint(states.region.unique())\nstates[states.region==9]\n\n[3 2 1 4 9]\n\n\n\n\n\n\n  \n    \n      \n      region\n      division\n      statefp\n      statens\n      geoid\n      stusps\n      name\n      lsad\n      mtfcc\n      funcstat\n      aland\n      awater\n      intptlat\n      intptlon\n      geometry\n    \n  \n  \n    \n      34\n      9\n      0\n      78\n      01802710\n      78\n      VI\n      United States Virgin Islands\n      00\n      G4000\n      A\n      348021909\n      1550236187\n      +18.3392359\n      -064.9500433\n      MULTIPOLYGON (((-64.76834 18.26033, -64.77074 ...\n    \n    \n      35\n      9\n      0\n      69\n      01779809\n      69\n      MP\n      Commonwealth of the Northern Mariana Islands\n      00\n      G4000\n      A\n      472292521\n      4644252458\n      +15.0010865\n      +145.6181702\n      MULTIPOLYGON (((145.05897 14.12500, 145.06302 ...\n    \n    \n      36\n      9\n      0\n      66\n      01802705\n      66\n      GU\n      Guam\n      00\n      G4000\n      A\n      543555849\n      934337453\n      +13.4417451\n      +144.7719021\n      POLYGON ((144.56343 13.44806, 144.56357 13.450...\n    \n    \n      41\n      9\n      0\n      60\n      01802701\n      60\n      AS\n      American Samoa\n      00\n      G4000\n      A\n      197759069\n      1307243751\n      -14.2671590\n      -170.6682674\n      MULTIPOLYGON (((-170.53809 -14.33613, -170.548...\n    \n    \n      49\n      9\n      0\n      72\n      01779808\n      72\n      PR\n      Puerto Rico\n      00\n      G4000\n      A\n      8869029522\n      4922249087\n      +18.2176480\n      -066.4107992\n      MULTIPOLYGON (((-66.32322 17.87767, -66.33170 ...\n    \n  \n\n\n\n\nWe can check that Alaska and the non-state regions are causing the long map:\n\n# plot data in states that is not Alaska (code 2) and doesn't have region code 9\nstates[(states.statefp!=2) & (states.region!=9)].plot()\n\n<AxesSubplot:>"
  },
  {
    "objectID": "lectures/lesson-12-csv-to-geo.html#data-selection",
    "href": "lectures/lesson-12-csv-to-geo.html#data-selection",
    "title": "14  csv to GeoDataFrame",
    "section": "14.4 Data selection",
    "text": "14.4 Data selection\nFor the pupose of this exercise, we want to keep only data for the contiguous states. Let’s overwrite the geo-dataframes accordingly:\n\nstates = states[(states.region!=9) & (~states.statefp.isin([2,15]))]\npower_plants = power_plants[~power_plants.state.isin(['Puerto Rico','Hawaii','Alaska'])]\n\n\n\n\n\n\n\n~ = not\n\n\n\nIn the previous code we used the syntax\n~df.column.isin([val1, val2, val3])\nThe ~ tilde symbol is used in Python to negate a statement. So the previous line could be read as “the values in df’s column which are not in the list [val1, val2, val3].”"
  },
  {
    "objectID": "lectures/lesson-12-csv-to-geo.html#plotting",
    "href": "lectures/lesson-12-csv-to-geo.html#plotting",
    "title": "14  csv to GeoDataFrame",
    "section": "14.5 Plotting",
    "text": "14.5 Plotting\nBefore we plot our data, let’s make sure they are in the same CRS:\n\nstates.crs == power_plants.crs\n\nTrue\n\n\nWe can now try plotting both datasets together. To color the power_plants dots according to color we just need\n\nfig, ax = plt.subplots()\n\n# add states \nstates.plot(ax=ax,\n            color='none',\n            edgecolor = 'slategray')\n\n# add electric power plants colored by energy source\npower_plants.plot(ax=ax, \n                  column='primsource', # color points according to primsource value\n                  legend=True,    # add legend\n                  markersize = 4, # adjust point size\n                  cmap='tab20', # this color map has 20 different colors\n                  alpha=0.5)\n\nplt.show()\n\n\n\n\nAnd we can finish by adding more information to contextualize our map:\n\n# figsize updates the figure size\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# --------------------------\n# remove the axis box around the map\nax.axis('off')\n\n# update title\nax.set_title('Operable electric generating plants in the contiguous United States',\nfontsize=20)\n\n# annotate the data source\nax.annotate(\"Data: U.S. Energy Information Administration (EIA), accessed Oct 30, 2023 \\nhttps://atlas.eia.gov/datasets/eia::power-plants/about\", \n            xy=(0.25, .06), # position\n            xycoords='figure fraction', \n            fontsize=10, \n            color='#555555')\n\n# --------------------------\n# add states \nstates.plot(ax=ax,\n               color='none',\n               edgecolor = '#362312')\n\n# add electric power plants colored by energy source\npower_plants.plot(ax=ax, \n                  column='primsource',\n                  legend=True,\n                  markersize = 4,\n                  cmap='tab20',\n                  alpha=0.5,\n                  # adjust legend\n                  legend_kwds={'loc': \"lower right\", \n                                'title':'Primary energy source',\n                                'title_fontsize':'small', \n                                'fontsize':'small'})\n\n<AxesSubplot:title={'center':'Operable electric generating plants in the contiguous United States'}>"
  },
  {
    "objectID": "appendices/set-pat.html",
    "href": "appendices/set-pat.html",
    "title": "Appendix A — Setting a PAT",
    "section": "",
    "text": "This section contains instructions for creatiog a Personal Access Token (PAT):\n\nIn GitHub navigate to:\n\n\nsettings\ndeveloper settings (last option on the left)\ntokens (classic)\ngenerate new token button (near right corner)\nClick on: Generate new token (classic)\n\n\nSelect the following options:\n\n\n☑️ Repo (clicking this will select all boxes in the section, that’s ok)\n☑️ workflow\n☑️ gist\n☑️ user (clicking this will select all boxes in the section, that’s ok)\n\n\nClick generate token.\nCopy and paste the PAT somewhere for now!\nBack at the terminal try to push your changes again using git push. When prompted, enter your GitHub username and the new PAT as the password.\n\n\n\n\n\n\n\nNote\n\n\n\nIn steps 1-5 we generated a PAT manually. This is the same process as using usethis::create_github_token from an R console as explained in the MEDS installation guide. The options we selected in step 2 are the same options for the PAT given by usethis::create_github_token."
  }
]