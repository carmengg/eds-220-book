# Pandas Fundamentals

## About
In this lesson we cover the two core objects in the `pandas` library, the `pandas.Series` and the `pandas.DataFrame`. We will also learn methods to select data from our datasets.

## `pandas`

`pandas` is a Python package to wrangle and analyze tabular data. It is built on top of NumPy and has become the core tool for doing data analysis in Python. 

The convention to import it is:

```{python}
import pandas as pd

# we will also import numpy 
import numpy as np
```


There is so much to learn about `pandas`. While we won't be able to cover every single functionality of this package in the next three lecutres, the goal is to get you started with the basic tools for data wrangling and give you a solid basis on which you can explore further. 

## Series

The first core data structure of pandas is the **series**. 
A series is a *one-dimensional* array of *indexed* data. A `pandas.Series` having an **index** is the main difference between a `pandas.Series` and a numpy array. See the difference:

```{python}
# a numpy array
# np.random.randn returns values from the std normal distribution
arr = np.random.randn(4) 
print(type(arr))
print(arr, "\n")

# a pandas series made from the previous array
s = pd.Series(arr)
print(type(s))
print(s)
```


### Creating a `pandas.Series`

The basic method to create a `pandas.Series` is to call

``` python
s = pd.Series(data, index=index)
```

The `data` parameter can be:

- a [numpy array](https://numpy.org/doc/stable/reference/arrays.ndarray.html) or [a list](https://realpython.com/python-list/)
- a [Python dictionary](https://realpython.com/lessons/dictionary-python/)
- [a number](https://commons.wikimedia.org/wiki/File:Number-three.JPG)

The `index` parameter is a list of index labels.

For now, we will create a `pandas.Series` from a numpy array or list. To use this method we need to pass a numpy array (or a list of objects that can be converted to NumPy types) as `data` and a list of indices of the same length as data. 

```{python}
# a Series from a numpy array 
pd.Series(np.arange(3), index=['a','b','c'])
```

 The `index` parameter is optional. If we don't include it, the default is to make the index equal to `[0,...,len(data)-1]`. For example:

```{python}
# a Series from a list of strings with default index
pd.Series(['EDS 220', 'EDS 222', 'EDS 223', 'EDS 242'])
```

<!--
#### From a dictionary

Remember a dictionary is a set of key-value pairs. If we create a `pandas.Series` via a dictionary the keys will become the index and the values the corresponding data.

```{python}
# construct dictionary
d = {'a':0, 'b':1, 'c':2}

# initialize a sries using a dictionary
pd.Series(d)
```

#### From a number
If we only provide a number as the data for the series, we need to provide an index. The number will be repeated to match the length of the index.

```{python}
pd.Series(3.0, index = ['A', 'B', 'C'])
```

-->


### Simple operations

Arithmetic operations work on series and also most NumPy functions. For example:

```{python}
# define a series
s = pd.Series([98,73,65],index=['Andrea', 'Beth', 'Carolina'])

# divide each element in series by 10
print(s /10, '\n')

# take the exponential of each element in series
print(np.exp(s), '\n')

# notice this doesn't change the values of our series
print(s)

```

We can also produce new `pandas.Series` with `True`/`False` values indicating whether the elements in a series satisfy a condition or not:

```{python}
s > 10
```

This kind of simple conditions on `pandas.Series` will be key when we are selecting data from data frames.

<!-- TO DO: ADD COMPARING VALUES OF A SERIES -->

### Attributes & Methods
`pandas.Series` have *many* attributes and methods, you can see a [full list in the `pandas` documentation](https://pandas.pydata.org/docs/reference/api/pandas.Series.html). For now we will cover two examples that have to do with identifying missing values.

`pandas` represents a missing or NA value with `NaN`, which stands for not a number. Let's construct a small series with some NA values:

```{python}
# series with NAs in it
s = pd.Series([1, 2, np.NaN, 4, np.NaN])
```

A `pandas.Series` has an *attribute* called `hasnans` that returns `True` if there are any NaNs:

```{python}
# check if series has NAs
s.hasnans
```

Then we might be intersted in knowing which elements in the series are NAs. We can do this using the [`isna` method](https://pandas.pydata.org/docs/reference/api/pandas.Series.isna.html):

```{python}
s.isna()
```

We can see the ouput is a `pd.Series` of boolean values indicating if an element in the row at the given index is NA (`True` = is NA) or not (`False` = not NA).

:::{.callout-note}
## moving on
There's much more to say about `pandas.Series`, but this is enought to get us going. At this point, we mainly want to know about `pandas.Series` because `pandas.Series` are the columns of `pandas.DataFrame`s.
:::

<!--
::: {.callout-caution}
## slicing with `loc`
Notice that when use slicing with `loc` we get both the start *and the end* of the indices we indicated. This is different to slicing in numpy arrays or lists where we do not get the element at the end of the slice. Compare the following:

```{python}
x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
print(x)

# slicing will return elements at indices 2 trhough 4 (inclusive)
x[2:5]
```

```{python}
# define a np array with integers from 0 to 9
y = np.arange(10)
print(y)

# slicing will return elements at indices 2 trhough 4 (inclusive)
y[2:5]
```

```{python}
 z = pd.Series(y)
 print(z)

# slicing will return elements with index labels 2 through 5 (inclusive)
 z.loc[2:5]
```
::: 
-->

## Data Frames

The Data Frame is the most used `pandas` object. It represents tabular data and we can think of it as a spreadhseet. Each column of a `pandas.DataFrame` is a `pandas.Series`. 

### Creating a `pandas.DataFrame`
There are [many ways of creating a `pandas.DataFrame`](https://pandas.pydata.org/docs/user_guide/dsintro.html#dataframe). 

<!--
Although we likely won't be creating data frames from scratch often, I'd like to go over creating a `pandas.DataFrame` from a `dict` of `pandas.Series` as this will help us understand the syntax for other Data Frame methods. 
-->

We already mentioned each column of a `pandas.DataFrame` is a `pandas.Series`. In fact, the `pandas.DataFrame` is a dictionary of `pandas.Series`, with each column name being the key and the column values being the key's value. Thus, we can create a `pandas.DataFrame` in this way:

```{python}
# initialize dictionary with columns' data 
d = {'col_name_1' : pd.Series(np.arange(3)),
     'col_name_2' : pd.Series([3.1, 3.2, 3.3]),
     }

# create data frame
df = pd.DataFrame(d)
df
``` 

We can change the index and column names by changing the `index` and `columns` attributes in the data frame. 

```{python}
# print original index
print(df.index)

# change the index
df.index = ['a','b','c']
df
```

```{python}
# print original column names
print(df.columns)

# change column names 
df.columns = ['C1','C2']
df
```

## Subsetting a `pandas.DataFrame`

Like it's often the case when working with `pandas`, there are *many* ways in which we can subset a data frame. We will review the core methods to do this. 
<!--
There are two ways to subset data in a Data Frame: by position and by label. 

* **Subsetting by label** means we want to select data from our data frame using the *names* of the columns or the index.

* **Subsetting by position** means we want to select data from our data frame based on the data's *order* in the data frame.
-->

For all examples we will use simplified data (glacial_loss.csv) from the National Snow and Ice Data Center ([Original dataset](â€‹http://dx.doi.org/10.7265/N52N506F)). The column descriptions are:

- **year**: â€‹calendar year
- **europe - antarctica**: â€‹change in glacial volume (km3â€‹ â€‹) in each region that year
- **global_glacial_volume_change**: â€‹cumulativeâ€‹ global glacial volume change (km3),
starting in 1961
- **annual_sea_level_rise**: â€‹annual rise in sea level (mm)
- **cumulative_sea_level_rise**:â€‹ cumulative rise in sea level (mm) since 1961

First, we read-in the file and get some baisc information about this data frame:
```{python}
# read in file
df = pd.read_csv('data/lesson-1/glacial_loss.csv')

# see the first five rows
df.head()
```

```{python}
# get column names
df.columns
```

```{python}
# check the data types of each column
df.dtypes
```

```{python}
# data frame's shape: output is a tuple (# rows, # columns)
df.shape
```

### Selecting a single column...

#### ...by column name
This is the simplest case for selecting data. Suppose we are interested in the annual sea level rise. Then we can access that single column in this way:
```{python}
# seelect a single column by using square brackets []
annual_rise = df['annual_sea_level_rise']

# check the type of the ouput
print(type(annual_rise))

annual_rise.head()
```

Since we only selected a single column the output is a `pandas.Series`. 

:::{.callout-note}
## `pd.DataFrame` = dictionary of columns
Remember we can think of a `pandas.DataFrame` as a dictionary of its columns? Then we can access a single column using the column name as the key, just like we would do in a dictionary. That is the  we just used: `df['column_name']`.
:::

This is an example of **selecting by label**, which means we want to select data from our data frame using the *names* of the columns, *not their position*.

#### ... with attribute syntax

We can also access a single column by using attribute syntax:

```{python}
annual_rise_2 = df.annual_sea_level_rise
annual_rise_2.head()
```


### Selecting multiple columns...

#### ... using a list of column names
This is another example of selecting by labels. We just need to pass a list with the column names to the square brackets `[]`. For example, say we want to look at the change in glacial volume in Europe and Asia, then we can select those columns like this:

```{python}
# select columns with names "europe" and "asia"
europe_asia = df[['europe','asia']]
```

Notice there are double square brackets. This is because we are passing the list of names `['europe','asia']` to the selection brakcets `[]`. 

```{python}
# check the type of the resulting selection
print(type(europe_asia))

# check the shape of the selection
print((europe_asia.shape))
```

#### ... using a slice
Yet another example of selecting by label! In this case we will use the `loc` function. This is a powerful function! The general syntax is 

```python
df.loc[ row-selection , column-selection]
```
where `row-selection` and `column-selection` are the rows and columns we want to subset from the data frame. 

Let's start by a simple example, where we want to select a slice of columns, say the change in glacial volume per year in all regions. This corresponds to all columns between `arctic` and `antarctica`.

```{python}
# select all columns between 'arctic' and 'antarctica'
all_regions = df.loc[:,'arctic':'antarctica']
all_regions.head()
```

Notice two things:

- we used the colon `:` as the `row-selection` parameter, which means "select all the rows"
- the slice of the data frame we got includes both endpoints of the slice `'arctic':'antarctica'`. In other words we get the `arctic` column *and* the `antarctica` column. This is different from how slicing works in base Python and NumPy, where the end point is not included.

### Selecting rows...
Now that we are familiar with some methods for selecting columns, let's move on to selecting rows. 

#### ... using a condition
Selecting which rows satisfy a particular condition is, in my experience, the most usual kind of row subsetting. The general syntax for this type of selection is `df[condition_on_rows]`. For example, suppose we are intersted in all data after 1996. We can select those rows in this way:

```{python}
# select all rows with year > 1996
after_96 = df[df['year']>1996]
after_96
```

Let's break down what is happening here. In this case the condition for our rows is `df['year']>1996`, this checks which rows have a value greater than 1996 in the year column. Let's see this explicitely:

```{python}
# check the type of df['year']>1996
print(type(df['year']>1996))

df['year']>1996
```

The output is a `pandas.Series` with boolean values (`True` or `False`) indicating which rows satisfy the condition year>1996. When we pass such a series of boolean values to the selection brackets `[]` we keep only those rows with a `True` value. 

Here's another example of using a condition. Suppose we want to look at data from years 1970 to 1979. One way of doing this is to use the `in` operator in our condition:

```{python}
seventies = df[df['year'].isin(range(1970,1980))]
seventies
```

Let's break it down: 

- `df['year']` is the column with the year values, a `pandas.Series`,

- in `df['year'].isin()`, we have that [`isin` is a method for the `pandas.Series`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.isin.html) and we are calling it using the dot `.`. 

- `range(1970,1980)` constructs consecutive integers from 1970 to 1979 - remember the right endopoint (1980) is not included!

- `df['year'].isin(range(1970,1980))` is then a `pandas.Series` of boolean values indicating which rows have year equal to 1970, ..., 1979. 

- when we put `df['year'].isin(range(1970,1980))` inside the selection brackets `[]` we obtain the rows of the data frame with year equal to 1970, ..., 1979.

:::{.callout-note}
## `loc` for row selection
It is equivalent to write

```python
# select rows with year<1965
df[df['year'] < 1965]
```
and
```python
# select rows with year<1965 using love
df.loc[ df['year'] <1965 , :]
```
In the second one:

- we are using the `df.loc[ row-selection , column-selection]` syntax

- the `row-selection` parameter is the condition `df['year']<1965`

- the `column-selection` parameter is a colon `:`, which indicates we want all columns for the rows we are selecting.

We prefer the first syntax when we are selecting rows and not columns since it is simpler.
:::

#### ... using multiple conditions
We can combine multipe conditions by surrounding each one in parenthesis `()` and using the or operator `|` and the and operator `and`.

***or* example**:

```{python}
# select rows with 
# annual_sea_level_rise<0.5 mm OR annual_sea_level_rise>0.8 mm

df[ (df['annual_sea_level_rise']<0.5) | (df['annual_sea_level_rise']>0.8)]
df.head()
```

***and* example**

```{python}
# select rows with cumulative_sea_level_rise>10 AND  global_glacial_volume_change<-300
df[ (df['cumulative_sea_level_rise']>10) & (df['global_glacial_volume_change']<-300)]
```


#### ... by position

All the selections we have done so far have been using labels or using a condition. Sometimes we might want to select certain rows depending on their *actual position* in the data frame. In this case we use the `iloc` method with the syntax `df.iloc[row-indices]`. `iloc` stands for integer-location based indexing. Let's see some examples:

```{python}
# select the fifht row = index 4
df.iloc[4]
```

```{python}
# select rows 23 through 30, inclduing 30
df.iloc[23:31]
```

Notice since we are back to indexing by position the right endpoint of the slice (6) is not included in the ouput. 

### Selecting rows and columns simultaneously...

Selecting rows and columns simultaneously can be done using `loc` (labels or conditions) or `iloc` (integer position).

#### ...by labels or conditions
When we want to select rows and columns simultaneously by labels or conditions we can use the method `loc` with the syntax 

```python
df.loc[ row-selection , column-selection]
```

specifying both paratmers: `row-selection` and `column-selection`. These parameters can be a condition (which generates a boolean array) or a subset of labels from the index or the column names. Let's see an examples:

```{python}
# select change in glacial volume in Europe per year after 2000
df.loc[df['year']>2000,['year','europe']]
```
Let's break it down:

- we are using the `df.loc[ row-selection , column-selection]` syntax

- the `row-selection` parameter is the condition `df['year']>1990`, which is a boolean array saying which years are greater than 1990

- the `column-selection` parameter is `['year','europe']` which is a list with the names of the two columns we are intersted in. 

#### ... by position

When we want to select rows and columns simultaneously by position we use the `iloc` method with the syntax:
```python
df.iloc[ row-indices , column-indices]
```

For example, 

```{python}
# select rows 3-7 (including 7) and columns 3 and 4
df.iloc[ 3:8, [3,4] ]
```

Let's break it down:

- we are using the `df.iloc[ row-indices , column-indices]` syntax

- the `row-indices` parameter is the slice *of integer indices* 3:8. Remember the right endpoint (8) won't be included.

- the `column-indices` parameter is the list of integer indices 3 and 4. This means we are selecting the fourth and fifth column.


### Notes about `loc` and `iloc`

::: {.callout-caution}
## `iloc` vs. `loc`
At the beginning, the difference between `iloc` and `loc` can be confusing. Remember the `i` in `iloc` stands for *integer-location*, so this function only uses integer indexing to retrieve information from the data frames in the same way as indexing for Python lists.

If you want to dive deeper, this is a great discussion about the difference between `iloc` and `loc`: [Stackoverflow - How are iloc and loc different?](https://stackoverflow.com/questions/31593201/how-are-iloc-and-loc-different/31593712#31593712)

And, as always, the documentation will provide you with more information:
[`pandas.DataFrame.loc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) and [`pandas.DataFrame.iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html).
:::


:::{.callout-warning}
## `iloc` for column selection? Avoid it!
We can also access columns by position using `iloc` - but it is best not to if possible.

Suppose we want to access the 10th column in the data frame - then we want to select a column *by position*. In this case the 10th column is the annual sea level rise data and the 10th position corresponds to the index 9. We can select this column by position using the `iloc` method*:

```{python}
# select column by position using iloc
# the syntax is iloc[rows,columns]
# [:,9] means "select all rows from the 10th column"
annual_rise_3 = df.iloc[:,9]
annual_rise_3.head()
```

Unless you are *really* looking for information about *the 10th column*, do not access a column by position. This is bound to break in many ways:

- it relies on a person correctly counting the position of a column. Even with a small dataset this can be prone to error.

- it is not explicit: if we want information about sea level rise `df.annual_sea_level_rise` or `df['annual_sea_level_rise']` are explicitely telling us we are accessing that information. `df.iloc[:,9]` is obscure and uninformative.

- datastets can get updated. Maybe a new column was added before `annual_sea_level_rise`, this would change the position of the column, which would make any code depending on `df.iloc[:,9]` invalid. Accessing by label helps reproducibility!

:::

## Resources

What is presented in this section is a comprehensive, but not an exhaustive list of methods to select data in `pandas.DataFrames`. There are *so many* ways to subset data to get the same result. Some of the content from this lesson is adapted from the following resources and I encourage you to read them to learn more! 

ðŸ“– [Pandas getting started tutorials - How to I select a subset of a
DataFrame](https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html) 

ðŸ“– [Pandas documentation - User Guide - Indexing and Selecting Data](https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-slicing-with-labels)

ðŸ“– [Python for Data Analysis, 3E - Getting started with pandas](https://wesmckinney.com/book/pandas-basics)

## Acknowledgements

The simplified glacial_loss.csv dataset was created by [Dr. Allison Horst](https://allisonhorst.github.io) as part of her course materials on environmental data science. 